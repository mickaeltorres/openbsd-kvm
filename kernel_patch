diff --git a/sys/arch/amd64/amd64/autoconf.c b/sys/arch/amd64/amd64/autoconf.c
index c75907a..85ef356 100644
--- a/sys/arch/amd64/amd64/autoconf.c
+++ b/sys/arch/amd64/amd64/autoconf.c
@@ -105,6 +105,9 @@ void		aesni_setup(void);
 extern int	amd64_has_aesni;
 #endif
 
+void		svm_setup(void);
+void		vmx_setup(void);
+
 /*
  * Determine i/o configuration for a machine.
  */
@@ -154,6 +157,8 @@ cpu_configure(void)
 	if (amd64_has_aesni)
 		aesni_setup();
 #endif
+	svm_setup();
+	vmx_setup();
 }
 
 void
diff --git a/sys/arch/amd64/amd64/conf.c b/sys/arch/amd64/amd64/conf.c
index 1009144..b96780b 100644
--- a/sys/arch/amd64/amd64/conf.c
+++ b/sys/arch/amd64/amd64/conf.c
@@ -112,6 +112,12 @@ int	nblkdev = nitems(bdevsw);
 	(dev_type_stop((*))) enodev, 0, seltrue, \
 	(dev_type_mmap((*))) enodev }
 
+/* open, close, ioctl, mmap */
+#define cdev_kvm_init(c,n) {						\
+    dev_init(c,n,open), dev_init(c,n,close),				\
+      (dev_type_read((*))) enodev, (dev_type_write((*))) enodev,	\
+      dev_init(c,n,ioctl), (dev_type_stop((*))) enodev, 0, seltrue, 	\
+      dev_init(c,n,mmap) }
 
 #define	mmread	mmrw
 #define	mmwrite	mmrw
@@ -164,6 +170,8 @@ cdev_decl(nvram);
 cdev_decl(drm);
 #include "viocon.h"
 cdev_decl(viocon);
+#include "kvm.h"
+cdev_decl(kvm);
 
 #include "wsdisplay.h"
 #include "wskbd.h"
@@ -295,6 +303,7 @@ struct cdevsw	cdevsw[] =
 	cdev_tty_init(NVIOCON,viocon),  /* 94: virtio console */
 	cdev_pvbus_init(NPVBUS,pvbus),	/* 95: pvbus(4) control interface */
 	cdev_ipmi_init(NIPMI,ipmi),	/* 96: ipmi */
+	cdev_kvm_init(NKVM,kvm),        /* 97: kvm */
 };
 int	nchrdev = nitems(cdevsw);
 
diff --git a/sys/arch/amd64/amd64/cpu.c b/sys/arch/amd64/amd64/cpu.c
index c1dedf9..f9bccc0 100644
--- a/sys/arch/amd64/amd64/cpu.c
+++ b/sys/arch/amd64/amd64/cpu.c
@@ -659,6 +659,9 @@ cpu_boot_secondary(struct cpu_info *ci)
 	}
 }
 
+void svm_enter(void);
+void vmx_enter(void);
+
 /*
  * The CPU ends up here when it's ready to run
  * This is called from code in mptramp.s; at this point, we are running
@@ -736,6 +739,12 @@ cpu_hatch(void *v)
 	nanouptime(&ci->ci_schedstate.spc_runtime);
 	splx(s);
 
+	// setup SVM for this AP
+	svm_enter();
+	
+	// setup VMX for this AP
+	vmx_enter();
+	
 	SCHED_LOCK(s);
 	cpu_switchto(NULL, sched_chooseproc());
 }
diff --git a/sys/arch/amd64/amd64/kvm.c b/sys/arch/amd64/amd64/kvm.c
new file mode 100644
index 0000000..66c9696
--- /dev/null
+++ b/sys/arch/amd64/amd64/kvm.c
@@ -0,0 +1,2825 @@
+/*
+ * Copyright (c) 2016 Mickael Torres
+ * All rights reserved.
+ *
+ * Permission to use, copy, modify, and distribute this software for any
+ * purpose with or without fee is hereby granted, provided that the above
+ * copyright notice and this permission notice appear in all copies.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
+ * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
+ * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
+ * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
+ * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
+ * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
+ */
+
+#include <sys/types.h>
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/proc.h>
+#include <sys/ioccom.h>
+#include <sys/malloc.h>
+#include <sys/errno.h>
+#include <sys/filedesc.h>
+#include <sys/file.h>
+#include <sys/vnode.h>
+#include <sys/namei.h>
+
+#include <uvm/uvm.h>
+#include <uvm/uvm_extern.h>
+
+#include <machine/cpufunc.h>
+#include <machine/specialreg.h>
+#include <machine/kvm.h>
+#include <machine/svm.h>
+#include <machine/vmx.h>
+
+// hardware acceleration type
+struct kvm_hw_ptr *kvm_hw;
+int vm_id;
+
+// ATTACH
+void kvmattach(int n)
+{
+  struct cpu_info *ci;
+
+  if (n > 1)
+    return;
+  
+  ci = curcpu();
+  
+  if (ci->ci_svm && (ci->ci_svm->flags & SVM_SVMEN))
+    kvm_hw = svm_get_hw();
+  else if (ci->ci_vmx && (ci->ci_vmx->flags & VMX_VMXON))
+    kvm_hw = vmx_get_hw();
+  else
+    kvm_hw = NULL;
+  vm_id = 1;
+}
+
+// OPEN
+int kvmopen(dev_t dev, int flag, int mode, struct proc *p)
+{
+  struct v_kvm_vpage *v;
+  
+  switch (minor(dev))
+  {
+  case 0: // /dev/kvm
+    if (!kvm_hw)
+      return (ENXIO);
+    if (p->p_p->ps_mainproc->p_kvm)
+      return (EBUSY);
+    p->p_p->ps_mainproc->p_kvm =
+      malloc(sizeof(struct kvm_p), M_MEMDESC, M_WAITOK);
+    if (!p->p_p->ps_mainproc->p_kvm)
+      return (ENOMEM);
+    bzero(p->p_p->ps_mainproc->p_kvm, sizeof(struct kvm_p));
+    SLIST_INIT(&p->p_p->ps_mainproc->p_kvm->vp);
+    // allocate the PML4 for nested paging
+    p->p_p->ps_mainproc->p_kvm->vpml4 = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+    pmap_extract(pmap_kernel(), p->p_p->ps_mainproc->p_kvm->vpml4,
+		 &p->p_p->ps_mainproc->p_kvm->ppml4);    
+    // add the entry to the page list
+    v = malloc(sizeof(struct v_kvm_vpage), M_MEMDESC, M_WAITOK);
+    v->vp = p->p_p->ps_mainproc->p_kvm->vpml4;
+    v->pp = p->p_p->ps_mainproc->p_kvm->ppml4;
+    v->sz = PAGE_SIZE;
+    SLIST_INSERT_HEAD(&p->p_p->ps_mainproc->p_kvm->vp, v, next);
+    return (0);
+    
+  case 1: // /dev/kvm_vm
+    if (!p->p_p->ps_mainproc->p_kvm)
+      return (EBADF);
+    if (p->p_p->ps_mainproc->p_kvm->created)
+      return (EBUSY);
+    return (0);
+    
+  case 2: // /dev/kvm_vcpu
+    if (!p->p_p->ps_mainproc->p_kvm || !p->p_p->ps_mainproc->p_kvm->created)
+      return (EBADF);
+    if (p->p_kvm_vcpu)
+      return (EBUSY);
+    if (p->p_p->ps_mainproc->p_kvm->vcpu_count >= 16)
+      return (EBUSY);
+    p->p_kvm_vcpu = malloc(sizeof(struct kvm_vcpu), M_MEMDESC, M_WAITOK);
+    if (!p->p_kvm_vcpu)
+      return (ENOMEM);
+    p->p_kvm_vcpu->hw_data = kvm_hw->init(p);
+    p->p_p->ps_mainproc->p_kvm->vcpu[p->p_p->ps_mainproc->p_kvm->vcpu_count]=
+      p->p_kvm_vcpu;
+    p->p_p->ps_mainproc->p_kvm->vcpu_count++;
+    return (0);
+    
+  default:
+    return (ENODEV);
+  }
+  return (0);
+}
+
+// CLOSE
+int kvmclose(dev_t dev, int flag, int mode, struct proc *p)
+{
+  struct kvm_p *kp = p->p_p->ps_mainproc->p_kvm;
+  struct v_kvm_vpage *v;
+  int i;
+  
+  switch (minor(dev))
+  {
+  case 0: // /dev/kvm
+    if (kp)
+    {
+      kp->shutdown = 1;
+      preempt(NULL);
+      for (i = 0; i < 32; i++)
+	if (kp->mem[i].used)
+	{
+	  // unmap region
+	  kvm_map_range(p, kp, kp->mem[i].gpaddr, 0, kp->mem[i].size, 1, 0);
+	  kp->mem[i].vaddr = 0;
+	}
+      for (i = 0; i < kp->vcpu_count; i++)
+	if (kp->vcpu[i])
+	{
+	  kvm_hw->free(kp->vcpu[i]->hw_data);
+	  if (kp->vcpu[i]->kr)
+	  {
+	    km_free(kp->vcpu[i]->kr, PAGE_SIZE, &kv_any, &kp_zero);
+	    kp->vcpu[i]->kr = 0;
+	  }
+	  free(kp->vcpu[i], M_MEMDESC, sizeof(struct kvm_vcpu));
+	  kp->vcpu[i] = NULL;
+	}
+      while (!SLIST_EMPTY(&kp->vp))
+      {
+	v = SLIST_FIRST(&kp->vp);
+	SLIST_REMOVE_HEAD(&kp->vp, next);
+	uvm_km_free(kernel_map, v->vp, v->sz);
+	free(v, M_MEMDESC, sizeof(struct v_kvm_vpage));
+      }
+      free(kp, M_MEMDESC, sizeof(struct kvm_p));
+      p->p_p->ps_mainproc->p_kvm = NULL;
+    }
+    return (0);
+  case 1: // /dev/kvm_vm
+  case 2: // /dev/kvm_vcpu
+    return (0);
+
+  default:
+    return (ENODEV);
+  }
+  return (0);
+}
+
+// MMAP
+paddr_t *kvmmmap(dev_t dev, off_t offset, int prot)
+{
+  struct proc *p = curproc;
+  paddr_t pa;
+  
+  switch (minor(dev))
+  {
+  case 2: // /dev/kvm_vcpu
+    if (!offset &&
+	(p->p_kvm_vcpu) &&
+	(p->p_kvm_vcpu->kr))
+    {
+      pmap_extract(pmap_kernel(), (vaddr_t)p->p_kvm_vcpu->kr, &pa);
+      return ((void *)pa);
+    }
+    return (MAP_FAILED);
+  default:
+    return (MAP_FAILED);
+  }
+}
+
+// create a VM and returns its new FD by opening /dev/kvm_vm
+int kvm_vm_fd(struct proc *p)
+{
+  struct filedesc *fdp = p->p_fd;
+  struct nameidata nd;
+  struct file *fp;
+  int err;
+  int fd;
+
+  if (p->p_p->ps_mainproc->p_kvm->created)
+    return (-EBUSY);
+  fdplock(fdp);
+  if ((err = falloc(p, &fp, &fd)) != 0)
+  {
+    fdpunlock(fdp);
+    return (-err);
+  }
+  NDINITAT(&nd, LOOKUP, FOLLOW, UIO_SYSSPACE, AT_FDCWD, "/dev/kvm_vm", p);
+  if ((err = vn_open(&nd, FREAD|FWRITE, 0)) != 0)
+  {
+    fdremove(fdp, fd);
+    closef(fp, p);
+    fdpunlock(fdp);
+    return (-err);
+  }
+  p->p_p->ps_mainproc->p_kvm->identity_base = KVM_DFLT_IDENTITY_BASE;
+  p->p_p->ps_mainproc->p_kvm->created = 1;
+  p->p_p->ps_mainproc->p_kvm->id = vm_id++;
+  // TODO:
+  // call handlers in VMX/SVM to clear vcpu address space tlb, if used
+  fp->f_type = DTYPE_VNODE;
+  fp->f_ops = &vnops;
+  fp->f_data = nd.ni_vp;
+  fp->f_flag = FREAD|FWRITE;
+  VOP_UNLOCK(nd.ni_vp, p);
+  FILE_SET_MATURE(fp, p);
+  fdpunlock(fdp);
+  return (fd);
+}
+
+int kvm_map_range(struct proc *p, struct kvm_p *kp, u_int64_t gaddr,
+		  u_int64_t haddr, u_int64_t size, int unmap, u_int16_t prot)
+{
+  struct v_kvm_vpage *v;
+  vaddr_t vpdpt;
+  paddr_t ppdpt;
+  vaddr_t vpd;
+  paddr_t ppd;
+  paddr_t ppt;
+  vaddr_t vpt;
+  paddr_t ga;
+  vaddr_t ha;
+  paddr_t pa;
+  u_int32_t pml4e;
+  u_int32_t pdpte;
+  u_int32_t pde;
+
+  gaddr &= 0xfffffffffffff000;
+  haddr &= 0xfffffffffffff000;
+
+  pml4e = 0xffffffff;
+  pdpte = 0xffffffff;
+  pde = 0xffffffff;
+  
+  for (ga = gaddr, ha = haddr; ga < (gaddr + size);
+       ga += PAGE_SIZE, ha += PAGE_SIZE)
+  {
+    // check if we already have the vpdpt
+    if (pml4e != KVM_G2PML4E(ga))
+    {
+      pml4e = KVM_G2PML4E(ga);
+      pdpte = 0xffffffff;
+      pde = 0xffffffff;
+      // check if the pml4e exists
+      if (!*(u_int64_t *)(kp->vpml4 + KVM_G2PML4E(ga)))
+      {
+	// if we want to unmap, and the pml4e doesn't exist, just exit
+	if (unmap)
+	  return (0);
+	// create a new EPT-PDPT
+	vpdpt = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+	pmap_extract(pmap_kernel(), vpdpt, &ppdpt);
+	*(u_int64_t *)(kp->vpml4 + KVM_G2PML4E(ga)) = ppdpt | KVM_RWX;
+	// add the entry to the page list
+	v = malloc(sizeof(struct v_kvm_vpage), M_MEMDESC, M_WAITOK);
+	v->vp = vpdpt;
+	v->pp = ppdpt;
+	SLIST_INSERT_HEAD(&kp->vp, v, next);
+      }
+      else
+      {
+	vpdpt = 0;
+	// find the pdpt vaddr
+	SLIST_FOREACH(v, &kp->vp, next)
+	  if (v->pp == (*(u_int64_t *)(kp->vpml4 + KVM_G2PML4E(ga)) &
+			0xfffffffffffff000))
+	  {
+	    vpdpt = v->vp;
+	    break;
+	  }
+      }
+      if (!vpdpt)
+	return (EFAULT);
+    }
+
+    // check if we already have the vpd
+    if (pdpte != KVM_G2PDPTE(ga))
+    {
+      pdpte = KVM_G2PDPTE(ga);
+      pde = 0xffffffff;
+      // check if the pdpte exists
+      if (!*(u_int64_t *)(vpdpt + KVM_G2PDPTE(ga)))
+      {
+	// if we want to unmap, and the pdpte doesn't exist, just exit
+	if (unmap)
+	  return (0);
+	// create a new EPT-PD
+	vpd = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+	pmap_extract(pmap_kernel(), vpd, &ppd);
+	*(u_int64_t *)(vpdpt + KVM_G2PDPTE(ga)) = ppd | KVM_RWX;
+	// add the entry to the page list
+	v = malloc(sizeof(struct v_kvm_vpage), M_MEMDESC, M_WAITOK);
+	v->vp = vpd;
+	v->pp = ppd;
+	SLIST_INSERT_HEAD(&kp->vp, v, next);
+      }
+      else
+      {
+	vpd = 0;
+	// find the pd vaddr
+	SLIST_FOREACH(v, &kp->vp, next)
+	  if (v->pp == (*(u_int64_t *)(vpdpt + KVM_G2PDPTE(ga)) &
+			0xfffffffffffff000))
+	  {
+	    vpd = v->vp;
+	    break;
+	  }
+      }
+      if (!vpd)
+	return (EFAULT);
+    }
+
+    // check if we already have te vpt
+    if (pde != KVM_G2PDE(ga))
+    {
+      pde = KVM_G2PDE(ga);
+      // check if the pde exists
+      if (!*(u_int64_t *)(vpd + KVM_G2PDE(ga)))
+      {
+	// if we want to unmap, and the pde doesn't exist, just exit
+	if (unmap)
+	  return (0);
+	// create a new EPT-PT
+	vpt = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+	pmap_extract(pmap_kernel(), vpt, &ppt);
+	*(u_int64_t *)(vpd + KVM_G2PDE(ga)) = ppt | KVM_RWX;
+	// add the entry to the page list
+	v = malloc(sizeof(struct v_vmx_vpage), M_MEMDESC, M_WAITOK);
+	v->vp = vpt;
+	v->pp = ppt;
+	SLIST_INSERT_HEAD(&kp->vp, v, next);
+      }
+      else
+      {
+	vpt = 0;
+	// find the pt vaddr
+	SLIST_FOREACH(v, &kp->vp, next)
+	  if (v->pp == (*(u_int64_t *)(vpd + KVM_G2PDE(ga)) &
+			0xfffffffffffff000))
+	  {
+	    vpt = v->vp;
+	    break;
+	  }
+      }
+      if (!vpt)
+	return (EFAULT);
+    }
+
+    // map the page
+    if (!unmap)
+    {
+      // before getting the physical page address, access the page
+      // to ensure it is 1/ allocated 2/ mapped
+      pa = *(u_int64_t *)ha;
+      pmap_extract(vm_map_pmap(&p->p_vmspace->vm_map), ha, &pa);
+      // map in EPT
+      *(u_int64_t *)(vpt + KVM_G2PTE(ga)) = pa | prot | kvm_hw->get_prot();
+    }
+    else
+    {
+      // unmap in EPT
+      *(u_int64_t *)(vpt + KVM_G2PTE(ga)) = 0;
+    }
+  }
+
+  if (!unmap)
+    // lock the pages in memory
+    uvm_map_pageable(&(p->p_vmspace->vm_map), haddr, size, FALSE, 0);
+  
+  return (0);
+}
+
+int kvm_set_prot(struct kvm_p *kp, u_int64_t gaddr, u_int16_t prot)
+{
+  struct v_kvm_vpage *v;
+  vaddr_t vpdpt;
+  vaddr_t vpd;
+  vaddr_t vpt;
+
+  gaddr &= 0xfffffffffffff000;
+
+  // check if the pml4e exists
+  if (!*(u_int64_t *)(kp->vpml4 + KVM_G2PML4E(gaddr)))
+    return (-1);
+  else
+  {
+    vpdpt = 0;
+    // find the pdpt vaddr
+    SLIST_FOREACH(v, &kp->vp, next)
+      if (v->pp == (*(u_int64_t *)(kp->vpml4 + KVM_G2PML4E(gaddr)) &
+		    0xfffffffffffff000))
+      {
+	vpdpt = v->vp;
+	break;
+      }
+  }
+  if (!vpdpt)
+    return (-1);
+
+  // check if the pdpte exists
+  if (!*(u_int64_t *)(vpdpt + KVM_G2PDPTE(gaddr)))
+    return (-1);
+  else
+  {
+    vpd = 0;
+    // find the pd vaddr
+    SLIST_FOREACH(v, &kp->vp, next)
+      if (v->pp == (*(u_int64_t *)(vpdpt + KVM_G2PDPTE(gaddr)) &
+		    0xfffffffffffff000))
+      {
+	vpd = v->vp;
+	break;
+      }
+  }
+  if (!vpd)
+    return (-1);
+
+  // check if the pde exists
+  if (!*(u_int64_t *)(vpd + KVM_G2PDE(gaddr)))
+    return (-1);
+  else
+  {
+    vpt = 0;
+    // find the pt vaddr
+    SLIST_FOREACH(v, &kp->vp, next)
+      if (v->pp == (*(u_int64_t *)(vpd + KVM_G2PDE(gaddr)) &
+		    0xfffffffffffff000))
+      {
+	vpt = v->vp;
+	break;
+      }
+  }
+  if (!vpt)
+    return (-1);
+
+  *(u_int64_t *)(vpt + KVM_G2PTE(gaddr)) =
+    ((*(u_int64_t *)(vpt + KVM_G2PTE(gaddr))) & 0xfffffffffffffff8) |
+    (prot & 0b111);
+
+  return (0);
+}
+
+// check the presence or absence of an extention
+int kvm_sys_check_ext(int ext)
+{
+  switch (ext)
+  {
+  case KVM_CAP_NR_MEMSLOTS:
+    return (32);
+  case KVM_CAP_NR_VCPUS:
+    return (ncpus);
+  case KVM_CAP_MAX_VCPUS:
+    return (16);
+  case KVM_CAP_USER_MEMORY:
+  case KVM_CAP_DESTROY_MEMORY_REGION_WORKS:
+  case KVM_CAP_SET_TSS_ADDR:
+  case KVM_CAP_EXT_CPUID:
+  case KVM_CAP_MP_STATE:
+  case KVM_CAP_SET_IDENTITY_MAP_ADDR:
+  case KVM_CAP_SYNC_MMU:
+  case KVM_CAP_USER_NMI:
+  case KVM_CAP_READONLY_MEM:
+    return (1);
+  }
+  //  printf("KVM: sys check_extension %d: unavailable\n", ext);
+  return (0);
+}
+
+int kvm_get_msr_idx_lst(struct kvm_msr_list *l)
+{
+  int n;
+
+  n = 0;
+  l->indices[n++] = MSR_EFER;
+  l->indices[n++] = MSR_SYSENTER_CS;
+  l->indices[n++] = MSR_SYSENTER_ESP;
+  l->indices[n++] = MSR_SYSENTER_EIP;
+  l->indices[n++] = MSR_STAR;
+  l->indices[n++] = MSR_LSTAR;
+  l->indices[n++] = MSR_CSTAR;
+  l->indices[n++] = MSR_SFMASK;
+  l->indices[n++] = MSR_FSBASE;
+  l->indices[n++] = MSR_GSBASE;
+  l->indices[n++] = MSR_KERNELGSBASE;
+  l->indices[n++] = MSR_CR_PAT;
+  l->indices[n++] = MSR_PERF_GLOBAL_CTRL;
+  l->nmsrs = n;
+  return (0);
+}
+
+int kvm_get_supported_cpuid(struct kvm_cpuid2 *c)
+{
+  u_int32_t val;
+
+  if (c->nent < 9)
+  {
+    c->nent = 9;
+    return (-E2BIG);
+  }
+  c->nent = 9;
+  // EAX = 0
+  c->entries[0].function = 0;
+  c->entries[0].index = 0;
+  c->entries[0].flags = 0;
+  CPUID(0, c->entries[0].eax, val, val, val);
+  if (c->entries[0].eax > 7)
+    c->entries[0].eax = 7;
+  // EAX = 1
+  c->entries[1].function = 1;
+  c->entries[1].index = 0;
+  c->entries[1].flags = 0;
+  CPUID(1, c->entries[1].eax, c->entries[1].ebx, c->entries[1].ecx, c->entries[1].edx);
+  c->entries[1].ebx &= 0x00ffffff;
+  c->entries[1].ecx &= 0x62da3203;
+  c->entries[1].edx &= 0x0f8bfbff;
+  // EAX = 7, ECX = 0
+  c->entries[2].function = 7;
+  c->entries[2].index = 0;
+  c->entries[2].flags = KVM_CPUID_FLAG_SIGNIFCANT_INDEX;
+  CPUID_LEAF(7, 0, c->entries[2].eax, c->entries[2].ebx, c->entries[2].ecx,
+	c->entries[2].edx);
+  c->entries[2].ebx &= 0x1dc4b99;
+  c->entries[2].ecx &= 0x00000008;
+  c->entries[2].edx = 0;
+  // EAX = 0x80000000
+  c->entries[3].function = 0x80000000;
+  c->entries[3].index = 0;
+  c->entries[3].flags = 0;
+  CPUID(0x80000000, c->entries[3].eax, c->entries[3].ebx,
+	     c->entries[3].ecx, c->entries[3].edx);
+  if (c->entries[3].eax > 0x80000008)
+    c->entries[3].eax = 0x80000008;
+  // EAX = 0x80000001
+  c->entries[4].function = 0x80000001;
+  c->entries[4].index = 0;
+  c->entries[4].flags = 0;
+  CPUID(0x80000001, c->entries[4].eax, c->entries[4].ebx,
+	     c->entries[4].ecx, c->entries[4].edx);
+  c->entries[4].ecx &= 0x00210bf3;
+  c->entries[4].edx &= 0xefd3fbff;
+  // EAX = 0x80000002
+  c->entries[5].function = 0x80000002;
+  c->entries[5].index = 0;
+  c->entries[5].flags = 0;
+  CPUID(0x80000002, c->entries[5].eax, c->entries[5].ebx,
+	     c->entries[5].ecx, c->entries[5].edx);
+  // EAX = 0x80000003
+  c->entries[6].function = 0x80000003;
+  c->entries[6].index = 0;
+  c->entries[6].flags = 0;
+  CPUID(0x80000003, c->entries[6].eax, c->entries[6].ebx,
+	     c->entries[6].ecx, c->entries[6].edx);
+  // EAX = 0x80000004
+  c->entries[7].function = 0x80000004;
+  c->entries[7].index = 0;
+  c->entries[7].flags = 0;
+  CPUID(0x80000004, c->entries[7].eax, c->entries[7].ebx,
+	     c->entries[7].ecx, c->entries[7].edx);
+  // EAX = 0x80000008
+  c->entries[8].function = 0x80000008;
+  c->entries[8].index = 0;
+  c->entries[8].flags = 0;
+  CPUID(0x80000008, c->entries[8].eax, c->entries[8].ebx,
+	     c->entries[8].ecx, c->entries[8].edx);
+  return (0);
+}
+
+// create a VCPU and returns its new FD by opening /dev/kvm_vcpu
+int kvm_vcpu_fd(struct proc *p)
+{
+  struct filedesc *fdp = p->p_fd;
+  struct nameidata nd;
+  struct file *fp;
+  int err;
+  int fd;
+
+  if (p->p_kvm_vcpu)
+    return (-EBUSY);
+  fdplock(fdp);
+  if ((err = falloc(p, &fp, &fd)) != 0)
+  {
+    fdpunlock(fdp);
+    return (-err);
+  }
+  NDINITAT(&nd, LOOKUP, FOLLOW, UIO_SYSSPACE, AT_FDCWD, "/dev/kvm_vcpu", p);
+  if ((err = vn_open(&nd, FREAD|FWRITE, 0)) != 0)
+  {
+    fdremove(fdp, fd);
+    closef(fp, p);
+    fdpunlock(fdp);
+    return (-err);
+  }
+  p->p_kvm_vcpu->kr = km_alloc(PAGE_SIZE, &kv_any, &kp_zero, &kd_waitok);
+  if (!p->p_kvm_vcpu->kr)
+    return (-ENOMEM);
+  p->p_kvm_vcpu->lapic = KVM_DEFAULT_LAPIC_BASE | KVM_LAPIC_EN |
+    (p->p_p->ps_mainproc->p_kvm->vcpu_count == 1 ? KVM_LAPIC_BSP : 0);
+  fp->f_type = DTYPE_VNODE;
+  fp->f_ops = &vnops;
+  fp->f_data = nd.ni_vp;
+  fp->f_flag = FREAD|FWRITE;
+  VOP_UNLOCK(nd.ni_vp, p);
+  FILE_SET_MATURE(fp, p);
+  fdpunlock(fdp);
+  return (fd);
+}
+
+int kvm_set_dirty_bmap(struct kvm_p *kp, paddr_t addr)
+{
+  int i;
+
+  // find addr in mappings
+  for (i = 0; i < 32; i++)
+    if (kp->mem[i].used)
+      if ((addr >= kp->mem[i].gpaddr) &&
+	  (addr <= (kp->mem[i].gpaddr + kp->mem[i].size - 1)))
+      {
+	addr -= kp->mem[i].gpaddr;
+	addr /= PAGE_SIZE;
+	if ((addr / 64) >= (kp->mem[i].dirty_size / 8))
+	  return (-1);
+	kp->mem[i].dirty[addr / 64] |= ((u_int64_t)1 << (addr % 64));
+	break;
+      }
+
+  if (i == 32)
+    return (-1);
+  return (0);
+}
+
+void kvm_clean_dirty_bmap(struct kvm_p *kp, u_int32_t slot)
+{
+  int i;
+  int j;
+  
+  // check slot nr
+  if (slot > 31)
+    return;
+
+  // clean
+  for (i = 0; i < (kp->mem[slot].dirty_size / 8); i++)
+    if (kp->mem[slot].dirty[i])
+    {
+      for (j = 0; j < 64; j++)
+	if (kp->mem[slot].dirty[i] & ((u_int64_t)1 << j))
+	  kvm_set_prot(kp, kp->mem[slot].gpaddr +
+		       (((i * 64) + j) * PAGE_SIZE), KVM_RX);
+      kp->mem[slot].dirty[i] = 0;
+    }
+}
+
+int kvm_set_user_memory_region(struct proc *p, struct kvm_p *kp,
+			       struct kvm_userspace_memory_region *m)
+{
+  int i;
+
+  // slot nr too big
+  if (m->slot > 31)
+    return (-EFAULT);
+
+  // not a multiple of page size
+  if (m->memory_size % PAGE_SIZE)
+    return (-EFAULT);
+
+  // check that memory belongs to user region
+  if ((m->userspace_addr > VM_MAXUSER_ADDRESS) ||
+      (m->memory_size > VM_MAXUSER_ADDRESS) ||
+      (m->memory_size + m->userspace_addr > VM_MAXUSER_ADDRESS) ||
+      (m->userspace_addr < VM_MIN_ADDRESS))
+    return (-EFAULT);
+
+  // already used
+  if (kp->mem[m->slot].used)
+  {
+    // unmap
+    if (kvm_map_range(p, kp, kp->mem[m->slot].gpaddr, 0, kp->mem[m->slot].size,
+		      1, 0))
+      return (-ENXIO);
+    // free vspace
+    if (kp->mem[m->slot].vaddr)
+      kp->mem[m->slot].vaddr = 0;
+    // free bitmap
+    if (kp->mem[m->slot].dirty && kp->mem[m->slot].dirty_size)
+      free(kp->mem[m->slot].dirty, M_MEMDESC, kp->mem[m->slot].dirty_size);
+    kp->mem[m->slot].dirty = 0;
+    kp->mem[m->slot].dirty_size = 0;
+    // free entry
+    kp->mem[m->slot].used = 0;
+  }
+
+  // now we may have unregistered existing slot, check the size
+  if (m->memory_size == 0)
+    return (0);
+
+  // check for overlap
+  for (i = 0; i < 32; i++)
+    if (kp->mem[i].used)
+      if ((kp->mem[i].gpaddr <= (m->guest_phys_addr + m->memory_size - 1)) &&
+	  (m->guest_phys_addr <= (kp->mem[i].gpaddr + kp->mem[i].size - 1)))
+	return (-EBUSY);
+
+  // kernel vspace; for now == userspace pointer
+  kp->mem[m->slot].vaddr = m->userspace_addr;
+
+  // update the physical guest page table
+  if (kvm_map_range(p, kp, m->guest_phys_addr, m->userspace_addr,
+		    m->memory_size, 0,
+		    ((m->flags & KVM_MEM_LOG_DIRTY_PAGES) ||
+		     (m->flags & KVM_MEM_READONLY)) ? KVM_RX : KVM_RWX))
+    return (-ENXIO);
+  
+  // dirty logging is required
+  if (m->flags & KVM_MEM_LOG_DIRTY_PAGES)
+  {
+    // allocate bitmap
+    // size is on bytes, multiple of 8 as bitmap is u_int64_t *
+    kp->mem[m->slot].dirty_size =
+      (((m->memory_size / PAGE_SIZE) + 7) / 8) * 8;
+    kp->mem[m->slot].dirty = malloc(kp->mem[m->slot].dirty_size,
+				    M_MEMDESC, M_WAITOK);
+    if (!kp->mem[m->slot].dirty)
+    {
+      kp->mem[m->slot].vaddr = 0;
+      printf("KVM: can't allocate dirty memory bitmap\n");
+      return (-EFAULT);
+    }
+    bzero(kp->mem[m->slot].dirty, kp->mem[m->slot].dirty_size);
+  }
+  else
+  {
+    kp->mem[m->slot].dirty_size = 0;
+    kp->mem[m->slot].dirty = NULL;
+  }
+
+  // update our slot
+  kp->mem[m->slot].used = 1;
+  kp->mem[m->slot].gpaddr = m->guest_phys_addr;
+  kp->mem[m->slot].size = m->memory_size;
+  return (0);
+}
+
+int kvm_get_dirty_log(struct kvm_p *kp, struct kvm_dirty_log *l)
+{
+  size_t size;
+  int err;
+  
+  // verify slot nr
+  if (l->slot > 31)
+    return (-EFAULT);
+
+  // verify slot has a bitmap
+  if (!kp->mem[l->slot].dirty)
+    return (-1);
+
+  // size to copy
+  size = ((kp->mem[l->slot].size / PAGE_SIZE) + 7) / 8;
+
+  // copy to user and clean
+  err = copyout(kp->mem[l->slot].dirty, l->dirty_bitmap, size);
+  if (!err)
+    kvm_clean_dirty_bmap(kp, l->slot);
+  
+  return (-err);
+}
+
+u_int64_t kvm_guest_translate_32(struct kvm_p *kp, struct kvm_run *run,
+				 struct kvm_sregs *s, u_int64_t v,
+				 u_int64_t *ret)
+{
+  u_int64_t cr3;
+  int i;
+  
+  // protected mode paging
+  cr3 = s->cr3 & 0xfffff000;
+  v &= 0xffffffff;
+  
+  // search mappings for pd
+  for (i = 0; i < 32; i++)
+    if (kp->mem[i].used)
+      if ((cr3 >= kp->mem[i].gpaddr) &&
+	  (cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	break;
+  // not found
+  if (i == 32)
+  {
+    printf("KVM: can't find PD in memory mappings\n");
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 43;
+    return (1);
+  }
+  cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+  cr3 = ((u_int32_t *)(cr3))[(v >> 22) & 0x3ff];
+  if (!(cr3 & 1))
+  {
+    printf("KVM: PD entry not present in memory mappings\n");
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 44;
+    return (1);
+  }
+
+  if (cr3 & (1 << 7)) // 4MB page
+  {
+    cr3 &= 0xffc00000;
+    v = cr3 + (v & 0x3fffff);
+  }
+  else
+  {
+    cr3 &= 0xfffff000;
+    // search mappings for pt
+    for (i = 0; i < 32; i++)
+      if (kp->mem[i].used)
+	if ((cr3 >= kp->mem[i].gpaddr) &&
+	    (cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	  break;
+    // not found
+    if (i == 32)
+    {
+      printf("KVM: can't find PT in memory mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 45;
+      return (1);
+    }
+    cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+    cr3 = ((u_int32_t *)(cr3))[(v >> 12) & 0x3ff];
+    if (!(cr3 & 1))
+    {
+      printf("KVM: PT entry not present in memory mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 46;
+      return (1);
+    }
+    
+    cr3 &= 0xfffff000;
+    v = cr3 + (v & 0xfff);
+  }
+
+  *ret = v;
+  return (0);
+}
+
+u_int64_t kvm_guest_translate_pae(struct kvm_p *kp, struct kvm_run *run,
+				  struct kvm_sregs *s, u_int64_t v,
+				  u_int64_t *ret)
+{
+  u_int64_t cr3;
+  int i;
+
+  // protected mode paging
+  if (!(s->cr4 & (1 << 5)))
+    return (kvm_guest_translate_32(kp, run, s, v, ret));
+
+  // PAE protected mode paging
+  cr3 = s->cr3 & 0xffffffe0;
+  v &= 0xffffffff;
+
+  // search mappings for pdp
+  for (i = 0; i < 32; i++)
+    if (kp->mem[i].used)
+      if ((cr3 >= kp->mem[i].gpaddr) &&
+	  (cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	break;
+  // not found
+  if (i == 32)
+  {
+    printf("KVM: can't find PDP in memory mappings\n");
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 47;
+    return (1);
+  }
+  cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+  cr3 = ((u_int64_t *)(cr3))[(v >> 30) & 0x3];
+  if (!(cr3 & 1))
+  {
+    printf("KVM: PDP entry not present in memory mappings\n");
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 48;
+    return (1);
+  }
+
+  cr3 &= 0xffffff000;
+  // search mappings for pd
+  for (i = 0; i < 32; i++)
+    if (kp->mem[i].used)
+      if ((cr3 >= kp->mem[i].gpaddr) &&
+	  (cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	break;
+  // not found
+  if (i == 32)
+  {
+    printf("KVM: can't find PD in memory mappings\n");
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 49;
+    return (1);
+  }
+  cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+  cr3 = ((u_int64_t *)(cr3))[(v >> 21) & 0x1ff];
+  if (!(cr3 & 1))
+  {
+    printf("KVM: PD entry not present in memory mappings\n");
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 50;
+    return (1);
+  }
+
+  if (cr3 & (1 << 7)) // 2MB page
+  {
+    cr3 &= 0xfffe00000;
+    v = cr3 + (v & 0x1fffff);
+  }
+  else
+  {
+    cr3 &= 0xffffff000;
+    // search mappings for pt
+    for (i = 0; i < 32; i++)
+      if (kp->mem[i].used)
+	if ((cr3 >= kp->mem[i].gpaddr) &&
+	    (cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	  break;
+    // not found
+    if (i == 32)
+    {
+      printf("KVM: can't find PT in memory mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 51;
+      return (1);
+    }
+    cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+    cr3 = ((u_int64_t *)(cr3))[(v >> 12) & 0x1ff];
+    if (!(cr3 & 1))
+    {
+      printf("KVM: PAGE not present in memory mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 52;
+      return (1);
+    }
+	
+    cr3 &= 0xffffff000;
+    v = cr3 + (v & 0xfff);
+  }
+
+  *ret = v;
+  return (0);
+}
+
+u_int64_t kvm_guest_translate(struct kvm_p *kp, struct kvm_run *run,
+			      struct kvm_sregs *s, u_int64_t v, u_int64_t *ret)
+{
+  u_int64_t cr3;
+  int i;
+
+  // if paging is enabled, we have to go through guest pml4/pde
+  if (s->cr0 & (1 << 31))
+  {
+    // pae paging
+    if (!((s->efer & (1 << 10)) && s->cs.l && !s->cs.db))
+      return (kvm_guest_translate_pae(kp, run, s, v, ret));
+    
+    // long mode paging
+    cr3 = s->cr3 & 0xfffffffffffff000;
+
+    // search mappings for pml4
+    for (i = 0; i < 32; i++)
+      if (kp->mem[i].used)
+	if ((cr3 >= kp->mem[i].gpaddr) &&
+	    (cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	  break;
+    // not found
+    if (i == 32)
+    {
+      printf("KVM: can't find PML4 in memory mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 2;
+      return (1);
+    }
+    cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+    cr3 = ((u_int64_t *)(cr3))[(v >> 39) & 0x1ff];
+    if (!(cr3 & 1))
+    {
+      printf("KVM: PML4 entry not present in memory mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 3;
+      return (1);
+    }
+    
+    cr3 &= 0x000ffffffffff000;
+    // search mappings for pdp
+    for (i = 0; i < 32; i++)
+      if (kp->mem[i].used)
+	if ((cr3 >= kp->mem[i].gpaddr) &&
+	    (cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	  break;
+    // not found
+    if (i == 32)
+    {
+      printf("KVM: can't find PDP in memory mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 4;
+      return (1);
+    }
+    cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+    cr3 = ((u_int64_t *)(cr3))[(v >> 30) & 0x1ff];
+    if (!(cr3 & 1))
+    {
+      printf("KVM: PDP entry not present in memory mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 5;
+      return (1);
+    }
+    
+    if (cr3 & (1 << 7)) // 1GB page
+    {
+      cr3 &= 0xfffffc0000000;
+      v = cr3 + (v & 0x3fffffff);
+    }
+    else
+    {
+      cr3 &= 0x000ffffffffff000;
+      // search mappings for pd
+      for (i = 0; i < 32; i++)
+	if (kp->mem[i].used)
+	  if ((cr3 >= kp->mem[i].gpaddr) &&
+	      (cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	    break;
+      // not found
+      if (i == 32)
+      {
+	printf("KVM: can't find PD in memory mappings\n");
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 6;
+	return (1);
+      }
+      cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+      cr3 = ((u_int64_t *)(cr3))[(v >> 21) & 0x1ff];
+      if (!(cr3 & 1))
+      {
+	printf("KVM: PD entry not present in memory mappings\n");
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 7;
+	return (1);
+      }
+
+      if (cr3 & (1 << 7)) // 2MB page
+      {
+	cr3 &= 0xfffffffe00000;
+	v = cr3 + (v & 0x1fffff);
+      }
+      else
+      {
+	cr3 &= 0x000ffffffffff000;
+	// search mappings for pt
+	for (i = 0; i < 32; i++)
+	  if (kp->mem[i].used)
+	    if ((cr3 >= kp->mem[i].gpaddr) &&
+		(cr3 <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	      break;
+	// not found
+	if (i == 32)
+	{
+	  printf("KVM: can't find PT in memory mappings\n");
+	  run->exit_reason = KVM_EXIT_UNKNOWN;
+	  run->hw.hardware_exit_reason = 8;
+	  return (1);
+	}
+	cr3 = kp->mem[i].vaddr + (cr3 - kp->mem[i].gpaddr);
+	cr3 = ((u_int64_t *)(cr3))[(v >> 12) & 0x1ff];
+	if (!(cr3 & 1))
+	{
+	  printf("KVM: PAGE not present in memory mappings\n");
+	  run->exit_reason = KVM_EXIT_UNKNOWN;
+	  run->hw.hardware_exit_reason = 9;
+	  return (1);
+	}
+	
+	cr3 &= 0x000ffffffffff000;
+	v = cr3 + (v & 0xfff);
+      }
+    }
+  }
+  *ret = v;
+  return (0);
+}
+
+int kvm_emul_modrm16(struct kvm_p *kp, u_int8_t *modrm, u_int32_t op_size)
+{
+  struct kvm_vcpu *vcpu = curproc->p_kvm_vcpu;
+  
+  // set register
+  switch (*modrm)
+  {
+  case 0x00 ... 0x07:
+  case 0x40 ... 0x47:
+  case 0x80 ... 0x87:
+  case 0xc0 ... 0xc7:
+    vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+    break;
+
+  case 0x08 ... 0x0f:
+  case 0x48 ... 0x4f:
+  case 0x88 ... 0x8f:
+  case 0xc8 ... 0xcf:
+    vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RCX);
+    break;
+
+  case 0x10 ... 0x17:
+  case 0x50 ... 0x57:
+  case 0x90 ... 0x97:
+  case 0xd0 ... 0xd7:
+    vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDX);
+    break;
+
+  case 0x18 ... 0x1f:
+  case 0x58 ... 0x5f:
+  case 0x98 ... 0x9f:
+  case 0xd8 ... 0xdf:
+    vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBX);
+    break;
+
+  case 0x20 ... 0x27:
+  case 0x60 ... 0x67:
+  case 0xa0 ... 0xa7:
+  case 0xe0 ... 0xe7:
+    if (op_size == 1)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+    else
+    {
+      printf("KVM: unandled register RSP in modrm16\n");
+      vcpu->kr->exit_reason = KVM_EXIT_UNKNOWN;
+      vcpu->kr->hw.hardware_exit_reason = 10;
+      return (1);
+    }
+    break;
+
+  case 0x28 ... 0x2f:
+  case 0x68 ... 0x6f:
+  case 0xa8 ... 0xaf:
+  case 0xe8 ... 0xef:
+    if (op_size == 1)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RCX);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBP);
+    break;
+
+  case 0x30 ... 0x37:
+  case 0x70 ... 0x77:
+  case 0xb0 ... 0xb7:
+  case 0xf0 ... 0xf7:
+    if (op_size == 1)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDX);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RSI);
+    break;
+
+  case 0x38 ... 0x3f:
+  case 0x78 ... 0x7f:
+  case 0xb8 ... 0xbf:
+  case 0xf8 ... 0xff:
+    if (op_size == 1)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBX);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDI);
+    break;
+  }
+  // set instruction size
+  if (*modrm < 0x40)
+  {
+    if ((*modrm % 8) == 6)
+      vcpu->insn_len += 2;
+    return (0);
+  }
+  if (*modrm < 0x80)
+  {
+    vcpu->insn_len += 1;
+    return (0);
+  }
+  if (*modrm < 0xc0)
+  {
+    vcpu->insn_len += 2;
+    return (0);
+  }
+
+  return (0);
+}
+
+int kvm_emul_modrm(struct kvm_p *kp, struct kvm_run *run,
+		   u_int8_t *modrm,
+		   u_int32_t prfx, u_int64_t rip,
+		   u_int32_t op_size, struct kvm_sregs *s)
+{
+  struct kvm_vcpu *vcpu = curproc->p_kvm_vcpu;
+  
+  //  if protection not enabled, use 16 bits modrm, or protected 16bits
+  if (!(s->cr0 & 1) ||
+      (!(s->efer & (1 << 10)) && !s->cs.db) ||
+      ((s->efer & (1 << 10)) && !s->cs.l && !s->cs.db))
+    return (kvm_emul_modrm16(kp, modrm, op_size));
+  
+  switch (*modrm) // MODRM
+  {
+    // RAX / R8
+  case 0x84:
+    vcpu->insn_len++;
+  case 0x80 ... 0x83:
+  case 0x85 ... 0x87:
+  case 0x05:
+    vcpu->insn_len += 3;
+  case 0x40 ... 0x43:
+  case 0x45 ... 0x47:
+    vcpu->insn_len += 1;
+  case 0x00 ... 0x03:
+  case 0x06 ... 0x07:
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R8);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+    break;
+  case 0x04:
+    if ((modrm[1] & 0b111) == 5)
+      vcpu->insn_len += 5;
+    else
+      vcpu->insn_len++;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R8);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+    break;
+  case 0x44:
+    vcpu->insn_len += 2;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R8);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+    break;
+    
+    // RCX / R9
+  case 0x8c:
+    vcpu->insn_len++;
+  case 0x88 ... 0x8b:
+  case 0x8d ... 0x8f:
+  case 0x0d:
+    vcpu->insn_len += 3;
+  case 0x48 ... 0x4b:
+  case 0x4d ... 0x4f:
+    vcpu->insn_len += 1;
+  case 0x08 ... 0x0b:
+  case 0x0e ... 0x0f:
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R9);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RCX);
+    break;
+  case 0x0c:
+    if ((modrm[1] & 0b111) == 5)
+      vcpu->insn_len += 5;
+    else
+      vcpu->insn_len++;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R9);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RCX);
+    break;
+  case 0x4c:
+    vcpu->insn_len += 2;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R9);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RCX);
+    break;
+
+    // RDX / R10
+  case 0x94:
+    vcpu->insn_len++;
+  case 0x90 ... 0x93:
+  case 0x95 ... 0x97:
+  case 0x15:
+    vcpu->insn_len += 3;
+  case 0x50 ... 0x53:
+  case 0x55 ... 0x57:
+    vcpu->insn_len += 1;
+  case 0x10 ... 0x13:
+  case 0x16 ... 0x17:
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R10);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDX);
+    break;
+  case 0x14:
+    if ((modrm[1] & 0b111) == 5)
+      vcpu->insn_len += 5;
+    else
+      vcpu->insn_len++;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R10);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDX);
+    break;
+  case 0x54:
+    vcpu->insn_len += 2;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R10);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDX);
+    break;
+    
+    // RBX / R11
+  case 0x9c:
+    vcpu->insn_len++;
+  case 0x98 ... 0x9b:
+  case 0x9d ... 0x9f:
+  case 0x1d:
+    vcpu->insn_len += 3;
+  case 0x58 ... 0x5b:
+  case 0x5d ... 0x5f:
+    vcpu->insn_len += 1;
+  case 0x18 ... 0x1b:
+  case 0x1e ... 0x1f:
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R11);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBX);
+    break;
+  case 0x1c:
+    if ((modrm[1] & 0b111) == 5)
+      vcpu->insn_len += 5;
+    else
+      vcpu->insn_len++;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R11);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBX); 
+    break;
+  case 0x5c:
+    vcpu->insn_len += 2;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R11);
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBX); 
+    break;
+
+    // RSP / R12
+  case 0xa4:
+    vcpu->insn_len++;
+  case 0xa0 ... 0xa3:
+  case 0xa5 ... 0xa7:
+  case 0x25:
+    vcpu->insn_len += 3;
+  case 0x60 ... 0x63:
+  case 0x65 ... 0x67:
+    vcpu->insn_len += 1;
+  case 0x20 ... 0x23:
+  case 0x26 ... 0x27:
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R12);
+    else
+    {
+      if (op_size != 1)
+      {
+	printf("KVM: unandled register RSP in modrm\n");
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 11;
+	return (1);
+      }
+      if ((prfx & PRFX_REX) || (prfx & PRFX_REXW) ||
+	  (prfx & PRFX_REXX) || (prfx & PRFX_REXB))
+      {
+	printf("KVM: unandled register RSP in modrm\n");
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 12;
+	return (1);
+      }
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX) + 1;
+    }
+    break;
+  case 0x24:
+    if ((modrm[1] & 0b111) == 5)
+      vcpu->insn_len += 5;
+    else
+      vcpu->insn_len++;
+    if (op_size != 1)
+    {
+      printf("KVM: unandled register RSP in modrm\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 13;
+      return (1);
+    }
+    if ((prfx & PRFX_REX) || (prfx & PRFX_REXW) ||
+	(prfx & PRFX_REXX) || (prfx & PRFX_REXB))
+    {
+      printf("KVM: unandled register RSP in modrm\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 14;
+      return (1);
+    }
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX) + 1;
+    break;
+  case 0x64:
+    vcpu->insn_len += 2;
+    if (op_size != 1)
+    {
+      printf("KVM: unandled register RSP in modrm\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 15;
+      return (1);
+    }
+    if ((prfx & PRFX_REX) || (prfx & PRFX_REXW) ||
+	(prfx & PRFX_REXX) || (prfx & PRFX_REXB))
+    {
+      printf("KVM: unandled register RSP in modrm\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 16;
+      return (1);
+    }
+    else
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX) + 1;
+    break;
+
+    // RBP / R13
+  case 0xac:
+    vcpu->insn_len++;
+  case 0xa8 ... 0xab:
+  case 0xad ... 0xaf:
+  case 0x2d:
+    vcpu->insn_len += 3;
+  case 0x68 ... 0x6b:
+  case 0x6d ... 0x6f:
+    vcpu->insn_len += 1;
+  case 0x28 ... 0x2b:
+  case 0x2e ... 0x2f:
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R13);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RCX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBP);
+    break;
+  case 0x2c:
+    if ((modrm[1] & 0b111) == 5)
+      vcpu->insn_len += 5;
+    else
+      vcpu->insn_len++;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R13);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RCX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBP);
+    break;
+  case 0x6c:
+    vcpu->insn_len += 2;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R13);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RCX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBP);
+    break;
+    
+    // RSI / R14
+  case 0xb4:
+    vcpu->insn_len++;
+  case 0xb0 ... 0xb3:
+  case 0xb5 ... 0xb7:
+  case 0x35:
+    vcpu->insn_len += 3;
+  case 0x70 ... 0x73:
+  case 0x75 ... 0x77:
+    vcpu->insn_len += 1;
+  case 0x30 ... 0x33:
+  case 0x36 ... 0x37:
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R14);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RSI);
+    break;
+  case 0x34:
+    if ((modrm[1] & 0b111) == 5)
+      vcpu->insn_len += 5;
+    else
+      vcpu->insn_len++;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R14);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RSI);
+    break;
+  case 0x74:
+    vcpu->insn_len += 2;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R14);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RSI);
+    break;
+
+    // RDI / R15
+  case 0xbc:
+    vcpu->insn_len++;
+  case 0xb8 ... 0xbb:
+  case 0xbd ... 0xbf:
+  case 0x3d:
+    vcpu->insn_len += 3;
+  case 0x78 ... 0x7b:
+  case 0x7d ... 0x7f:
+    vcpu->insn_len += 1;
+  case 0x38 ... 0x3b:
+  case 0x3e ... 0x3f:
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R15);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDI);
+    break;
+  case 0x3c:
+    if ((modrm[1] & 0b111) == 5)
+      vcpu->insn_len += 5;
+    else
+      vcpu->insn_len++;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R15);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDI);
+    break;
+  case 0x7c:
+    vcpu->insn_len += 2;
+    if (prfx & PRFX_REXR)
+      vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_R15);
+    else
+      if (!(prfx & PRFX_REX) && !(prfx & PRFX_REXW) &&
+	  !(prfx & PRFX_REXX) && !(prfx & PRFX_REXB) &&
+	  (op_size == 1))
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RBX) + 1;
+      else
+	vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RDI);
+    break;
+    
+  default: // modrm not handled
+    printf("KVM: insn decoder unknown modrm @%llx [%02x]\n",
+	   rip, *modrm);
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 17;
+    return (1);
+  }
+
+  return (0);
+}
+
+void kvm_emul_prefix_decode(u_int8_t **insn, u_int32_t *prfx)
+{
+  while (1)
+  {
+    if (**insn == 0x26) // ES
+    {
+      *prfx |= PRFX_SEG_ES;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x40) // REX
+    {
+      *prfx |= PRFX_REX;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x41) // REX.B
+    {
+      *prfx |= PRFX_REXB;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x42) // REX.X
+    {
+      *prfx |= PRFX_REXX;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x43) // REX.XB
+    {
+      *prfx |= PRFX_REXX | PRFX_REXB;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x44) // REX.R
+    {
+      *prfx |= PRFX_REXR;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x45) // REX.RB
+    {
+      *prfx |= PRFX_REXR | PRFX_REXB;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x46) // REX.RX
+    {
+      *prfx |= PRFX_REXR | PRFX_REXX;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x47) // REX.RXB
+    {
+      *prfx |= PRFX_REXR | PRFX_REXX | PRFX_REXB;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x48) // REX.W
+    {
+      *prfx |= PRFX_REXW;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x49) // REX.WB
+    {
+      *prfx |= PRFX_REXW | PRFX_REXB;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x4a) // REX.WX
+    {
+      *prfx |= PRFX_REXW | PRFX_REXX;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x4b) // REX.WXB
+    {
+      *prfx |= PRFX_REXW | PRFX_REXX | PRFX_REXB;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x4c) // REX.WR
+    {
+      *prfx |= PRFX_REXW | PRFX_REXR;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x4d) // REX.WRB
+    {
+      *prfx |= PRFX_REXW | PRFX_REXR | PRFX_REXB;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x4e) // REX.WRX
+    {
+      *prfx |= PRFX_REXW | PRFX_REXR | PRFX_REXX;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x4f) // REX.WRXB
+    {
+      *prfx |= PRFX_REXW | PRFX_REXR | PRFX_REXX | PRFX_REXB;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x66) // OPSIZE
+    {
+      *prfx |= PRFX_OPSIZE;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0x67) // ADDRSIZE
+    {
+      *prfx |= PRFX_ADDRSIZE;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0xf2) // REP
+    {
+      *prfx |= PRFX_REP;
+      (*insn)++;
+      continue;
+    }
+    if (**insn == 0xf3) // REP
+    {
+      *prfx |= PRFX_REP;
+      (*insn)++;
+      continue;
+    }
+    break;
+  }
+}
+
+void kvm_emul_get_size(struct kvm_p *kp, struct kvm_sregs *s,
+		       u_int32_t *op_size, u_int32_t *addr_size,
+		       u_int32_t prfx)
+{
+  struct kvm_vcpu *vcpu = curproc->p_kvm_vcpu;
+  
+  // size
+  if (s->cr0 & 1) // PE
+  {
+    if ((s->efer & (1 << 10)) && s->cs.l)
+    {
+      *op_size = vcpu->mmio.size = 4;
+      *addr_size = 8;
+    }
+    else
+    {
+      if (s->cs.db) // default size
+	*op_size = *addr_size = vcpu->mmio.size = 4;
+      else
+	*op_size = *addr_size = vcpu->mmio.size = 2;
+    }
+  }
+  else
+    *op_size = *addr_size = vcpu->mmio.size = 2;
+
+  if (prfx & PRFX_OPSIZE)
+  {
+    if (*op_size == 2)
+      *op_size = vcpu->mmio.size = 4;
+    else
+      if (*op_size == 4)
+	*op_size = vcpu->mmio.size = 2;
+  }
+
+  if (prfx & PRFX_ADDRSIZE)
+  {
+    if (*addr_size == 2)
+      *addr_size = 4;
+    else
+      if (*addr_size == 4)
+	*addr_size = 2;
+      else
+	if (*addr_size == 8)
+	  *addr_size = 4;
+  }
+  
+  if (prfx & PRFX_REXW)
+    vcpu->mmio.size = *op_size = 8;
+}
+
+void kvm_handle_mmio(struct kvm_run *run, struct kvm_p *kp)
+{
+  struct kvm_vcpu *vcpu = curproc->p_kvm_vcpu;
+  struct kvm_sregs s;
+  struct kvm_regs r;
+  u_int32_t addr_size;
+  u_int32_t op_size;
+  u_int32_t prfx;
+  u_int64_t rip;
+  u_int8_t *insn;
+  u_int64_t val;
+  u_int64_t val2;
+  int64_t signext;
+  int i;
+  int j;
+
+  kvm_hw->get_sregs(vcpu->hw_data, &s, kp);
+  kvm_hw->get_regs(vcpu->hw_data, &r);
+  
+  // instruction fetch is an error
+  if (vcpu->mmio.acc & 0b100)
+  {
+    printf("KVM: trying to execute outside of guest RAM @%llx\n",
+	   vcpu->mmio.gpaddr);
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 18;
+    return;
+  }
+  
+  // check for a write to a RX page (for dirty log / RO memory)
+  if ((vcpu->mmio.acc & 0b10) && (vcpu->mmio.acc & 1))
+  {
+    // mark it dirty in the bitmap
+    if (!kvm_set_dirty_bmap(kp, vcpu->mmio.gpaddr))
+    {
+      // set the page writeable if dirty logging
+      kvm_set_prot(kp, vcpu->mmio.gpaddr, KVM_RWX);
+      return;
+    }
+    // return to userland for RO memory write
+  }
+
+  // TODO: use provided insn buffer on SVM
+  // instruction is at CS:RIP
+  rip = s.cs.base + r.rip;
+
+  // check if we need to translate guest paging
+  if (kvm_guest_translate(kp, run, &s, rip, &rip))
+    return;
+
+  // search mappings
+  for (i = 0; i < 32; i++)
+    if (kp->mem[i].used)
+      if ((rip >= kp->mem[i].gpaddr) &&
+	  (rip <= (kp->mem[i].gpaddr + kp->mem[i].size)))
+	break;
+  
+  // not found
+  if (i == 32)
+  {
+    printf("KVM: can't find RIP in memory mappings\n");
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 19;
+    return;
+  }
+
+  // our instruction pointer
+  insn = (void *)(kp->mem[i].vaddr + (rip - kp->mem[i].gpaddr));
+
+  // direction
+  vcpu->mmio.dir = (vcpu->mmio.acc >> 1) & 1;
+  
+  // instruction decoding
+  prfx = 0;
+  kvm_emul_prefix_decode(&insn, &prfx);
+  kvm_emul_get_size(kp, &s, &op_size, &addr_size, prfx);
+  vcpu->mmio.flag = KVM_MMIO_MOV;
+  
+  switch (*insn)
+  {
+  case 0x0a: // OR R8, [R/M8]
+    if (vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a read, exit is a write\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 21;
+      return;
+    }
+    vcpu->insn_len = 2;
+    vcpu->mmio.flag = KVM_MMIO_OR;
+    op_size = vcpu->mmio.size = 1;
+    if (kvm_emul_modrm(kp, run, insn + 1, prfx, rip, op_size, &s))
+      return;
+    break;
+    
+  case 0x0f:
+    switch (insn[1])
+    {
+    case 0xb6: // MOVZX R32, [R/M8]
+      vcpu->mmio.size = 1;
+    case 0xb7: // MOVZX R32, [R/M16]
+      if (vcpu->mmio.dir)
+      {
+	printf("KVM: opcode @%llx [0f%02x] is a read, exit is a write\n",
+	       rip, insn[1]);
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 22;
+	return;
+      }
+      vcpu->insn_len = 3;
+      if (kvm_emul_modrm(kp, run, insn + 2, prfx, rip, op_size, &s))
+	return;
+      if (op_size == 8)
+	*(u_int64_t *)(vcpu->mmio.ptr) = 0;
+      if (op_size == 4)
+	*(u_int32_t *)(vcpu->mmio.ptr) = 0;
+      if (op_size == 2)
+	*(u_int16_t *)(vcpu->mmio.ptr) = 0;
+      break;
+    default:
+      printf("KVM: insn decoder unknown opcode @%llx "
+	     "after extension 0f [%02x]\n",
+	     rip, insn[1]);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 23;
+      return;
+    }
+    break;
+    
+  case 0x23: // AND R, [OFF16/32]
+    if (vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a read, exit is a write\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 24;
+      return;
+    }
+    vcpu->insn_len = 2;
+    vcpu->mmio.flag = KVM_MMIO_AND;
+    if (kvm_emul_modrm(kp, run, insn + 1, prfx, rip, op_size, &s))
+      return;
+    break;
+
+  case 0x81:
+    if (vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a read, exit is a write\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 25;
+      return;
+    }
+    switch ((insn[1] >> 3) & 0x7)
+    {
+    case 7: // CMP [R], IMM32
+      vcpu->insn_len = 2 + op_size;
+      vcpu->mmio.flag = KVM_MMIO_CMP;
+      vcpu->mmio.ptr = (void *)(insn + 2);
+      break;
+    default:
+      printf("KVM: insn decoder unknown opcode @%llx [%02x%02x]\n",
+	     rip, *insn, insn[1]);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 26;
+      return;
+    }
+    break;
+
+  case 0x83:
+    if (!vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a write, exit is a read\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 25;
+      return;
+    }
+    switch ((insn[1] >> 3) & 0x7)
+    {
+    case 1: // OR [R], IMM8
+      vcpu->insn_len = 3;
+      vcpu->mmio.flag = KVM_MMIO_OR | KVM_MMIO_TO_MMIO;
+      vcpu->mmio.ptr2 = *(u_int8_t *)(insn + 2);
+      vcpu->mmio.ptr = &(vcpu->mmio.ptr2);
+      vcpu->mmio.dir = 0;
+      break;
+    default:
+      printf("KVM: insn decoder unknown opcode @%llx [%02x%02x]\n",
+	     rip, *insn, insn[1]);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 27;
+      return;
+    }
+    break;
+    
+  case 0x88: // MOV [R/IMM8], R
+    op_size = vcpu->mmio.size = 1;
+  case 0x89: // MOV [R/IMM16/32], R
+    if (!vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a write, exit is a read\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 28;
+      return;
+    }
+    vcpu->insn_len = 2;
+    if (kvm_emul_modrm(kp, run, insn + 1, prfx, rip, op_size, &s))
+      return;
+    break;
+
+  case 0x8a: // MOV R8, [R/M8]
+    op_size = vcpu->mmio.size = 1;
+  case 0x8b: // MOV R, [IMM16/32]
+    if (vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a read, exit is a write\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 30;
+      return;
+    }
+    vcpu->insn_len = 2;
+    if (kvm_emul_modrm(kp, run, insn + 1, prfx, rip, op_size, &s))
+      return;
+    break;
+
+  case 0xa1: // MOV [E]AX, [IMM16/32]
+    if (vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a read, exit is a write\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 31;
+      return;
+    }
+    vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+    vcpu->insn_len = addr_size + 1;
+    break;
+    
+  case 0xa3: // MOV [OFF16/32/64], [R]AX
+    if (!vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a write, exit is a read\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 32;
+      return;
+    }
+    vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+    vcpu->insn_len = addr_size + 1;
+    break;
+
+  case 0xa4: // MOVSB
+    op_size = vcpu->mmio.size = 1;
+  case 0xa5: // MOVSL
+    val = s.ds.base + (r.rsi & ((addr_size == 2) ? 0xffff :
+				((addr_size == 4) ? 0xffffffff :
+				 0xffffffffffffffff)));
+    val2 = s.es.base + (r.rdi & ((addr_size == 2) ? 0xffff :
+				 ((addr_size == 4) ? 0xffffffff :
+				  0xffffffffffffffff)));
+    if (kvm_guest_translate(kp, run, &s, val, &val))
+      return;
+    if (kvm_guest_translate(kp, run, &s, val2, &val2))
+      return;    
+    // search mappings
+    for (i = 0; i < 32; i++)
+      if (kp->mem[i].used)
+	if ((val >= kp->mem[i].gpaddr) &&
+	    (val < (kp->mem[i].gpaddr + kp->mem[i].size)))
+	  break;
+    for (j = 0; j < 32; j++)
+      if (kp->mem[j].used)
+	if ((val2 >= kp->mem[j].gpaddr) &&
+	    (val2 < (kp->mem[j].gpaddr + kp->mem[j].size)))
+	  break;
+    // MMIO to MMIO
+    if ((i == 32) && (j == 32))
+    {
+      if (vcpu->mmio.dir)
+      {
+	printf("KVM: exit is a write during MMIO to MMIO MOVS copy\n");
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 33;
+	return;
+      }
+      // MMIO to MMIO copy
+      vcpu->mmio.flag = KVM_MMIO_TO_MMIO;
+      vcpu->mmio.ptr2 = val2;
+    }
+    // MMIO read
+    else if (i == 32)
+    {
+      if (vcpu->mmio.dir)
+      {
+	printf("KVM: exit is a write insn is a read\n");
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 53;
+	return;
+      }
+      vcpu->mmio.ptr = (void *)(kp->mem[j].vaddr +
+				   (val2 - kp->mem[j].gpaddr));
+      vcpu->mmio.flag = KVM_MMIO_MOV;
+    }
+    // MMIO write
+    else if (j == 32)
+    {
+      if (!vcpu->mmio.dir)
+      {
+	printf("KVM: exit is a read insn is a write\n");
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 54;
+	return;
+      }
+      vcpu->mmio.ptr = (void *)(kp->mem[i].vaddr +
+				   (val - kp->mem[i].gpaddr));
+      vcpu->mmio.flag = KVM_MMIO_MOV;
+    }
+    // increment or decrement esi edi
+    if (r.rflags & (1 << 10))
+    {
+      r.rsi -= vcpu->mmio.size;
+      r.rdi -= vcpu->mmio.size;
+    }
+    else
+    {
+      r.rsi += vcpu->mmio.size;
+      r.rdi += vcpu->mmio.size;
+    }
+    kvm_hw->set_regs(vcpu->hw_data, &r);
+    vcpu->insn_len = 1;
+    break;
+
+  case 0xab: // STOS
+    if (!vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a write, exit is a read\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 34;
+      return;
+    }
+    vcpu->mmio.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+    // increment or decrement RDI
+    if (r.rflags & (1 << 10))
+      r.rdi -= vcpu->mmio.size;
+    else
+      r.rdi += vcpu->mmio.size;
+    kvm_hw->set_regs(vcpu->hw_data, &r);
+    vcpu->insn_len = 1;
+    break;
+
+  case 0xc6: // MOV [OFF16/32], IMM8
+    op_size = vcpu->mmio.size = 1;
+  case 0xc7: // MOV [OFF16/32], IMM16/32
+    if (!vcpu->mmio.dir)
+    {
+      printf("KVM: opcode @%llx [%02x] is a write, exit is a read\n",
+	     rip, *insn);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 35;
+      return;
+    }
+    val = (vcpu->mmio.size != 8) ? vcpu->mmio.size : 4;
+    vcpu->insn_len = 2 + val;
+    if (kvm_emul_modrm(kp, run, insn + 1, prfx, rip, op_size, &s))
+      return;
+    vcpu->mmio.ptr = (void *)(insn + vcpu->insn_len - val);
+    if (prfx & PRFX_REXW)
+    {
+      signext = *(int32_t *)(insn + vcpu->insn_len - val);
+      vcpu->mmio.ptr = (void *)&signext;
+    }
+    break;
+    
+  default: // opcode not handled yet
+    printf("KVM: insn decoder unknown opcode @%llx [%02x] %llx\n",
+	   rip, *insn, vcpu->mmio.gpaddr);
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = 36;
+    return;
+  }
+
+  if (prfx & PRFX_SEG_ES)
+    vcpu->insn_len++;
+
+  if (prfx & PRFX_ADDRSIZE)
+    vcpu->insn_len++;
+
+  if (prfx & PRFX_OPSIZE)
+    vcpu->insn_len++;
+
+  if ((prfx & PRFX_REXR) || (prfx & PRFX_REXB) || (prfx & PRFX_REXW) ||
+      (prfx & PRFX_REXX) || (prfx & PRFX_REX))
+    vcpu->insn_len++;
+
+  if (prfx & PRFX_REP)
+  {
+    switch (addr_size)
+    {
+    case 2:
+      (*(u_int16_t *)(&r.rcx))--;
+      if (!(r.rcx & 0xffff))
+	vcpu->insn_len++;
+      else
+	vcpu->insn_len = 0;
+      break;
+    case 4:
+      (*(u_int32_t *)(&r.rcx))--;
+      if (!(r.rcx & 0xffffffff))
+	vcpu->insn_len++;
+      else
+	vcpu->insn_len = 0;
+    case 8:
+      (*(u_int64_t *)(&r.rcx))--;
+      if (!r.rcx)
+	vcpu->insn_len++;
+      else
+	vcpu->insn_len = 0;
+      break;
+    }
+    kvm_hw->set_regs(vcpu->hw_data, &r);
+  }
+
+  vcpu->mmio.flag |= KVM_MMIO_EN;
+  run->exit_reason = KVM_EXIT_MMIO;
+  run->mmio.phys_addr = vcpu->mmio.gpaddr;
+  run->mmio.len = vcpu->mmio.size;
+  run->mmio.is_write = vcpu->mmio.dir;
+
+  // write
+  if (vcpu->mmio.dir)
+    switch (vcpu->mmio.size)
+    {
+    case 1:
+      *(u_int8_t *)(&(run->mmio.data)) = *(u_int8_t *)(vcpu->mmio.ptr);
+      break;
+    case 2:
+      *(u_int16_t *)(&(run->mmio.data)) = *(u_int16_t *)(vcpu->mmio.ptr);
+      break;
+    case 4:
+      *(u_int32_t *)(&(run->mmio.data)) = *(u_int32_t *)(vcpu->mmio.ptr);
+      break;
+    case 8:
+      *(u_int64_t *)(&(run->mmio.data)) = *(u_int64_t *)(vcpu->mmio.ptr);
+      break;
+    }
+  else
+    *(u_int64_t *)(&(run->mmio.data)) = 0;
+
+  return;
+}
+
+void kvm_handle_io(void *hw, struct kvm_run *run, struct kvm_p *kp,
+		   u_int64_t off)
+{
+  struct kvm_vcpu *vcpu = curproc->p_kvm_vcpu;
+  struct kvm_sregs s;
+  struct kvm_regs r;
+  u_int64_t val;
+  int i;
+
+  kvm_hw->get_sregs(vcpu->hw_data, &s, kp);
+  kvm_hw->get_regs(vcpu->hw_data, &r);
+  
+  // REP prefix ?
+  if (vcpu->io.rep)
+  {
+    if (vcpu->io.str)
+    {
+      if (off)
+	run->io.count -= (off / vcpu->io.size);
+      else
+	run->io.count = (u_int32_t)r.rcx;
+    }
+    else
+    {
+      printf("KVM: unhandled rep I/O operation on non-string opcode\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 37;
+      return;
+    }
+  }
+  else
+    run->io.count = 1;
+  vcpu->io.count = run->io.count;
+  
+  // check IO size
+  if ((vcpu->io.count * vcpu->io.size) > sizeof(run->s))
+  {
+    run->io.count = sizeof(run->s) / vcpu->io.size;
+    vcpu->io.count -= run->io.count;
+  }
+  else
+    vcpu->io.count = 0;
+
+  // string operation
+  if (vcpu->io.str)
+  {
+    if (vcpu->io.dir == KVM_EXIT_IO_IN)
+    {
+      // operation is at ES:(E)DI
+      val = s.es.base;
+      switch (vcpu->io.addr_size)
+      {
+      case 2:
+	val += (r.rdi & 0xffff);
+	break;
+      case 4:
+	val += (r.rdi & 0xffffffff);
+	break;
+      case 8:
+	val += r.rdi;
+	break;
+      default:
+	printf("KVM: unhandled I/O addr size %x\n", vcpu->io.addr_size);
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 38;
+	return;
+      }
+    }
+    else
+    {
+      // operation is at SEG:(E)SI
+      switch (vcpu->io.seg)
+      {
+      case 0:
+	val = s.es.base;
+	break;
+      case 1:
+	val = s.cs.base;
+	break;
+      case 2:
+	val = s.ss.base;
+	break;
+      case 3:
+	val = s.ds.base;
+	break;
+      case 4:
+	val = s.fs.base;
+	break;
+      case 5:
+	val = s.gs.base;
+	break;
+      default:
+	printf("KVM: unhandled segment %x\n", vcpu->io.seg);
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 39;
+	return;
+      }
+      
+      // add (E)SI
+      switch (vcpu->io.addr_size)
+      {
+      case 2:
+	val += (r.rsi & 0xffff);
+	break;
+      case 4:
+	val += (r.rsi & 0xffffffff);
+	break;
+      case 8:
+	val += r.rsi;
+	break;	
+      default:
+	printf("KVM: unhandled I/O addr size %x\n", vcpu->io.addr_size);
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 40;
+	return;
+      }
+    }
+
+    // add offset from precedent un-finished operation (if any)
+    val += off;
+    
+    // check if we need to translate with guest page tables
+    if (kvm_guest_translate(kp, run, &s, val, &val))
+      return;
+
+    // search the address in our memory mappings
+    for (i = 0; i < 32; i++)
+      if (kp->mem[i].used)
+	if ((val >= kp->mem[i].gpaddr) &&
+	    (val < (kp->mem[i].gpaddr + kp->mem[i].size)))
+	  break;
+    
+    // not found
+    if (i == 32)
+    {
+      printf("KVM: ptr to I/O string operation not in mappings\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 41;
+      return;
+    }
+    
+    // check for overflow
+    if ((val + (run->io.count * vcpu->io.size))
+	> (kp->mem[i].gpaddr + kp->mem[i].size))
+    {
+      printf("KVM: I/O string operation would overflow memory mapping\n");
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = 42;
+      return;
+    }
+    
+    vcpu->io.ptr = (void *)(kp->mem[i].vaddr +
+			       (val - kp->mem[i].gpaddr));    
+  }
+  else
+    vcpu->io.ptr = kvm_hw->get_reg_ptr(vcpu->hw_data, KVM_REG_RAX);
+
+  // if out, do the operation
+  if (vcpu->io.dir == KVM_EXIT_IO_OUT)
+    for (i = 0; i < run->io.count; i++)
+      switch (vcpu->io.size)
+      {
+      case 1:
+	*(u_int8_t *)vcpu->io.user_ptr = *(u_int8_t *)vcpu->io.ptr;
+	vcpu->io.user_ptr++;
+	vcpu->io.ptr++;
+	break;
+      case 2:
+	*(u_int16_t *)vcpu->io.user_ptr = *(u_int16_t *)vcpu->io.ptr;
+	vcpu->io.user_ptr += 2;
+	vcpu->io.ptr += 2;
+	break;
+      case 4:
+	*(u_int32_t *)vcpu->io.user_ptr = *(u_int32_t *)vcpu->io.ptr;
+	vcpu->io.user_ptr += 4;
+	vcpu->io.ptr += 4;
+	break;
+      default:
+	// size unhandled
+	printf("KVM: I/O op size not handled (%lld)\n", vcpu->io.size);
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = 43;
+	break;
+      }
+}
+
+int kvm_handle_pre_io(struct kvm_run *run, struct kvm_p *kp)
+{
+  struct kvm_vcpu *vcpu = curproc->p_kvm_vcpu;
+  int i;
+
+  vcpu->io.flag = 0;
+  if (vcpu->io.dir == KVM_EXIT_IO_IN)
+  {
+    if (run->exit_reason != KVM_EXIT_IO)
+    {
+      printf("KVM: ERR userland did not fulfill IO access\n");
+      return (-ENXIO);
+    }
+    for (i = 0; i < run->io.count; i++)
+    {
+      switch (vcpu->io.size)
+      {
+      case 1:
+	*(u_int8_t *)vcpu->io.ptr = *(u_int8_t *)vcpu->io.user_ptr;
+	vcpu->io.user_ptr++;
+	vcpu->io.ptr++;
+	break;
+      case 2:
+	*(u_int16_t *)vcpu->io.ptr = *(u_int16_t *)vcpu->io.user_ptr;
+	vcpu->io.user_ptr += 2;
+	vcpu->io.ptr += 2;
+	break;
+      case 4:
+	*(u_int32_t *)vcpu->io.ptr = *(u_int32_t *)vcpu->io.user_ptr;
+	vcpu->io.user_ptr += 4;
+	vcpu->io.ptr += 4;
+	break;
+      default:
+	// size unhandled
+	printf("KVM: prerun I/O op size not handled (%d)\n", run->io.size);
+	return (-ENXIO);
+      }
+    }
+  }
+
+  // operation not finished
+  if (vcpu->io.count)
+    return (1);
+  
+  return (0);
+}
+
+int kvm_handle_pre_mmio(struct kvm_run *run, struct kvm_p *kp)
+{
+  struct kvm_vcpu *vcpu = curproc->p_kvm_vcpu;
+  struct kvm_regs r;
+  u_int64_t flags;
+
+  if (vcpu->mmio.dir == 0)
+  {
+    if (run->exit_reason != KVM_EXIT_MMIO)
+    {
+      printf("KVM: ERR userland did not fulfill MMIO access\n");
+      return (-ENXIO);
+    }
+    switch (vcpu->mmio.flag)
+    {
+    case KVM_MMIO_EN | KVM_MMIO_AND: // AND
+      switch (vcpu->mmio.size)
+      {
+      case 1:
+	asm volatile("mov (%%rbx), %%dl;"
+		     "and %%dl, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 2:
+	asm volatile("mov (%%rbx), %%dx;"
+		     "and %%dx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 4:
+	asm volatile("mov (%%rbx), %%edx;"
+		     "and %%edx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 8:
+	asm volatile("mov (%%rbx), %%rdx;"
+		     "and %%rdx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      }
+      kvm_hw->get_regs(vcpu->hw_data, &r);
+      r.rflags &= KVM_FLAGS_CLR_MASK;
+      r.rflags |= (flags & KVM_FLAGS_SET_MASK);
+      kvm_hw->set_regs(vcpu->hw_data, &r);
+      break;
+      
+    case KVM_MMIO_EN | KVM_MMIO_CMP: // CMP
+      switch (vcpu->mmio.size)
+      {
+      case 1:
+	asm volatile("mov (%%rbx), %%dl;"
+		     "cmp %%dl, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 2:
+	asm volatile("mov (%%rbx), %%dx;"
+		     "cmp %%dx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 4:
+	asm volatile("mov (%%rbx), %%edx;"
+		     "cmp %%edx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 8:
+	asm volatile("mov (%%rbx), %%rdx;"
+		     "cmp %%rdx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      }
+      kvm_hw->get_regs(vcpu->hw_data, &r);
+      r.rflags &= KVM_FLAGS_CLR_MASK;
+      r.rflags |= (flags & KVM_FLAGS_SET_MASK);
+      kvm_hw->set_regs(vcpu->hw_data, &r);
+      break;
+
+    case KVM_MMIO_EN | KVM_MMIO_OR | KVM_MMIO_TO_MMIO: // OR
+    case KVM_MMIO_EN | KVM_MMIO_OR: // OR
+      switch (vcpu->mmio.size)
+      {
+      case 1:
+	asm volatile("mov (%%rbx), %%dl;"
+		     "or %%dl, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 2:
+	asm volatile("mov (%%rbx), %%dx;"
+		     "or %%dx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 4:
+	asm volatile("mov (%%rbx), %%edx;"
+		     "or %%edx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      case 8:
+	asm volatile("mov (%%rbx), %%rdx;"
+		     "or %%rdx, (%%rax);"
+		     "pushf;"
+		     "pop %%rax;" :
+		     "=a"(flags) :
+		     "a"(vcpu->mmio.ptr), "b"(&(run->mmio.data)) :
+		     "rdx", "memory", "cc");
+	break;
+      }
+      kvm_hw->get_regs(vcpu->hw_data, &r);
+      r.rflags &= KVM_FLAGS_CLR_MASK;
+      r.rflags |= (flags & KVM_FLAGS_SET_MASK);
+      kvm_hw->set_regs(vcpu->hw_data, &r);
+      if (vcpu->mmio.flag & KVM_MMIO_TO_MMIO)
+      {
+	*(u_int64_t *)&(run->mmio.data) = vcpu->mmio.ptr2;
+	run->mmio.is_write = 1;
+	vcpu->mmio.dir = 1;
+	vcpu->mmio.flag = KVM_MMIO_MOV | KVM_MMIO_EN;
+	run->exit_reason = KVM_EXIT_MMIO;
+	return (0);
+      }
+      break;
+      
+    case KVM_MMIO_EN | KVM_MMIO_MOV: // MOV
+      switch (vcpu->mmio.size)
+      {
+      case 1:
+	*(u_int8_t *)(vcpu->mmio.ptr) = *(u_int8_t *)(&(run->mmio.data));
+	break;
+      case 2:
+	*(u_int16_t *)(vcpu->mmio.ptr) = *(u_int16_t *)(&(run->mmio.data));
+	break;
+      case 4:
+	*(u_int32_t *)(vcpu->mmio.ptr) = *(u_int32_t *)(&(run->mmio.data));
+	break;
+      case 8:
+	*(u_int64_t *)(vcpu->mmio.ptr) = *(u_int64_t *)(&(run->mmio.data));
+	break;
+      }
+      break;
+
+    case KVM_MMIO_EN | KVM_MMIO_MOV | KVM_MMIO_TO_MMIO: // MOV
+      run->mmio.phys_addr = vcpu->mmio.ptr2;
+      run->mmio.len = vcpu->mmio.size;
+      run->mmio.is_write = 1;
+      vcpu->mmio.dir = 1;
+      vcpu->mmio.flag = KVM_MMIO_MOV | KVM_MMIO_EN;
+      run->exit_reason = KVM_EXIT_MMIO;
+      return (0);
+
+    default:
+      printf("KVM: unknown flags for MMIO operation: %x\n", vcpu->mmio.flag);
+      return (-ENXIO);
+      break;
+    }
+  }
+  else
+    if (vcpu->mmio.flag & KVM_MMIO_TO_MMIO)
+    {
+      printf("KVM: ERR userland did not fulfill MMIO MOVSB access\n");
+      return (-ENXIO);
+    }
+  vcpu->mmio.flag = KVM_MMIO_MOV;
+  
+  return (0);
+}
+
+// IOCTL
+int kvmioctl(dev_t dev, u_long cmd, caddr_t addr, int flag, struct proc *p)
+{
+  struct kvm_vcpu *vcpu = curproc->p_kvm_vcpu;
+  struct kvm_p *kp = p->p_p->ps_mainproc->p_kvm;
+  u_int64_t ret;
+
+  if (!kp)
+    return (-ENXIO);
+  
+  switch (minor(dev))
+  {
+  case 0: // /dev/kvm
+    if (!kp)
+      return (-ENODEV);
+    switch (cmd)
+    {
+    case KVM_GET_API_VERSION:
+      return (KVM_API_VERSION);
+    case KVM_CREATE_VM:
+      return (kvm_vm_fd(p));
+    case KVM_GET_MSR_INDEX_LIST:
+      return (kvm_get_msr_idx_lst((struct kvm_msr_list *)addr));
+    case KVM_CHECK_EXTENSION:
+      return (kvm_sys_check_ext(*(int *)addr));
+    case KVM_GET_SUPPORTED_CPUID:
+      return (kvm_get_supported_cpuid((struct kvm_cpuid2 *)addr));
+    case KVM_GET_VCPU_MMAP_SIZE:
+      return (sizeof(struct kvm_run));
+    default:
+      printf("KVM: unhandled sys cmd: %lu\n", cmd);
+      return (-ENOTTY);
+    }
+    
+  case 1: // /dev/kvm_vm
+    if (!kp)
+      return (-ENODEV);
+    switch (cmd)
+    {
+    case KVM_SET_TSS_ADDR:
+      if (kp->tss_base)
+	return (-ENOTTY);
+      kp->tss_base = *(u_int32_t *)addr;
+      return (0);
+    case KVM_SET_IDENTITY_MAP_ADDR:
+      if (kp->identity_base != KVM_DFLT_IDENTITY_BASE)
+	return (-ENOTTY);
+      kp->identity_base = *(u_int32_t *)addr;
+      return (0);
+    case KVM_SET_USER_MEMORY_REGION:
+      return (kvm_set_user_memory_region(p, kp,
+	      (struct kvm_userspace_memory_region *)addr));
+    case KVM_CREATE_VCPU:
+      return (kvm_vcpu_fd(p));
+    case KVM_GET_DIRTY_LOG:
+      return (kvm_get_dirty_log(kp, (struct kvm_dirty_log *)addr));
+    default:
+      printf("KVM: unhandled vm cmd: %lu\n", cmd);
+      return (-ENOTTY);
+    }
+    
+  case 2: // /dev/kvm_vcpu
+    if (!kp || !(curproc->p_kvm_vcpu) || kp->shutdown)
+      return (-ENODEV);
+    switch (cmd)
+    {
+    case KVM_SET_CPUID2:
+      if (((struct kvm_cpuid2 *)addr)->nent > 32)
+	return (-ENOMEM);
+      memcpy(&(vcpu->cpuid), addr, sizeof(vcpu->cpuid));
+      return (0);
+    case KVM_SET_SIGNAL_MASK:
+      return (0);
+    case KVM_GET_REGS:
+      return (kvm_hw->get_regs(vcpu->hw_data, (struct kvm_regs *)addr));
+    case KVM_SET_REGS:
+      return (kvm_hw->set_regs(vcpu->hw_data, (struct kvm_regs *)addr));
+    case KVM_GET_FPU:
+      return (kvm_hw->get_fpu(vcpu->hw_data, (struct kvm_fpu *)addr));
+    case KVM_SET_FPU:
+      return (kvm_hw->set_fpu(vcpu->hw_data, (struct kvm_fpu *)addr));
+    case KVM_GET_SREGS:
+      return (kvm_hw->get_sregs(vcpu->hw_data, (struct kvm_sregs *)addr, kp));
+    case KVM_SET_SREGS:
+      return (kvm_hw->set_sregs(vcpu->hw_data, (struct kvm_sregs *)addr, kp));
+    case KVM_GET_MSRS:
+      return (kvm_hw->get_msrs(vcpu->hw_data, (struct kvm_msrs *)addr));
+    case KVM_SET_MSRS:
+      return (kvm_hw->set_msrs(vcpu->hw_data, (struct kvm_msrs *)addr));
+    case KVM_GET_MP_STATE:
+      ((struct kvm_mp_state *)addr)->mp_state = KVM_MP_STATE_RUNNABLE;
+      return (0);
+    case KVM_SET_MP_STATE:
+      return (0);
+    case KVM_RUN:
+      // handle MMIO access from userland
+      if (vcpu->mmio.flag & KVM_MMIO_EN)
+      {
+	if ((ret = kvm_handle_pre_mmio(vcpu->kr, kp)))
+	  return (ret);
+	// MMIO to MMIO
+	if (vcpu->mmio.flag & KVM_MMIO_EN)
+	  return (0);
+      }
+      // handle IO operation from userland
+      if (vcpu->io.flag & 1)
+      {
+	if ((ret = kvm_handle_pre_io(vcpu->kr, kp)) < 0)
+	  return (ret);
+	if (ret == 1)
+	{
+	  kvm_handle_io(vcpu->hw_data, vcpu->kr, kp,
+			sizeof(vcpu->kr->s));
+	  return (0);
+	}
+      }
+      // run
+      ret = kvm_hw->run(vcpu->hw_data, vcpu->kr, kp, p);
+      // handle MMIO access to userland
+      if (vcpu->kr->exit_reason == KVM_EXIT_MMIO)
+	kvm_handle_mmio(vcpu->kr, kp);
+      // handle IO operation to userland
+      if (vcpu->kr->exit_reason == KVM_EXIT_IO)
+	kvm_handle_io(vcpu->hw_data, vcpu->kr, kp, 0);
+      // update lapic base
+      vcpu->kr->apic_base = vcpu->lapic;
+      return (ret);
+    case KVM_INTERRUPT:
+      return (kvm_hw->set_intr(vcpu->hw_data,
+			       ((struct kvm_interrupt *)addr)->irq));
+    case KVM_NMI:
+      return (kvm_hw->set_intr(vcpu->hw_data, 0xffffffff));
+    default:
+      printf("KVM: unhandled vcpu cmd: %lu\n", cmd);
+      return (-ENOTTY);
+    }
+    
+  default:
+    return (-ENODEV);
+  }
+}
diff --git a/sys/arch/amd64/amd64/svm.c b/sys/arch/amd64/amd64/svm.c
new file mode 100644
index 0000000..1ca67c9
--- /dev/null
+++ b/sys/arch/amd64/amd64/svm.c
@@ -0,0 +1,1141 @@
+/*
+ * Copyright (c) 2016 Mickael Torres
+ * All rights reserved.
+ *
+ * Permission to use, copy, modify, and distribute this software for any
+ * purpose with or without fee is hereby granted, provided that the above
+ * copyright notice and this permission notice appear in all copies.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
+ * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
+ * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
+ * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
+ * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
+ * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
+ */
+
+#include <sys/types.h>
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/stdint.h>
+#include <sys/malloc.h>
+#include <sys/lock.h>
+#include <sys/queue.h>
+#include <sys/user.h>
+#include <sys/proc.h>
+#include <sys/pool.h>
+
+#include <uvm/uvm.h>
+
+#include <machine/kvm.h>
+#include <machine/svm.h>
+
+// from fpu.c
+#define fxsave(addr)            __asm("fxsave %0" : "=m" (*addr))
+#define fxrstor(addr)           __asm("fxrstor %0" : : "m" (*addr))
+
+// kvm fct ptr
+struct kvm_hw_ptr kvm_svm_ptr;
+
+// global enable flag
+int svm_en;
+
+// lock
+struct mutex svm_mutex;
+
+// initialize a new CPU
+struct v_svm *svm_init(struct proc *p)
+{
+  struct v_svm_vpage *v;
+  struct v_svm *svm;
+
+  svm = malloc(sizeof(struct v_svm), M_MEMDESC, M_WAITOK);
+  if (svm)
+  {
+    bzero(svm, sizeof(struct v_svm));
+
+    // initialize the page list
+    SLIST_INIT(&svm->vp);
+
+    // allocate VMCB region
+    svm->vmcb = (struct svm_vmcb *)uvm_km_zalloc(kernel_map, PAGE_SIZE);
+    pmap_extract(pmap_kernel(), (vaddr_t)svm->vmcb, &(svm->p_vmcb));
+    
+    // add the entry to the page list
+    v = malloc(sizeof(struct v_svm_vpage), M_MEMDESC, M_WAITOK);
+    v->vp = (vaddr_t)svm->vmcb;
+    v->pp = svm->p_vmcb;
+    v->sz = PAGE_SIZE;
+    SLIST_INSERT_HEAD(&svm->vp, v, next);
+
+    // allocate IO and MSR bitmaps, we use dma_alloc for contiguous phys pg
+    svm->v_iobm = (u_int64_t)dma_alloc(PAGE_SIZE * 3, PR_WAITOK);
+    svm->v_msrbm = (u_int64_t)dma_alloc(PAGE_SIZE * 2, PR_WAITOK);
+    pmap_extract(pmap_kernel(), svm->v_iobm, &svm->p_iobm);
+    pmap_extract(pmap_kernel(), svm->v_msrbm, &svm->p_msrbm);
+  }
+
+  return (svm);
+}
+
+// free a CPU
+void svm_free(struct v_svm *svm)
+{
+  struct v_svm_vpage *v;
+
+  if (svm)
+  {
+    dma_free((void *)svm->v_iobm, PAGE_SIZE * 3);
+    dma_free((void *)svm->v_msrbm, PAGE_SIZE * 2);
+    // free the page list
+    while (!SLIST_EMPTY(&svm->vp))
+    {
+      v = SLIST_FIRST(&svm->vp);
+      SLIST_REMOVE_HEAD(&svm->vp, next);
+      uvm_km_free(kernel_map, v->vp, v->sz);
+      free(v, M_MEMDESC, sizeof(struct v_svm_vpage));
+    }
+    // TODO
+    // make sure to free all
+    free(svm, M_MEMDESC, sizeof(struct v_svm));
+  }
+}
+
+void svm_enter(void)
+{
+  struct cpu_info *ci;
+  u_int64_t msr;
+  u_int32_t eax;
+  u_int32_t ebx;
+  u_int32_t ecx;
+  u_int32_t edx;
+
+  // check if SVM is enabled
+  if (!svm_en)
+    return;
+
+  // acquire lock
+  mtx_enter(&svm_mutex);
+
+  // set svm parameters for current cpu
+  ci = curcpu();
+  while (!(ci->ci_svm = malloc(sizeof(struct svminfo), M_MEMDESC, M_NOWAIT)))
+    ;
+  bzero(ci->ci_svm, sizeof(struct svminfo));
+  if (!ci->ci_svm)
+    panic("svm_enter: can't allocate memory");
+  
+  // SVM CPUID: eax revision, ebx ASID number, edx features
+  CPUID(SVM_CPUID, eax, ebx, ecx, edx);
+  ci->ci_svm->revision = eax;
+  ci->ci_svm->nasid = ebx;
+  ci->ci_svm->features = edx;
+
+  // enable SVM
+  msr = rdmsr(MSR_EFER);
+  msr |= SVM_EFER_SVME;
+  wrmsr(MSR_EFER, msr);
+  
+  msr = rdmsr(MSR_EFER);
+  if (!(msr & SVM_EFER_SVME))
+  {
+    mtx_leave(&svm_mutex);
+    return;
+  }
+
+  // allocate host save-state area for this CPU
+  ci->ci_svm->vhost = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+  pmap_extract(pmap_kernel(), ci->ci_svm->vhost, &ci->ci_svm->phost);
+
+  // mark that SVM is enabled on this CPU
+  ci->ci_svm->flags = SVM_SVMEN;
+
+  printf("cpu%d: SVM enabled with nPT, nASID %x, rev %d\n",
+	 ci->ci_cpuid, ci->ci_svm->nasid, ci->ci_svm->revision);
+
+  // release lock
+  mtx_leave(&svm_mutex);
+}
+
+void svm_setup(void)
+{
+  //  struct cpu_info *ci;
+  u_int64_t msr;
+  u_int32_t eax;
+  u_int32_t ebx;
+  u_int32_t ecx;
+  u_int32_t edx;
+
+  // not enabled for now
+  svm_en = 0;
+  
+  // check that we are on an amd CPU with SVM
+  if (!(ecpu_ecxfeature & CPUIDECX_SVM))
+    return;
+  
+  // check that SVM is not disabled by the BIOS
+  msr = rdmsr(SVM_CR);
+  if (msr & SVM_CR_SVMDIS)
+    return;
+
+  // SVM CPUID: eax revision, ebx ASID number, edx features
+  CPUID(SVM_CPUID, eax, ebx, ecx, edx);
+
+  // check nPT (nested paging) is supported
+  if (!(edx & SVM_CPUID_NPT))
+    return;
+
+  // initialize lock
+  mtx_init(&svm_mutex, IPL_HIGH);
+
+  // we support SVM, enable it for APs
+  svm_en = 1;
+
+  // enable SVM
+  svm_enter();
+}
+
+int svm_get_regs(struct v_svm *svm, struct kvm_regs *r)
+{
+  // from VMCB
+  r->rax = svm->vmcb->rax;
+  r->rsp = svm->vmcb->rsp;
+  r->rip = svm->vmcb->rip;
+  r->rflags = svm->vmcb->rflags;
+
+  // from our guest struct
+  r->rbx = svm->guest.rbx;
+  r->rcx = svm->guest.rcx;
+  r->rdx = svm->guest.rdx;
+  r->rsi = svm->guest.rsi;
+  r->rdi = svm->guest.rdi;
+  r->rbp = svm->guest.rbp;
+  r->r8 = svm->guest.r8;
+  r->r9 = svm->guest.r9;
+  r->r10 = svm->guest.r10;
+  r->r11 = svm->guest.r11;
+  r->r12 = svm->guest.r12;
+  r->r13 = svm->guest.r13;
+  r->r14 = svm->guest.r14;
+  r->r15 = svm->guest.r15;
+  return (0);
+}
+
+int svm_set_regs(struct v_svm *svm, struct kvm_regs *r)
+{
+  // from VMCB
+  svm->vmcb->rax = r->rax;
+  svm->vmcb->rsp = r->rsp;
+  svm->vmcb->rip = r->rip;
+  svm->vmcb->rflags = r->rflags;
+  
+  // from our guest struct
+  svm->guest.rbx = r->rbx;
+  svm->guest.rcx = r->rcx;
+  svm->guest.rdx = r->rdx;
+  svm->guest.rsi = r->rsi;
+  svm->guest.rdi = r->rdi;
+  svm->guest.rbp = r->rbp;
+  svm->guest.r8 = r->r8;
+  svm->guest.r9 = r->r9;
+  svm->guest.r10 = r->r10;
+  svm->guest.r11 = r->r11;
+  svm->guest.r12 = r->r12;
+  svm->guest.r13 = r->r13;
+  svm->guest.r14 = r->r14;
+  svm->guest.r15 = r->r15;
+  return (0);
+}
+
+int svm_get_fpu(struct v_svm *svm, struct kvm_fpu *f)
+{
+  f->fcw = svm->guest.fpu.fp_fxsave.fx_fcw;
+  f->fsw = svm->guest.fpu.fp_fxsave.fx_fsw;
+  f->ftwx = svm->guest.fpu.fp_fxsave.fx_ftw;
+  f->last_opcode = svm->guest.fpu.fp_fxsave.fx_fop;
+  f->last_ip = svm->guest.fpu.fp_fxsave.fx_rip;
+  f->last_dp = svm->guest.fpu.fp_fxsave.fx_rdp;
+  f->mxcsr = svm->guest.fpu.fp_fxsave.fx_mxcsr;
+  memcpy(f->fpr, svm->guest.fpu.fp_fxsave.fx_st, 128);
+  memcpy(f->xmm, svm->guest.fpu.fp_fxsave.fx_xmm, 256);
+  return (0);
+}
+
+int svm_set_fpu(struct v_svm *svm, struct kvm_fpu *f)
+{
+  svm->guest.fpu.fp_fxsave.fx_fcw = f->fcw;
+  svm->guest.fpu.fp_fxsave.fx_fsw = f->fsw;
+  svm->guest.fpu.fp_fxsave.fx_ftw = f->ftwx;
+  svm->guest.fpu.fp_fxsave.fx_fop = f->last_opcode;
+  svm->guest.fpu.fp_fxsave.fx_rip = f->last_ip;
+  svm->guest.fpu.fp_fxsave.fx_rdp = f->last_dp;
+  svm->guest.fpu.fp_fxsave.fx_mxcsr = f->mxcsr;
+  memcpy(svm->guest.fpu.fp_fxsave.fx_st, f->fpr, 128);
+  memcpy(svm->guest.fpu.fp_fxsave.fx_xmm, f->xmm, 256);
+  return (0);
+}
+
+int svm_get_sregs(struct v_svm *svm, struct kvm_sregs *s, struct kvm_p *kp)
+{
+  // CS
+  s->cs.selector = svm->vmcb->cs.sel;
+  s->cs.base = svm->vmcb->cs.base;
+  s->cs.limit = svm->vmcb->cs.lim;
+  s->cs.type = svm->vmcb->cs.attr & 0b1111;
+  s->cs.present = (svm->vmcb->cs.attr >> 7) & 1;
+  s->cs.dpl = (svm->vmcb->cs.attr >> 5) & 0b11;
+  s->cs.db = (svm->vmcb->cs.attr >> 10) & 1;
+  s->cs.s = (svm->vmcb->cs.attr >> 4) & 1;
+  s->cs.l = (svm->vmcb->cs.attr >> 9) & 1;
+  s->cs.g = (svm->vmcb->cs.attr >> 11) & 1;
+  s->cs.avl = (svm->vmcb->cs.attr >> 8) & 1;
+  s->cs.unusable = 0; //(svm->vmcb->cs.attr >> 16) & 1;
+  // DS
+  s->ds.selector = svm->vmcb->ds.sel;
+  s->ds.base = svm->vmcb->ds.base;
+  s->ds.limit = svm->vmcb->ds.lim;
+  s->ds.type = svm->vmcb->ds.attr & 0b1111;
+  s->ds.present = (svm->vmcb->ds.attr >> 7) & 1;
+  s->ds.dpl = (svm->vmcb->ds.attr >> 5) & 0b11;
+  s->ds.db = (svm->vmcb->ds.attr >> 10) & 1;
+  s->ds.s = (svm->vmcb->ds.attr >> 4) & 1;
+  s->ds.l = (svm->vmcb->ds.attr >> 9) & 1;
+  s->ds.g = (svm->vmcb->ds.attr >> 11) & 1;
+  s->ds.avl = (svm->vmcb->ds.attr >> 8) & 1;
+  s->ds.unusable = 0; //(svm->vmcb->ds.attr >> 16) & 1;
+  // ES
+  s->es.selector = svm->vmcb->es.sel;
+  s->es.base = svm->vmcb->es.base;
+  s->es.limit = svm->vmcb->es.lim;
+  s->es.type = svm->vmcb->es.attr & 0b1111;
+  s->es.present = (svm->vmcb->es.attr >> 7) & 1;
+  s->es.dpl = (svm->vmcb->es.attr >> 5) & 0b11;
+  s->es.db = (svm->vmcb->es.attr >> 10) & 1;
+  s->es.s = (svm->vmcb->es.attr >> 4) & 1;
+  s->es.l = (svm->vmcb->es.attr >> 9) & 1;
+  s->es.g = (svm->vmcb->es.attr >> 11) & 1;
+  s->es.avl = (svm->vmcb->es.attr >> 8) & 1;
+  s->es.unusable = 0; //(svm->vmcb->es.attr >> 16) & 1;
+  // FS
+  s->fs.selector = svm->vmcb->fs.sel;
+  s->fs.base = svm->vmcb->fs.base;
+  s->fs.limit = svm->vmcb->fs.lim;
+  s->fs.type = svm->vmcb->fs.attr & 0b1111;
+  s->fs.present = (svm->vmcb->fs.attr >> 7) & 1;
+  s->fs.dpl = (svm->vmcb->fs.attr >> 5) & 0b11;
+  s->fs.db = (svm->vmcb->fs.attr >> 10) & 1;
+  s->fs.s = (svm->vmcb->fs.attr >> 4) & 1;
+  s->fs.l = (svm->vmcb->fs.attr >> 9) & 1;
+  s->fs.g = (svm->vmcb->fs.attr >> 11) & 1;
+  s->fs.avl = (svm->vmcb->fs.attr >> 8) & 1;
+  s->fs.unusable = 0; //(svm->vmcb->fs.attr >> 16) & 1;
+  // GS
+  s->gs.selector = svm->vmcb->gs.sel;
+  s->gs.base = svm->vmcb->gs.base;
+  s->gs.limit = svm->vmcb->gs.lim;
+  s->gs.type = svm->vmcb->gs.attr & 0b1111;
+  s->gs.present = (svm->vmcb->gs.attr >> 7) & 1;
+  s->gs.dpl = (svm->vmcb->gs.attr >> 5) & 0b11;
+  s->gs.db = (svm->vmcb->gs.attr >> 10) & 1;
+  s->gs.s = (svm->vmcb->gs.attr >> 4) & 1;
+  s->gs.l = (svm->vmcb->gs.attr >> 9) & 1;
+  s->gs.g = (svm->vmcb->gs.attr >> 11) & 1;
+  s->gs.avl = (svm->vmcb->gs.attr >> 8) & 1;
+  s->gs.unusable = 0; //(svm->vmcb->gs.attr >> 16) & 1;
+  // SS
+  s->ss.selector = svm->vmcb->ss.sel;
+  s->ss.base = svm->vmcb->ss.base;
+  s->ss.limit = svm->vmcb->ss.lim;
+  s->ss.type = svm->vmcb->ss.attr & 0b1111;
+  s->ss.present = (svm->vmcb->ss.attr >> 7) & 1;
+  s->ss.dpl = (svm->vmcb->ss.attr >> 5) & 0b11;
+  s->ss.db = (svm->vmcb->ss.attr >> 10) & 1;
+  s->ss.s = (svm->vmcb->ss.attr >> 4) & 1;
+  s->ss.l = (svm->vmcb->ss.attr >> 9) & 1;
+  s->ss.g = (svm->vmcb->ss.attr >> 11) & 1;
+  s->ss.avl = (svm->vmcb->ss.attr >> 8) & 1;
+  s->ss.unusable = 0; //(svm->vmcb->ss.attr >> 16) & 1;
+  // TR
+  s->tr.selector = svm->vmcb->tr.sel;
+  s->tr.base = svm->vmcb->tr.base;
+  s->tr.limit = svm->vmcb->tr.lim;
+  s->tr.type = svm->vmcb->tr.attr & 0b1111;
+  s->tr.present = (svm->vmcb->tr.attr >> 7) & 1;
+  s->tr.dpl = (svm->vmcb->tr.attr >> 5) & 0b11;
+  s->tr.db = (svm->vmcb->tr.attr >> 10) & 1;
+  s->tr.s = (svm->vmcb->tr.attr >> 4) & 1;
+  s->tr.l = (svm->vmcb->tr.attr >> 9) & 1;
+  s->tr.g = (svm->vmcb->tr.attr >> 11) & 1;
+  s->tr.avl = (svm->vmcb->tr.attr >> 8) & 1;
+  s->tr.unusable = 0; //(svm->vmcb->tr.attr >> 16) & 1;
+  // LDTR
+  s->ldt.selector = svm->vmcb->ldtr.sel;
+  s->ldt.base = svm->vmcb->ldtr.base;
+  s->ldt.limit = svm->vmcb->ldtr.lim;
+  s->ldt.type = svm->vmcb->ldtr.attr & 0b1111;
+  s->ldt.present = (svm->vmcb->ldtr.attr >> 7) & 1;
+  s->ldt.dpl = (svm->vmcb->ldtr.attr >> 5) & 0b11;
+  s->ldt.db = (svm->vmcb->ldtr.attr >> 10) & 1;
+  s->ldt.s = (svm->vmcb->ldtr.attr >> 4) & 1;
+  s->ldt.l = (svm->vmcb->ldtr.attr >> 9) & 1;
+  s->ldt.g = (svm->vmcb->ldtr.attr >> 11) & 1;
+  s->ldt.avl = (svm->vmcb->ldtr.attr >> 8) & 1;
+  s->ldt.unusable = 0; //(svm->vmcb->ldtr.attr >> 16) & 1;
+  // GDT
+  s->gdt.base = svm->vmcb->gdtr.base;
+  s->gdt.limit = svm->vmcb->gdtr.lim;
+  // IDT
+  s->idt.base = svm->vmcb->idtr.base;
+  s->idt.limit = svm->vmcb->idtr.lim;
+  // CR
+  s->cr0 = svm->vmcb->cr0;
+  s->cr2 = svm->vmcb->cr2;
+  s->cr3 = svm->vmcb->cr3;
+  s->cr4 = svm->vmcb->cr4;
+  s->cr8 = svm->vmcb->v_tpr;
+  // EFER
+  s->efer = svm->vmcb->efer;
+  // APIC-access
+  s->apic_base = curproc->p_kvm_vcpu->lapic;
+  // interrupts
+  s->interrupt_bitmap[0] = svm->intr[0];
+  s->interrupt_bitmap[1] = svm->intr[1];
+  s->interrupt_bitmap[2] = svm->intr[2];
+  s->interrupt_bitmap[3] = svm->intr[3];
+  
+  return (0);
+}
+
+int svm_set_sregs(struct v_svm *svm, struct kvm_sregs *s, struct kvm_p *kp)
+{
+  // CS
+  svm->vmcb->cs.sel = s->cs.selector;
+  svm->vmcb->cs.base = s->cs.base;
+  svm->vmcb->cs.lim = s->cs.limit;
+  svm->vmcb->cs.attr = (s->cs.type & 0b1111) |
+    ((s->cs.present & 1) << 7) |
+    ((s->cs.dpl & 0b11) << 5) |
+    ((s->cs.db & 1) << 10) |
+    ((s->cs.s & 1) << 4) |
+    ((s->cs.l & 1) << 9) |
+    ((s->cs.g & 1) << 11) |
+    ((s->cs.avl & 1) << 8);
+  // DS
+  svm->vmcb->ds.sel = s->ds.selector;
+  svm->vmcb->ds.base = s->ds.base;
+  svm->vmcb->ds.lim = s->ds.limit;
+  svm->vmcb->ds.attr = (s->ds.type & 0b1111) |
+    ((s->ds.present & 1) << 7) |
+    ((s->ds.dpl & 0b11) << 5) |
+    ((s->ds.db & 1) << 10) |
+    ((s->ds.s & 1) << 4) |
+    ((s->ds.l & 1) << 9) |
+    ((s->ds.g & 1) << 11) |
+    ((s->ds.avl & 1) << 8);
+  // ES
+  svm->vmcb->es.sel = s->es.selector;
+  svm->vmcb->es.base = s->es.base;
+  svm->vmcb->es.lim = s->es.limit;
+  svm->vmcb->es.attr = (s->es.type & 0b1111) |
+    ((s->es.present & 1) << 7) |
+    ((s->es.dpl & 0b11) << 5) |
+    ((s->es.db & 1) << 10) |
+    ((s->es.s & 1) << 4) |
+    ((s->es.l & 1) << 9) |
+    ((s->es.g & 1) << 11) |
+    ((s->es.avl & 1) << 8);
+  // FS
+  svm->vmcb->fs.sel = s->fs.selector;
+  svm->vmcb->fs.base = s->fs.base;
+  svm->vmcb->fs.lim = s->fs.limit;
+  svm->vmcb->fs.attr = (s->fs.type & 0b1111) |
+    ((s->fs.present & 1) << 7) |
+    ((s->fs.dpl & 0b11) << 5) |
+    ((s->fs.db & 1) << 10) |
+    ((s->fs.s & 1) << 4) |
+    ((s->fs.l & 1) << 9) |
+    ((s->fs.g & 1) << 11) |
+    ((s->fs.avl & 1) << 8);
+  // GS
+  svm->vmcb->gs.sel = s->gs.selector;
+  svm->vmcb->gs.base = s->gs.base;
+  svm->vmcb->gs.lim = s->gs.limit;
+  svm->vmcb->gs.attr = (s->gs.type & 0b1111) |
+    ((s->gs.present & 1) << 7) |
+    ((s->gs.dpl & 0b11) << 5) |
+    ((s->gs.db & 1) << 10) |
+    ((s->gs.s & 1) << 4) |
+    ((s->gs.l & 1) << 9) |
+    ((s->gs.g & 1) << 11) |
+    ((s->gs.avl & 1) << 8);
+  // SS
+  svm->vmcb->ss.sel = s->ss.selector;
+  svm->vmcb->ss.base = s->ss.base;
+  svm->vmcb->ss.lim = s->ss.limit;
+  svm->vmcb->ss.attr = (s->ss.type & 0b1111) |
+    ((s->ss.present & 1) << 7) |
+    ((s->ss.dpl & 0b11) << 5) |
+    ((s->ss.db & 1) << 10) |
+    ((s->ss.s & 1) << 4) |
+    ((s->ss.l & 1) << 9) |
+    ((s->ss.g & 1) << 11) |
+    ((s->ss.avl & 1) << 8);
+  // TR
+  svm->vmcb->tr.sel = s->tr.selector;
+  svm->vmcb->tr.base = s->tr.base;
+  svm->vmcb->tr.lim = s->tr.limit;
+  svm->vmcb->tr.attr = (s->tr.type & 0b1111) |
+    ((s->tr.present & 1) << 7) |
+    ((s->tr.dpl & 0b11) << 5) |
+    ((s->tr.db & 1) << 10) |
+    ((s->tr.s & 1) << 4) |
+    ((s->tr.l & 1) << 9) |
+    ((s->tr.g & 1) << 11) |
+    ((s->tr.avl & 1) << 8);
+  // LDTR
+  svm->vmcb->ldtr.sel = s->ldt.selector;
+  svm->vmcb->ldtr.base = s->ldt.base;
+  svm->vmcb->ldtr.lim = s->ldt.limit;
+  svm->vmcb->ldtr.attr = (s->ldt.type & 0b1111) |
+    ((s->ldt.present & 1) << 7) |
+    ((s->ldt.dpl & 0b11) << 5) |
+    ((s->ldt.db & 1) << 10) |
+    ((s->ldt.s & 1) << 4) |
+    ((s->ldt.l & 1) << 9) |
+    ((s->ldt.g & 1) << 11) |
+    ((s->ldt.avl & 1) << 8);
+  // GDT
+  svm->vmcb->gdtr.base = s->gdt.base;
+  svm->vmcb->gdtr.lim = s->gdt.limit;
+  // IDT
+  svm->vmcb->idtr.base = s->idt.base;
+  svm->vmcb->idtr.lim = s->idt.limit;
+  // CR
+  svm->vmcb->cr0 = s->cr0;
+  svm->vmcb->cr2 = s->cr2;
+  svm->vmcb->cr3 = s->cr3;
+  svm->vmcb->cr4 = s->cr4;
+  svm->vmcb->v_tpr = s->cr8;
+  // EFER
+  svm->vmcb->efer = s->efer;
+  // APIC-access
+  curproc->p_kvm_vcpu->lapic = s->apic_base;
+  // interrupts
+  svm->intr[0] = s->interrupt_bitmap[0];
+  svm->intr[1] = s->interrupt_bitmap[1];
+  svm->intr[2] = s->interrupt_bitmap[2];
+  svm->intr[3] = s->interrupt_bitmap[3];
+  
+  return (0);
+}
+
+void *svm_get_reg_ptr(struct v_svm *svm, int reg)
+{
+  switch (reg)
+  {
+  case KVM_REG_RAX:
+    return ((void *)&(svm->vmcb->rax));
+  case KVM_REG_RBX:
+    return ((void *)&(svm->guest.rbx));
+  case KVM_REG_RCX:
+    return ((void *)&(svm->guest.rcx));
+  case KVM_REG_RDX:
+    return ((void *)&(svm->guest.rdx));
+  case KVM_REG_RSI:
+    return ((void *)&(svm->guest.rsi));
+  case KVM_REG_RDI:
+    return ((void *)&(svm->guest.rdi));
+  case KVM_REG_RBP:
+    return ((void *)&(svm->guest.rbp));
+  case KVM_REG_R8:
+    return ((void *)&(svm->guest.r8));
+  case KVM_REG_R9:
+    return ((void *)&(svm->guest.r9));
+  case KVM_REG_R10:
+    return ((void *)&(svm->guest.r10));
+  case KVM_REG_R11:
+    return ((void *)&(svm->guest.r11));
+  case KVM_REG_R12:
+    return ((void *)&(svm->guest.r12));
+  case KVM_REG_R13:
+    return ((void *)&(svm->guest.r13));
+  case KVM_REG_R14:
+    return ((void *)&(svm->guest.r14));
+  case KVM_REG_R15:
+    return ((void *)&(svm->guest.r15));
+  }
+  return ((void *)1);
+}
+
+int svm_get_msrs(struct v_svm *svm, struct kvm_msrs *m)
+{
+  int i;
+
+  for (i = 0; i < m->nmsrs; i++)
+    switch(m->entries[i].index)
+    {
+    case MSR_EFER:
+      m->entries[i].data = svm->vmcb->efer | SVM_EFER_SVME;
+      break;
+    case MSR_SYSENTER_CS:
+      m->entries[i].data = svm->vmcb->sysenter_cs;
+      break;
+    case MSR_SYSENTER_ESP:
+      m->entries[i].data = svm->vmcb->sysenter_esp;
+      break;
+    case MSR_SYSENTER_EIP:
+      m->entries[i].data = svm->vmcb->sysenter_eip;
+      break;
+    case MSR_STAR:
+      m->entries[i].data = svm->vmcb->star;
+      break;
+    case MSR_LSTAR:
+      m->entries[i].data = svm->vmcb->lstar;
+      break;
+    case MSR_CSTAR:
+      m->entries[i].data = svm->vmcb->cstar;
+      break;
+    case MSR_SFMASK:
+      m->entries[i].data = svm->vmcb->sfmask;
+      break;
+    case MSR_FSBASE:
+      m->entries[i].data = svm->guest.fsbase;
+      break;
+    case MSR_GSBASE:
+      m->entries[i].data = svm->guest.gsbase;
+      break;
+    case MSR_KERNELGSBASE:
+      m->entries[i].data = svm->vmcb->kern_gs_base;
+      break;
+    case MSR_CR_PAT:
+      m->entries[i].data = svm->vmcb->g_pat;
+      break;
+    default:
+      m->entries[i].data = 0;
+    }
+  return (0);
+}
+
+int svm_set_msrs(struct v_svm *svm, struct kvm_msrs *m)
+{
+  int i;
+
+  for (i = 0; i < m->nmsrs; i++)
+    switch(m->entries[i].index)
+    {
+    case MSR_EFER:
+      svm->vmcb->efer = m->entries[i].data | SVM_EFER_SVME;
+      break;
+    case MSR_SYSENTER_CS:
+      svm->vmcb->sysenter_cs = m->entries[i].data;
+      break;
+    case MSR_SYSENTER_ESP:
+      svm->vmcb->sysenter_esp = m->entries[i].data;
+      break;
+    case MSR_SYSENTER_EIP:
+      svm->vmcb->sysenter_eip = m->entries[i].data;
+      break;
+    case MSR_STAR:
+      svm->vmcb->star = m->entries[i].data;
+      break;
+    case MSR_LSTAR:
+      svm->vmcb->lstar = m->entries[i].data;
+      break;
+    case MSR_CSTAR:
+      svm->vmcb->cstar = m->entries[i].data;
+      break;
+    case MSR_SFMASK:
+      svm->vmcb->sfmask = m->entries[i].data;
+      break;
+    case MSR_FSBASE:
+      svm->guest.fsbase = m->entries[i].data;
+      break;
+    case MSR_GSBASE:
+      svm->guest.gsbase = m->entries[i].data;
+      break;
+    case MSR_KERNELGSBASE:
+      svm->vmcb->kern_gs_base = m->entries[i].data;
+      break;
+    case MSR_CR_PAT:
+      svm->vmcb->g_pat = m->entries[i].data;
+      break;
+    default:
+      break;
+    }
+  return (0);
+}
+
+int svm_set_interrupt(struct v_svm *svm, u_int32_t intr)
+{
+  if (intr == 0xffffffff)
+  {
+    if (svm->nmi == 1)
+      return (-EEXIST);
+    svm->nmi = 1;
+    return (0);
+  }
+  if (intr > 255)
+    return (-EINVAL);
+  if (svm->intr[intr / 64] & ((u_int64_t) 1 << (intr % 64)))
+    return (-EEXIST);
+  svm->intr[intr / 64] |= ((u_int64_t) 1 << (intr % 64));
+
+  return (0);
+}
+
+int svm_run(struct v_svm *svm, struct kvm_run *run, struct kvm_p *kp,
+	    struct proc *p)
+{
+  u_int64_t ret;
+  xdtr_t gdtr;
+  u_int64_t fsbase;
+  u_int64_t gsbase;
+  u_int64_t kgsbase;
+  u_int64_t star;
+  u_int64_t lstar;
+  u_int64_t cstar;
+  u_int64_t sfmask;
+  u_int64_t val;
+  int i;
+
+  // save FPU/MMX/SSE... state
+  // XXX implement lazy save/restore
+  fxsave(&p->p_addr->u_pcb.pcb_savefpu);
+
+  // save MSR for SYSCALL/SYSRET
+  star = rdmsr(MSR_STAR);
+  lstar = rdmsr(MSR_LSTAR);
+  cstar = rdmsr(MSR_CSTAR);
+  sfmask = rdmsr(MSR_SFMASK);
+
+  ret = 0;
+
+  // by default do not flush anything
+  //  svm->vmcb->tlb_ctl = 0;
+  svm->vmcb->tlb_ctl = 3;
+
+  // set control if not already done
+  if (!(svm->flags & SVM_VM_FL_STARTED))
+  {
+    svm->vmcb->int_cr_read = 0;        // do not catch any CR read
+    svm->vmcb->int_cr_write = 0;       // do not catch any CR write
+    svm->vmcb->int_dr_read = 0;        // do not catch any DR read
+    svm->vmcb->int_dr_write = 0;       // do not catch any DR write
+    svm->vmcb->int_exc = 0;            // do not catch guest exceptions
+    // for the next value, see APMv2 (24593) appendix B - VMCB Layout
+    svm->vmcb->intercept = 0x1fd904000f;
+    svm->vmcb->iopm_base_pa = svm->p_iobm; // I/O bitmap phys addr
+    svm->vmcb->msrpm_base_pa = svm->p_msrbm; // MSR bmap phys addr
+    // fill the bmaps, we intercept every MSR and I/O access
+    memset((void *)svm->v_iobm, 0xff, PAGE_SIZE * 3);
+    memset((void *)svm->v_msrbm, 0xff, PAGE_SIZE * 2);
+    // authorize safe MSRs
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_SYSENTER_CS);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_SYSENTER_ESP);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_SYSENTER_EIP);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_STAR);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_LSTAR);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_CSTAR);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_SFMASK);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_FSBASE);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_GSBASE);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_KERNELGSBASE);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_CR_PAT);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_PERF_GLOBAL_CTRL);
+    SVM_MSR_BMAP_AUTH(svm->v_msrbm, MSR_TSC);
+    svm->vmcb->asid = kp->id;     // VM id max is calculated for ASID
+    svm->vmcb->tlb_ctl = 3;            // first time launching, flush TLB
+    svm->vmcb->v_intr_masking = 1;     // use virtual TPR
+    svm->vmcb->np_enable = 1;          // enable nested paging
+    svm->vmcb->n_cr3 = kp->ppml4; // nPT
+    svm->vmcb->lbr_virt_en = 1;        // enable LBR virtualization
+    svm->vmcb->vmcb_clean = 0;        // we don't use caching yet
+  }
+
+  // events injection
+  if (!(svm->vmcb->evt_inj & ((u_int32_t)1 << 31)) &&
+      !svm->vmcb->v_irq)
+  {
+    // inject NMI if needed
+    if (svm->nmi)
+    {
+      svm->vmcb->evt_inj = 0x80000202;
+      svm->nmi = 0;
+    }
+    else
+      // inject interrupt if needed
+      if (svm->intr[0] || svm->intr[1] || svm->intr[2] ||
+	  svm->intr[3])
+	if (svm->vmcb->rflags & ((u_int64_t)1 << 9))
+	  for (i = 0; i < 256; i++)
+	    if (svm->intr[i / 64] & ((u_int64_t) 1 << (i % 64)))
+	    {
+	      svm->intr[i / 64] &= ~((u_int64_t) 1 << (i % 64));
+	      svm->vmcb->evt_inj = (i & 0xff) | (1 << 31);
+	      break;
+	    }
+  }
+
+  // check if we can exit now for interrupt request
+  if (run->request_interrupt_window &&
+      (svm->vmcb->rflags & ((u_int64_t)1 << 9)) &&
+      !(svm->vmcb->evt_inj & ((u_int32_t)1 << 31)) &&
+      !svm->vmcb->v_irq &&
+      !svm->vmcb->intr_shadow)
+  {
+    run->exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN;
+    run->ready_for_interrupt_injection = 1;
+    run->if_flag = (svm->vmcb->rflags & ((u_int64_t)1 << 9)) ? 1 : 0;
+    run->flags = 0;
+    run->request_interrupt_window = 0;
+    p->p_kvm_vcpu->insn_len = 0;
+    return (0);
+  }
+
+  // exit on interrupt window
+  if (run->request_interrupt_window &&
+      !svm->vmcb->v_irq)
+  {
+    svm->vmcb->v_irq = 1;
+    svm->vmcb->v_ign_tpr = 1;
+    svm->vmcb->v_intr_vec = 0;
+    svm->vmcb->intercept |= 0b10000; // intercept VINTR
+  }
+  
+  // load the registers with data from kvm_run
+  svm->vmcb->v_tpr = run->cr8;
+  
+  // save GDTR for later
+  asm volatile("sgdt %0;" :: "m"(gdtr));
+
+  // save MSRs
+  fsbase = rdmsr(MSR_FSBASE);
+  gsbase = rdmsr(MSR_GSBASE);
+  kgsbase = rdmsr(MSR_KERNELGSBASE);
+
+  // disable global interrupts
+  asm volatile("cli; clgi;");
+  // unlock the kernel
+  KERNEL_UNLOCK();
+
+  // SVME must always be set
+  svm->vmcb->efer |= SVM_EFER_SVME;
+
+  // set the host state area of the current CPU
+  wrmsr(SVM_HSAVE_PA, p->p_cpu->ci_svm->phost);
+  
+  // if insn_len is set, we have to advance RIP
+  if (p->p_kvm_vcpu->insn_len)
+  {
+    svm->vmcb->rip += p->p_kvm_vcpu->insn_len;
+    p->p_kvm_vcpu->insn_len = 0;
+  }
+  
+  // restore guest FPU/MMX/SSE...
+  fxrstor(&svm->guest.fpu);
+
+  // restore MSRs not handled by SVM
+  wrmsr(MSR_FSBASE, svm->guest.fsbase);
+  wrmsr(MSR_GSBASE, svm->guest.gsbase);
+  
+  asm volatile("push %%rbx;"              // save host context
+	       "push %%rcx;"
+	       "push %%rdx;"
+	       "push %%rsi;"
+	       "push %%rdi;"
+	       "push %%r8;"
+	       "push %%r9;"
+	       "push %%r10;"
+	       "push %%r11;"
+	       "push %%r12;"
+	       "push %%r13;"
+	       "push %%r14;"
+	       "push %%r15;"
+	       "mov %%gs, %%bx;"
+	       "push %%rbx;"
+	       "mov %%fs, %%bx;"
+	       "push %%rbx;"
+	       "sldt %%bx;"
+	       "push %%rbx;"              // LDT
+	       "xor %%rbx, %%rbx;"
+	       "str %%bx;"
+	       "push %%rbx;"              // TR
+	       "push %%rbp;"
+	       "push %%rcx;"              // &svm->guest
+	       "mov %%rcx, %%rbp;"        // restore guest state
+	       "mov 0x00(%%rbp), %%rbx;"
+	       "mov 0x08(%%rbp), %%rcx;"
+	       "mov 0x10(%%rbp), %%rdx;"
+	       "mov 0x18(%%rbp), %%rsi;"
+	       "mov 0x20(%%rbp), %%rdi;"
+	       "mov 0x28(%%rbp), %%r8;"
+	       "mov 0x30(%%rbp), %%r9;"
+	       "mov 0x38(%%rbp), %%r10;"
+	       "mov 0x40(%%rbp), %%r11;"
+	       "mov 0x48(%%rbp), %%r12;"
+	       "mov 0x50(%%rbp), %%r13;"
+	       "mov 0x58(%%rbp), %%r14;"
+	       "mov 0x60(%%rbp), %%r15;"
+	       "mov 0x68(%%rbp), %%rbp;"
+	       "sti;"                    // enable physical ints in vmrun
+	       ".byte 0x0f,0x01,0xda;"   // VMLOAD
+	       ".byte 0x0f,0x01,0xd8;"   // VMRUN
+	       ".byte 0x0f,0x01,0xdb;"   // VMSAVE
+	       "cli;"
+	       "mov %%rbp, %%rax;"
+	       "pop %%rbp;"              // &svm->guest
+	       "mov %%rbx, 0x00(%%rbp);" // save guest
+	       "mov %%rcx, 0x08(%%rbp);"
+	       "mov %%rdx, 0x10(%%rbp);"
+	       "mov %%rsi, 0x18(%%rbp);"
+	       "mov %%rdi, 0x20(%%rbp);"
+	       "mov %%r8, 0x28(%%rbp);"
+	       "mov %%r9, 0x30(%%rbp);"
+	       "mov %%r10, 0x38(%%rbp);"
+	       "mov %%r11, 0x40(%%rbp);"
+	       "mov %%r12, 0x48(%%rbp);"
+	       "mov %%r13, 0x50(%%rbp);"
+	       "mov %%r14, 0x58(%%rbp);"
+	       "mov %%r15, 0x60(%%rbp);"
+	       "mov %%rax, 0x68(%%rbp);"
+	       "pop %%rbp;"              // restore host context
+	       "pop %%rbx;"              // get TR selector
+	       "push %%rbx;"             // store TR selector
+	       "mov %0, %%rax;"
+	       "add %%rax, %%rbx;"    // rbx now has the descriptor for TR
+	       "add $4, %%rbx;"
+	       "mov (%%rbx), %%eax;"
+	       "and $0xfffffdff, %%eax;" // clear the busy bit
+	       "mov %%eax, (%%rbx);"
+	       "pop %%rbx;"              // get TR selector
+	       "ltr %%bx;"               // load TR
+	       "pop %%rbx;"              // get LDT
+	       "lldt %%bx;"              // load LDT
+	       "pop %%rbx;"
+	       "mov %%bx, %%fs;"
+	       "pop %%rbx;"
+	       "mov %%bx, %%gs;"
+	       "pop %%r15;"
+	       "pop %%r14;"
+	       "pop %%r13;"
+	       "pop %%r12;"
+	       "pop %%r11;"
+	       "pop %%r10;"
+	       "pop %%r9;"
+	       "pop %%r8;"
+	       "pop %%rdi;"
+	       "pop %%rsi;"
+	       "pop %%rdx;"
+	       "pop %%rcx;"
+	       "pop %%rbx;"
+	       :: "m"(gdtr.base), "a"(svm->p_vmcb), "c"(&svm->guest)
+	       : "r15", "r14", "r13", "r12", "r11",
+		 "r10", "r9", "r8", "rdi", "rsi", "rdx", "rbx", "rsp");
+
+  // save MSRs not handled by SVM
+  svm->guest.fsbase = rdmsr(MSR_FSBASE);
+  svm->guest.gsbase = rdmsr(MSR_GSBASE);
+  
+  // restore FSBASE & GSBASE
+  wrmsr(MSR_FSBASE, fsbase);
+  wrmsr(MSR_GSBASE, gsbase);
+  wrmsr(MSR_KERNELGSBASE, kgsbase);
+
+  // save guest FPU/MMX/SSE...
+  fxsave(&svm->guest.fpu);
+
+  // restore FPU/MMX/SSE...
+  fxrstor(&p->p_addr->u_pcb.pcb_savefpu);
+
+  if (svm->vmcb->exit_code != -1)
+    svm->flags |= SVM_VM_FL_STARTED;
+
+  wrmsr(MSR_STAR, star);
+  wrmsr(MSR_LSTAR, lstar);
+  wrmsr(MSR_CSTAR, cstar);
+  wrmsr(MSR_SFMASK, sfmask);
+
+  // enable global interrupts so that we can take potential intr now
+  asm volatile("stgi; sti; nop;");
+  // potential interrupt taken, lock the kernel
+  KERNEL_LOCK();
+  
+  // detect if exit occured during exception delivery
+  // XXX handle this case
+  if (svm->vmcb->exit_int_info & (1 << 31))
+    printf("SVM: exit during exception delivery\n");
+
+  // interrupt window
+  if (svm->vmcb->intercept & 0b10000)
+  {
+    svm->vmcb->v_irq = 0;
+    svm->vmcb->v_ign_tpr = 0;
+    svm->vmcb->v_intr_vec = 0;
+    svm->vmcb->intercept &= ~0b10000;
+  }
+  
+  // default KVM exit reason
+  run->exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN;
+  p->p_kvm_vcpu->insn_len = 0;
+  switch (svm->vmcb->exit_code)
+  {
+  case SVM_EXIT_INTR:             // external interrupt
+    break;
+
+  case SVM_EXIT_VINTR:            // interrupt window
+    break;
+    
+  case SVM_EXIT_CPUID:            // CPUID
+    if ((svm->vmcb->rax == 4) || (svm->vmcb->rax == 0xb))
+      i = p->p_kvm_vcpu->cpuid.nent;
+    else
+      for (i = 0; i < p->p_kvm_vcpu->cpuid.nent; i++)
+	if (p->p_kvm_vcpu->cpuid.entries[i].function == svm->vmcb->rax)
+	{
+	  svm->vmcb->rax = p->p_kvm_vcpu->cpuid.entries[i].eax;
+	  svm->guest.rbx = p->p_kvm_vcpu->cpuid.entries[i].ebx;
+	  svm->guest.rcx = p->p_kvm_vcpu->cpuid.entries[i].ecx;
+	  svm->guest.rdx = p->p_kvm_vcpu->cpuid.entries[i].edx;
+	  break;
+	}
+    if (i == p->p_kvm_vcpu->cpuid.nent)
+    {
+      svm->vmcb->rax = 0;
+      svm->guest.rbx = 0;
+      svm->guest.rcx = 0;
+      svm->guest.rdx = 0;
+    }
+    p->p_kvm_vcpu->insn_len = 2;
+    break;
+    
+  case SVM_EXIT_HLT:              // HLT
+    run->exit_reason = KVM_EXIT_HLT;
+    p->p_kvm_vcpu->insn_len = 1;
+    break;
+    
+  case SVM_EXIT_VMMCALL:          // hypercall
+    run->exit_reason = KVM_EXIT_HYPERCALL;
+    run->hypercall.nr = svm->vmcb->rax;
+    run->hypercall.args[0] = svm->guest.rbx;
+    run->hypercall.args[1] = svm->guest.rcx;
+    run->hypercall.args[2] = svm->guest.rdx;
+    run->hypercall.args[3] = svm->guest.rsi;
+    run->hypercall.args[4] = 0;
+    run->hypercall.args[5] = 0;
+    p->p_kvm_vcpu->insn_len = 3;
+    break;
+
+  case SVM_EXIT_IOIO:             // I/O port access
+    run->exit_reason = KVM_EXIT_IO;
+    run->io.direction = svm->vmcb->exit_info1 & 1 ? KVM_EXIT_IO_IN :
+      KVM_EXIT_IO_OUT;
+    run->io.size = (svm->vmcb->exit_info1 >> 4) & 0b111;
+    run->io.port = (svm->vmcb->exit_info1 >> 16) & 0xffff;
+    // offset in kvm_run struct
+    run->io.data_offset = offsetof(struct kvm_run, s);
+    p->p_kvm_vcpu->io.dir = run->io.direction;
+    p->p_kvm_vcpu->io.size = run->io.size;
+    p->p_kvm_vcpu->insn_len = svm->vmcb->exit_info2 - svm->vmcb->rip;
+    p->p_kvm_vcpu->io.user_ptr = (void *)(((vaddr_t)run) + run->io.data_offset);
+    p->p_kvm_vcpu->io.flag = 1;
+    p->p_kvm_vcpu->io.rep = (svm->vmcb->exit_info1 >> 3) & 1;
+    p->p_kvm_vcpu->io.str = (svm->vmcb->exit_info1 >> 2) & 1;
+    p->p_kvm_vcpu->io.addr_size = ((svm->vmcb->exit_info1 >> 7) & 0b111) * 2;
+    p->p_kvm_vcpu->io.seg = (svm->vmcb->exit_info1 >> 10) & 0b111;
+    break;
+    
+  case SVM_EXIT_MSR:              // MSR access
+    val = svm->guest.rcx & 0xffffffff;
+    switch (val)
+    {
+    case MSR_APICBASE:
+      if (!svm->vmcb->exit_info1)
+      {
+	svm->vmcb->rax = p->p_kvm_vcpu->lapic & 0xffffffff;
+	svm->guest.rdx = p->p_kvm_vcpu->lapic >> 32;
+      }
+      else
+	p->p_kvm_vcpu->lapic = (svm->vmcb->rax & 0xffffffff)
+	  | (svm->guest.rdx << 32);
+      break;
+    case MSR_EFER:
+      if (!svm->vmcb->exit_info1)
+      {
+	svm->vmcb->efer |= SVM_EFER_SVME;
+	svm->vmcb->rax = svm->vmcb->efer & 0xffffffff;
+	svm->guest.rdx = svm->vmcb->efer >> 32;
+      }
+      else
+      {
+	svm->vmcb->efer = (svm->vmcb->rax & 0xffffffff)
+	  | (svm->guest.rdx << 32) | SVM_EFER_SVME;
+      }
+      break;
+    default:
+      if (!svm->vmcb->exit_info1)
+      {
+	svm->vmcb->rax = 0;
+	svm->guest.rdx = 0;
+      }
+    }
+    p->p_kvm_vcpu->insn_len = 2;
+    break;
+    
+  case SVM_EXIT_NPF:              // MMIO
+    run->exit_reason = KVM_EXIT_MMIO;
+    p->p_kvm_vcpu->mmio.gpaddr = svm->vmcb->exit_info2;
+    p->p_kvm_vcpu->mmio.acc =
+      ((svm->vmcb->exit_info1 & 1) ? 1 : 0) |
+      ((svm->vmcb->exit_info1 & ((u_int64_t)1 << 1)) ? 0b10 : 0) |
+      ((svm->vmcb->exit_info1 & ((u_int64_t)1 << 4)) ? 0b100 : 0);
+    break;
+    
+  default:
+    printf("SVM: unandled exit %llx\n", svm->vmcb->exit_code);
+    run->exit_reason = KVM_EXIT_UNKNOWN;
+    run->hw.hardware_exit_reason = svm->vmcb->exit_code;
+    break;
+  }
+
+  if ((!svm->vmcb->v_irq) && (!(svm->vmcb->evt_inj & ((u_int32_t)1 << 31))))
+  {
+    run->ready_for_interrupt_injection = 1;
+    run->request_interrupt_window = 0;
+  }
+  else
+    run->ready_for_interrupt_injection = 0;
+  
+  run->if_flag = (svm->vmcb->rflags & ((u_int64_t)1 << 9)) ? 1 : 0;
+  run->flags = 0;
+  run->cr8 = svm->vmcb->v_tpr;
+  
+  if (curcpu()->ci_schedstate.spc_schedflags & SPCF_SHOULDYIELD)
+    preempt(NULL);
+  
+  return (0);
+}
+
+int svm_get_prot(void)
+{
+  return (0);
+}
+
+struct kvm_hw_ptr *svm_get_hw(void)
+{
+  kvm_svm_ptr.init = (void *)svm_init;
+  kvm_svm_ptr.free = (void *)svm_free;
+  kvm_svm_ptr.get_prot = (void *)svm_get_prot;
+  kvm_svm_ptr.get_regs = (void *)svm_get_regs;
+  kvm_svm_ptr.set_regs = (void *)svm_set_regs;
+  kvm_svm_ptr.get_fpu = (void *)svm_get_fpu;
+  kvm_svm_ptr.set_fpu = (void *)svm_set_fpu;
+  kvm_svm_ptr.get_sregs = (void *)svm_get_sregs;
+  kvm_svm_ptr.set_sregs = (void *)svm_set_sregs;
+  kvm_svm_ptr.get_reg_ptr = (void *)svm_get_reg_ptr;
+  kvm_svm_ptr.get_msrs = (void *)svm_get_msrs;
+  kvm_svm_ptr.set_msrs = (void *)svm_set_msrs;
+  kvm_svm_ptr.set_intr = (void *)svm_set_interrupt;
+  kvm_svm_ptr.run = (void *)svm_run;
+  return (&kvm_svm_ptr);
+}
diff --git a/sys/arch/amd64/amd64/vmx.c b/sys/arch/amd64/amd64/vmx.c
new file mode 100644
index 0000000..4402e4f
--- /dev/null
+++ b/sys/arch/amd64/amd64/vmx.c
@@ -0,0 +1,1773 @@
+/*
+ * Copyright (c) 2016 Mickael Torres
+ * All rights reserved.
+ *
+ * Permission to use, copy, modify, and distribute this software for any
+ * purpose with or without fee is hereby granted, provided that the above
+ * copyright notice and this permission notice appear in all copies.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
+ * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
+ * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
+ * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
+ * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
+ * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
+ */
+
+#include <sys/types.h>
+#include <sys/param.h>
+#include <sys/systm.h>
+#include <sys/stdint.h>
+#include <sys/malloc.h>
+#include <sys/lock.h>
+#include <sys/queue.h>
+#include <sys/user.h>
+#include <sys/proc.h>
+
+#include <uvm/uvm.h>
+
+#include <machine/kvm.h>
+#include <machine/vmx.h>
+
+void vmx_exit(void);
+
+// from fpu.c
+#define fxsave(addr)            __asm("fxsave %0" : "=m" (*addr))
+#define fxrstor(addr)           __asm("fxrstor %0" : : "m" (*addr))
+
+// kvm fct ptr
+struct kvm_hw_ptr kvm_vmx_ptr;
+
+// global enable flag
+int vmx_en;
+
+// lock
+struct mutex vmx_mutex;
+
+// vmxon function
+int vmx_vmxon(paddr_t paddr)
+{
+  int ret;
+
+  // VMXON
+  asm volatile(".byte 0xf3,0x0f,0xc7,0x30;" // vmxon [%rax]
+  	       "mov %%rax, %%rcx;"
+	       "lahf;"
+	       "xor %%ebx, %%ebx;"
+	       "mov %%ah, %%bl;"
+	       "and $65, %%bl;"
+	       "mov %%rcx, %%rax;"
+	       : "=b"(ret) : "a"(&paddr) : "rcx");
+
+  return (ret);
+}
+
+// vmptrld function
+int vmx_vmxptrld(paddr_t paddr)
+{
+  int ret;
+
+  // VMXPTRLD
+  asm volatile(".byte 0xf,0xc7,0x30;"       // vmptrld [%rax]
+  	       "mov %%rax, %%rcx;"
+	       "lahf;"
+	       "xor %%ebx, %%ebx;"
+	       "mov %%ah, %%bl;"
+	       "and $65, %%bl;"
+	       "mov %%rcx, %%rax;"
+	       : "=b"(ret) : "a"(&paddr) : "rcx");
+  
+  return (ret);
+}
+
+// vmclear function
+int vmx_vmclear(paddr_t paddr)
+{
+  int ret;
+
+  // VMCLEAR
+  asm volatile(".byte 0x66,0x0f,0xc7,0x30;" // vmclear [%rax]
+  	       "mov %%rax, %%rcx;"
+	       "lahf;"
+	       "xor %%ebx, %%ebx;"
+	       "mov %%ah, %%bl;"
+	       "and $65, %%bl;"
+	       "mov %%rcx, %%rax;"
+	       : "=b"(ret) : "a"(&paddr) : "rcx");
+  
+  return (ret);
+}
+
+// vmread function
+int vmx_vmread(u_int64_t idx, u_int64_t *d)
+{
+  u_int64_t data;
+  int ret;
+
+  // VMREAD
+  asm volatile(".byte 0x0f,0x78,0xc3;"      // VMREAD rax, rbx
+  	       "mov %%rax, %%rdx;"
+	       "lahf;"
+	       "xor %%ecx, %%ecx;"
+	       "mov %%ah, %%cl;"
+	       "and $65, %%cl;"
+	       "mov %%rdx, %%rax;"
+	       : "=c"(ret), "=b"(data) : "a"(idx) : "rdx");
+  *d = data;
+  
+  return (ret);
+}
+
+// vmwrite function
+int vmx_vmwrite(u_int64_t idx, u_int64_t d)
+{
+  int ret;
+
+  // VMWRITE
+  asm volatile(".byte 0x0f,0x79,0xc3;"      // VMWRITE rax, rbx
+  	       "mov %%rax, %%rdx;"
+	       "lahf;"
+	       "xor %%ecx, %%ecx;"
+	       "mov %%ah, %%cl;"
+	       "and $65, %%cl;"
+	       "mov %%rdx, %%rax;"
+	       : "=c"(ret) : "a"(idx), "b"(d) : "rdx");
+  
+  return (ret);
+}
+
+// initialize a new CPU
+struct v_vmx *vmx_init(struct proc *p)
+{
+  struct v_vmx_vpage *v;
+  struct v_vmx *vmx;
+
+  vmx = malloc(sizeof(struct v_vmx), M_MEMDESC, M_WAITOK);
+  if (vmx)
+  {
+    bzero(vmx, sizeof(struct v_vmx));
+
+    // initialize the page list
+    SLIST_INIT(&vmx->vp);
+
+    // allocate the VMCS
+    vmx->vmcs_vregion = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+    pmap_extract(pmap_kernel(), vmx->vmcs_vregion, &(vmx->vmcs_pregion));
+    *(u_int32_t *)(vmx->vmcs_vregion) = p->p_cpu->ci_vmx->vmcs_rev_id;
+
+    // add the entry to the page list
+    v = malloc(sizeof(struct v_vmx_vpage), M_MEMDESC, M_WAITOK);
+    v->vp = vmx->vmcs_vregion;
+    v->pp = vmx->vmcs_pregion;
+    SLIST_INSERT_HEAD(&vmx->vp, v, next);
+    
+    // VMCLEAR
+    vmx_vmclear(vmx->vmcs_pregion);
+
+    // allocate IO and MSR bitmaps
+    vmx->vio_bmap_a = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+    vmx->vio_bmap_b = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+    vmx->vmsr_bmap = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+    pmap_extract(pmap_kernel(), vmx->vio_bmap_a, &vmx->pio_bmap_a);
+    pmap_extract(pmap_kernel(), vmx->vio_bmap_b, &vmx->pio_bmap_b);
+    pmap_extract(pmap_kernel(), vmx->vmsr_bmap, &vmx->pmsr_bmap);
+    
+    // add the entry to the page list
+    v = malloc(sizeof(struct v_vmx_vpage), M_MEMDESC, M_WAITOK);
+    v->vp = vmx->vio_bmap_a;
+    v->pp = vmx->pio_bmap_a;
+    SLIST_INSERT_HEAD(&vmx->vp, v, next);
+    v = malloc(sizeof(struct v_vmx_vpage), M_MEMDESC, M_WAITOK);
+    v->vp = vmx->vio_bmap_b;
+    v->pp = vmx->pio_bmap_b;
+    SLIST_INSERT_HEAD(&vmx->vp, v, next);
+    v = malloc(sizeof(struct v_vmx_vpage), M_MEMDESC, M_WAITOK);
+    v->vp = vmx->vmsr_bmap;
+    v->pp = vmx->pmsr_bmap;
+    SLIST_INSERT_HEAD(&vmx->vp, v, next);
+
+    // allocate MSR host/guest save region
+    vmx->vgmsr = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+    vmx->vhmsr = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+    pmap_extract(pmap_kernel(), vmx->vgmsr, &vmx->pgmsr);
+    pmap_extract(pmap_kernel(), vmx->vhmsr, &vmx->phmsr);
+
+    // add the entry to the page list
+    v = malloc(sizeof(struct v_vmx_vpage), M_MEMDESC, M_WAITOK);
+    v->vp = vmx->vgmsr;
+    v->pp = vmx->pgmsr;
+    SLIST_INSERT_HEAD(&vmx->vp, v, next);
+    v = malloc(sizeof(struct v_vmx_vpage), M_MEMDESC, M_WAITOK);
+    v->vp = vmx->vhmsr;
+    v->pp = vmx->phmsr;
+    SLIST_INSERT_HEAD(&vmx->vp, v, next);
+  }
+  return (vmx);
+}
+
+// free a CPU
+void vmx_free(struct v_vmx *vmx)
+{
+  struct v_vmx_vpage *v;
+  
+  if (vmx)
+  {
+    // free the page list
+    while (!SLIST_EMPTY(&vmx->vp))
+    {
+      v = SLIST_FIRST(&vmx->vp);
+      SLIST_REMOVE_HEAD(&vmx->vp, next);
+      uvm_km_free(kernel_map, v->vp, PAGE_SIZE);
+      free(v, M_MEMDESC, sizeof(struct v_vmx_vpage));
+    }
+    // TODO
+    // make sure to free all
+    free(vmx, M_MEMDESC, sizeof(struct v_vmx));
+  }
+}
+
+// enable VMX
+void vmx_enter(void)
+{
+  struct cpu_info *ci;
+  u_int64_t cr4;
+  u_int64_t msr;
+  u_int64_t paddr;
+  u_int32_t ret;
+
+  // check if VMX is enabled
+  if (!vmx_en)
+    return;
+
+  // acquire lock
+  mtx_enter(&vmx_mutex);
+
+  // set vmx parameters for current cpu
+  ci = curcpu();
+  ci->ci_vmx = (struct vmxinfo *) uvm_km_zalloc(kernel_map,
+						sizeof(*(ci->ci_vmx)));
+  if (!ci->ci_vmx)
+    panic("vmx_enter: can't allocate memory");
+
+  // get vmcs parameters
+  msr = rdmsr(IA32_VMX_BASIC);
+  ci->ci_vmx->vmcs_rev_id = msr & UINT32_MAX;
+  ci->ci_vmx->vmcs_region_size = (msr >> 32) & 0xfff;
+  ci->ci_vmx->vmcs_region_type = (msr >> 50) & 0xf;
+  ci->ci_vmx->vmcs_flags = (msr >> 54) & 3;
+
+  // enabling VMX
+  cr4 = rcr4();
+  cr4 |= CR4_VMXE;
+  lcr4(cr4);
+
+  // allocate VMXON region
+  ci->ci_vmx->vmxon_vregion = uvm_km_zalloc(kernel_map, PAGE_SIZE);
+  if (!ci->ci_vmx->vmxon_vregion)
+    panic("vmx_enter: can't allocate memory");
+  KASSERT(!(ci->ci_vmx->vmxon_vregion % PAGE_SIZE));
+  pmap_extract(pmap_kernel(), ci->ci_vmx->vmxon_vregion,
+	       &ci->ci_vmx->vmxon_pregion);
+  *(u_int32_t *)(ci->ci_vmx->vmxon_vregion) = ci->ci_vmx->vmcs_rev_id;
+  paddr = ci->ci_vmx->vmxon_pregion;
+
+  // VMXON
+  ret = vmx_vmxon(paddr);
+  if (ret)
+  {
+    mtx_leave(&vmx_mutex);
+    return;
+  }
+
+  // mark that VMXON has been executed on this CPU
+  // this allows ioctl to be executed on this CPU
+  ci->ci_vmx->flags |= VMX_VMXON;
+
+  printf("cpu%d: VMX enabled with EPT, %dB region size, rev %d\n",
+	 ci->ci_cpuid, ci->ci_vmx->vmcs_region_size, ci->ci_vmx->vmcs_rev_id);
+
+  // release lock
+  mtx_leave(&vmx_mutex);
+}
+
+// check that VMX with EPT is supported then enable it
+// and call vmx_enter for the BSP
+void vmx_setup(void)
+{
+  u_int64_t msr;
+
+  // not enabled for now
+  vmx_en = 0;
+
+  // check that we are on an intel CPU with VMX
+  if (!(cpu_ecxfeature & CPUIDECX_VMX))
+    return;
+  
+  // check that VMX is enabled at least outside of SMX mode
+  msr = rdmsr(MSR_IA32_FEATURE_CONTROL);
+  if (!(msr & VMX_ENABLE))
+  {
+    if (!(msr & VMX_FEATURE_LOCK)) // not locked, try to update it
+    {
+      wrmsr(MSR_IA32_FEATURE_CONTROL, msr | VMX_FEATURE_LOCK | VMX_ENABLE);
+      msr = rdmsr(MSR_IA32_FEATURE_CONTROL);
+      if (!(msr & VMX_ENABLE)) // can't update
+	return;
+    }
+    else
+      return;
+  }
+
+  // check for secondary control
+  msr = rdmsr(IA32_VMX_PROCBASED_CTLS);
+  if (!(msr & VMX_ACTIVATE_SECONDARY_CONTROL))
+    return;
+
+  // check that EPT is supported
+  msr = rdmsr(IA32_VMX_PROCBASED2_CTLS);
+  if (!(msr & VMX_ENABLE_EPT))
+    return;
+
+  // check that 2MB pages EPT is supported
+  //  msr = rdmsr(IA32_VMX_EPT_VPID_CAP);
+  //  if (!(msr & VMX_EPT_2MB))
+  //    return;
+
+  // check that unrestricted guest is supported
+  msr = rdmsr(IA32_VMX_MISC);
+  if (!(msr & VMX_UNRESTRICTED_GUEST))
+    return;
+
+  // initialize lock
+  mtx_init(&vmx_mutex, IPL_HIGH);
+
+  // we support VMX, enable it for APs
+  vmx_en = 1;
+
+  // enable VMX mode
+  vmx_enter();
+}
+
+int vmx_get_regs(struct v_vmx *vmx, struct kvm_regs *r)
+{
+  int ret;
+
+  ret = 0;
+
+  // VMXPTRLD
+  ret = vmx_vmxptrld(vmx->vmcs_pregion);
+  if (ret)
+    return (-EFAULT);
+
+  // read registers
+  ret |= vmx_vmread(VMX_VMCS_REG_RIP, &r->rip);
+  ret |= vmx_vmread(VMX_VMCS_REG_RFLAGS, &r->rflags);
+  ret |= vmx_vmread(VMX_VMCS_REG_RSP, &r->rsp);
+
+  r->rax = vmx->guest.rax;
+  r->rbx = vmx->guest.rbx;
+  r->rcx = vmx->guest.rcx;
+  r->rdx = vmx->guest.rdx;
+  r->rsi = vmx->guest.rsi;
+  r->rdi = vmx->guest.rdi;
+  r->rbp = vmx->guest.rbp;
+  r->r8 = vmx->guest.r8;
+  r->r9 = vmx->guest.r9;
+  r->r10 = vmx->guest.r10;
+  r->r11 = vmx->guest.r11;
+  r->r12 = vmx->guest.r12;
+  r->r13 = vmx->guest.r13;
+  r->r14 = vmx->guest.r14;
+  r->r15 = vmx->guest.r15;
+
+  // VMCLEAR
+  ret |= vmx_vmclear(vmx->vmcs_pregion);
+  
+  if (ret)
+    return (-EFAULT);
+  
+  return (0);
+}
+
+int vmx_set_regs(struct v_vmx *vmx, struct kvm_regs *r)
+{
+  int ret;
+
+  ret = 0;
+
+  // VMXPTRLD
+  ret = vmx_vmxptrld(vmx->vmcs_pregion);
+  if (ret)
+    return (-EFAULT);
+
+  // write registers
+  ret |= vmx_vmwrite(VMX_VMCS_REG_RIP, r->rip);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_RFLAGS, r->rflags);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_RSP, r->rsp);
+
+  vmx->guest.rax = r->rax;
+  vmx->guest.rbx = r->rbx;
+  vmx->guest.rcx = r->rcx;
+  vmx->guest.rdx = r->rdx;
+  vmx->guest.rsi = r->rsi;
+  vmx->guest.rdi = r->rdi;
+  vmx->guest.rbp = r->rbp;
+  vmx->guest.r8 = r->r8;
+  vmx->guest.r9 = r->r9;
+  vmx->guest.r10 = r->r10;
+  vmx->guest.r11 = r->r11;
+  vmx->guest.r12 = r->r12;
+  vmx->guest.r13 = r->r13;
+  vmx->guest.r14 = r->r14;
+  vmx->guest.r15 = r->r15;
+
+  // VMCLEAR
+  ret |= vmx_vmclear(vmx->vmcs_pregion);
+  
+  if (ret)
+    return (-EFAULT);
+  
+  return (0);
+}
+
+int vmx_get_fpu(struct v_vmx *vmx, struct kvm_fpu *f)
+{
+  f->fcw = vmx->guest.fpu.fp_fxsave.fx_fcw;
+  f->fsw = vmx->guest.fpu.fp_fxsave.fx_fsw;
+  f->ftwx = vmx->guest.fpu.fp_fxsave.fx_ftw;
+  f->last_opcode = vmx->guest.fpu.fp_fxsave.fx_fop;
+  f->last_ip = vmx->guest.fpu.fp_fxsave.fx_rip;
+  f->last_dp = vmx->guest.fpu.fp_fxsave.fx_rdp;
+  f->mxcsr = vmx->guest.fpu.fp_fxsave.fx_mxcsr;
+  memcpy(f->fpr, vmx->guest.fpu.fp_fxsave.fx_st, 128);
+  memcpy(f->xmm, vmx->guest.fpu.fp_fxsave.fx_xmm, 256);
+
+  return (0);
+}
+
+int vmx_set_fpu(struct v_vmx *vmx, struct kvm_fpu *f)
+{
+  vmx->guest.fpu.fp_fxsave.fx_fcw = f->fcw;
+  vmx->guest.fpu.fp_fxsave.fx_fsw = f->fsw;
+  vmx->guest.fpu.fp_fxsave.fx_ftw = f->ftwx;
+  vmx->guest.fpu.fp_fxsave.fx_fop = f->last_opcode;
+  vmx->guest.fpu.fp_fxsave.fx_rip = f->last_ip;
+  vmx->guest.fpu.fp_fxsave.fx_rdp = f->last_dp;
+  vmx->guest.fpu.fp_fxsave.fx_mxcsr = f->mxcsr;
+  memcpy(vmx->guest.fpu.fp_fxsave.fx_st, f->fpr, 128);
+  memcpy(vmx->guest.fpu.fp_fxsave.fx_xmm, f->xmm, 256);
+  
+  return (0);
+}
+
+int vmx_get_sregs(struct v_vmx *vmx, struct kvm_sregs *s, struct kvm_p *kp)
+{
+  u_int64_t val;
+  int ret;
+
+  ret = 0;
+
+  // VMXPTRLD
+  ret = vmx_vmxptrld(vmx->vmcs_pregion);
+  if (ret)
+    return (-EFAULT);
+
+  // read registers
+  // CS
+  ret |= vmx_vmread(VMX_VMCS_REG_CS_SEL, &val);
+  s->cs.selector = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_CS_BASE, &val);
+  s->cs.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_CS_LIM, &val);
+  s->cs.limit = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_CS_ACC, &val);
+  s->cs.type = val & 0b1111;
+  s->cs.present = (val >> 7) & 1;
+  s->cs.dpl = (val >> 5) & 0b11;
+  s->cs.db = (val >> 14) & 1;
+  s->cs.s = (val >> 4) & 1;
+  s->cs.l = (val >> 13) & 1;
+  s->cs.g = (val >> 15) & 1;
+  s->cs.avl = (val >> 12) & 1;
+  s->cs.unusable = (val >> 16) & 1;
+  // DS
+  ret |= vmx_vmread(VMX_VMCS_REG_DS_SEL, &val);
+  s->ds.selector = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_DS_BASE, &val);
+  s->ds.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_DS_LIM, &val);
+  s->ds.limit = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_DS_ACC, &val);
+  s->ds.type = val & 0b1111;
+  s->ds.present = (val >> 7) & 1;
+  s->ds.dpl = (val >> 5) & 0b11;
+  s->ds.db = (val >> 14) & 1;
+  s->ds.s = (val >> 4) & 1;
+  s->ds.l = (val >> 13) & 1;
+  s->ds.g = (val >> 15) & 1;
+  s->ds.avl = (val >> 12) & 1;
+  s->ds.unusable = (val >> 16) & 1;
+  // ES
+  ret |= vmx_vmread(VMX_VMCS_REG_ES_SEL, &val);
+  s->es.selector = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_ES_BASE, &val);
+  s->es.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_ES_LIM, &val);
+  s->es.limit = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_ES_ACC, &val);
+  s->es.type = val & 0b1111;
+  s->es.present = (val >> 7) & 1;
+  s->es.dpl = (val >> 5) & 0b11;
+  s->es.db = (val >> 14) & 1;
+  s->es.s = (val >> 4) & 1;
+  s->es.l = (val >> 13) & 1;
+  s->es.g = (val >> 15) & 1;
+  s->es.avl = (val >> 12) & 1;
+  s->es.unusable = (val >> 16) & 1;
+  // FS
+  ret |= vmx_vmread(VMX_VMCS_REG_FS_SEL, &val);
+  s->fs.selector = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_FS_BASE, &val);
+  s->fs.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_FS_LIM, &val);
+  s->fs.limit = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_FS_ACC, &val);
+  s->fs.type = val & 0b1111;
+  s->fs.present = (val >> 7) & 1;
+  s->fs.dpl = (val >> 5) & 0b11;
+  s->fs.db = (val >> 14) & 1;
+  s->fs.s = (val >> 4) & 1;
+  s->fs.l = (val >> 13) & 1;
+  s->fs.g = (val >> 15) & 1;
+  s->fs.avl = (val >> 12) & 1;
+  s->fs.unusable = (val >> 16) & 1;
+  // GS
+  ret |= vmx_vmread(VMX_VMCS_REG_GS_SEL, &val);
+  s->gs.selector = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_GS_BASE, &val);
+  s->gs.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_GS_LIM, &val);
+  s->gs.limit = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_GS_ACC, &val);
+  s->gs.type = val & 0b1111;
+  s->gs.present = (val >> 7) & 1;
+  s->gs.dpl = (val >> 5) & 0b11;
+  s->gs.db = (val >> 14) & 1;
+  s->gs.s = (val >> 4) & 1;
+  s->gs.l = (val >> 13) & 1;
+  s->gs.g = (val >> 15) & 1;
+  s->gs.avl = (val >> 12) & 1;
+  s->gs.unusable = (val >> 16) & 1;
+  // SS
+  ret |= vmx_vmread(VMX_VMCS_REG_SS_SEL, &val);
+  s->ss.selector = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_SS_BASE, &val);
+  s->ss.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_SS_LIM, &val);
+  s->ss.limit = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_SS_ACC, &val);
+  s->ss.type = val & 0b1111;
+  s->ss.present = (val >> 7) & 1;
+  s->ss.dpl = (val >> 5) & 0b11;
+  s->ss.db = (val >> 14) & 1;
+  s->ss.s = (val >> 4) & 1;
+  s->ss.l = (val >> 13) & 1;
+  s->ss.g = (val >> 15) & 1;
+  s->ss.avl = (val >> 12) & 1;
+  s->ss.unusable = (val >> 16) & 1;
+  // TR
+  ret |= vmx_vmread(VMX_VMCS_REG_TR_SEL, &val);
+  s->tr.selector = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_TR_BASE, &val);
+  s->tr.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_TR_LIM, &val);
+  s->tr.limit = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_TR_ACC, &val);
+  s->tr.type = val & 0b1111;
+  s->tr.present = (val >> 7) & 1;
+  s->tr.dpl = (val >> 5) & 0b11;
+  s->tr.db = (val >> 14) & 1;
+  s->tr.s = (val >> 4) & 1;
+  s->tr.l = (val >> 13) & 1;
+  s->tr.g = (val >> 15) & 1;
+  s->tr.avl = (val >> 12) & 1;
+  s->tr.unusable = (val >> 16) & 1;
+  // LDTR
+  ret |= vmx_vmread(VMX_VMCS_REG_LDTR_SEL, &val);
+  s->ldt.selector = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_LDTR_BASE, &val);
+  s->ldt.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_LDTR_LIM, &val);
+  s->ldt.limit = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_LDTR_ACC, &val);
+  s->ldt.type = val & 0b1111;
+  s->ldt.present = (val >> 7) & 1;
+  s->ldt.dpl = (val >> 5) & 0b11;
+  s->ldt.db = (val >> 14) & 1;
+  s->ldt.s = (val >> 4) & 1;
+  s->ldt.l = (val >> 13) & 1;
+  s->ldt.g = (val >> 15) & 1;
+  s->ldt.avl = (val >> 12) & 1;
+  s->ldt.unusable = (val >> 16) & 1;
+  // GDT
+  ret |= vmx_vmread(VMX_VMCS_REG_GDTR_BASE, &val);
+  s->gdt.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_GDTR_LIM, &val);
+  s->gdt.limit = val;
+  // IDT
+  ret |= vmx_vmread(VMX_VMCS_REG_IDTR_BASE, &val);
+  s->idt.base = val;
+  ret |= vmx_vmread(VMX_VMCS_REG_IDTR_LIM, &val);
+  s->idt.limit = val;
+  // CR
+  ret |= vmx_vmread(VMX_VMCS_REG_CR0, &s->cr0);
+  s->cr2 = vmx->guest.cr2;
+  ret |= vmx_vmread(VMX_VMCS_REG_CR3, &s->cr3);
+  ret |= vmx_vmread(VMX_VMCS_REG_CR4, &s->cr4);
+  s->cr8 = vmx->guest.cr8;
+  // EFER
+  ret |= vmx_vmread(VMX_VMCS_REG_IA32_EFER, &s->efer);
+  // APIC-access
+  s->apic_base = curproc->p_kvm_vcpu->lapic;
+  // interrupts
+  s->interrupt_bitmap[0] = vmx->intr[0];
+  s->interrupt_bitmap[1] = vmx->intr[1];
+  s->interrupt_bitmap[2] = vmx->intr[2];
+  s->interrupt_bitmap[3] = vmx->intr[3];
+  
+  // VMCLEAR
+  ret |= vmx_vmclear(vmx->vmcs_pregion);
+  
+  if (ret)
+    return (-EFAULT);
+  
+  return (0);
+}
+
+int vmx_set_sregs(struct v_vmx *vmx, struct kvm_sregs *s, struct kvm_p *kp)
+{
+  u_int64_t cr0_0;
+  u_int64_t cr0_1;
+  u_int64_t cr4_0;
+  u_int64_t cr4_1;
+  u_int64_t val;
+  int ret;
+
+  ret = 0;
+
+  cr0_0 = rdmsr(IA32_VMX_CR0_FIXED0);
+  cr0_1 = rdmsr(IA32_VMX_CR0_FIXED1);
+  cr4_0 = rdmsr(IA32_VMX_CR4_FIXED0);
+  cr4_1 = rdmsr(IA32_VMX_CR4_FIXED1);
+  cr0_0 &= 0x7ffffffe; // unrestricted guest permits to relax PG, PE
+  
+  // VMXPTRLD
+  ret = vmx_vmxptrld(vmx->vmcs_pregion);
+  if (ret)
+    return (-EFAULT);
+
+  // write registers
+  // CS
+  val = s->cs.selector;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_CS_SEL, val);
+  val = s->cs.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_CS_BASE, val);
+  val = s->cs.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_CS_LIM, val);
+  val = (s->cs.type & 0b1111) |
+    ((s->cs.present & 1) << 7) |
+    ((s->cs.dpl & 0b11) << 5) |
+    ((s->cs.db & 1) << 14) |
+    ((s->cs.s & 1) << 4) |
+    ((s->cs.l & 1) << 13) |
+    ((s->cs.g & 1) << 15) |
+    ((s->cs.avl & 1) << 12) |
+    ((s->cs.unusable & 1) << 16);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_CS_ACC, val);
+  // DS
+  val = s->ds.selector;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_DS_SEL, val);
+  val = s->ds.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_DS_BASE, val);
+  val = s->ds.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_DS_LIM, val);
+  val = (s->ds.type & 0b1111) |
+    ((s->ds.present & 1) << 7) |
+    ((s->ds.dpl & 0b11) << 5) |
+    ((s->ds.db & 1) << 14) |
+    ((s->ds.s & 1) << 4) |
+    ((s->ds.l & 1) << 13) |
+    ((s->ds.g & 1) << 15) |
+    ((s->ds.avl & 1) << 12) |
+    ((s->ds.unusable & 1) << 16);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_DS_ACC, val);
+  // ES
+  val = s->es.selector;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_ES_SEL, val);
+  val = s->es.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_ES_BASE, val);
+  val = s->es.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_ES_LIM, val);
+  val = (s->es.type & 0b1111) |
+    ((s->es.present & 1) << 7) |
+    ((s->es.dpl & 0b11) << 5) |
+    ((s->es.db & 1) << 14) |
+    ((s->es.s & 1) << 4) |
+    ((s->es.l & 1) << 13) |
+    ((s->es.g & 1) << 15) |
+    ((s->es.avl & 1) << 12) |
+    ((s->es.unusable & 1) << 16);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_ES_ACC, val);
+  // FS
+  val = s->fs.selector;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_FS_SEL, val);
+  val = s->fs.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_FS_BASE, val);
+  val = s->fs.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_FS_LIM, val);
+  val = (s->fs.type & 0b1111) |
+    ((s->fs.present & 1) << 7) |
+    ((s->fs.dpl & 0b11) << 5) |
+    ((s->fs.db & 1) << 14) |
+    ((s->fs.s & 1) << 4) |
+    ((s->fs.l & 1) << 13) |
+    ((s->fs.g & 1) << 15) |
+    ((s->fs.avl & 1) << 12) |
+    ((s->fs.unusable & 1) << 16);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_FS_ACC, val);
+  // GS
+  val = s->gs.selector;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_GS_SEL, val);
+  val = s->gs.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_GS_BASE, val);
+  val = s->gs.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_GS_LIM, val);
+  val = (s->gs.type & 0b1111) |
+    ((s->gs.present & 1) << 7) |
+    ((s->gs.dpl & 0b11) << 5) |
+    ((s->gs.db & 1) << 14) |
+    ((s->gs.s & 1) << 4) |
+    ((s->gs.l & 1) << 13) |
+    ((s->gs.g & 1) << 15) |
+    ((s->gs.avl & 1) << 12) |
+    ((s->gs.unusable & 1) << 16);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_GS_ACC, val);
+  // SS
+  val = s->ss.selector;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_SS_SEL, val);
+  val = s->ss.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_SS_BASE, val);
+  val = s->ss.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_SS_LIM, val);
+  val = (s->ss.type & 0b1111) |
+    ((s->ss.present & 1) << 7) |
+    ((s->ss.dpl & 0b11) << 5) |
+    ((s->ss.db & 1) << 14) |
+    ((s->ss.s & 1) << 4) |
+    ((s->ss.l & 1) << 13) |
+    ((s->ss.g & 1) << 15) |
+    ((s->ss.avl & 1) << 12) |
+    ((s->ss.unusable & 1) << 16);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_SS_ACC, val);
+  // TR
+  val = s->tr.selector;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_TR_SEL, val);
+  val = s->tr.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_TR_BASE, val);
+  val = s->tr.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_TR_LIM, val);
+  val = (s->tr.type & 0b1111) |
+    ((s->tr.present & 1) << 7) |
+    ((s->tr.dpl & 0b11) << 5) |
+    ((s->tr.db & 1) << 14) |
+    ((s->tr.s & 1) << 4) |
+    ((s->tr.l & 1) << 13) |
+    ((s->tr.g & 1) << 15) |
+    ((s->tr.avl & 1) << 12) |
+    ((s->tr.unusable & 1) << 16);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_TR_ACC, val);
+  // LDTR
+  val = s->ldt.selector;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_LDTR_SEL, val);
+  val = s->ldt.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_LDTR_BASE, val);
+  val = s->ldt.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_LDTR_LIM, val);
+  val = (s->ldt.type & 0b1111) |
+    ((s->ldt.present & 1) << 7) |
+    ((s->ldt.dpl & 0b11) << 5) |
+    ((s->ldt.db & 1) << 14) |
+    ((s->ldt.s & 1) << 4) |
+    ((s->ldt.l & 1) << 13) |
+    ((s->ldt.g & 1) << 15) |
+    ((s->ldt.avl & 1) << 12) |
+    ((s->ldt.unusable & 1) << 16);
+  ret |= vmx_vmwrite(VMX_VMCS_REG_LDTR_ACC, val);
+  // GDT
+  val = s->gdt.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_GDTR_BASE, val);
+  val = s->gdt.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_GDTR_LIM, val);
+  // IDT
+  val =s->idt.base;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_IDTR_BASE, val);
+  val = s->idt.limit;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_IDTR_LIM, val);
+  // CR
+  val = s->cr0 | cr0_0;
+  val &= cr0_1;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_CR0, val);
+  vmx->guest.cr2 = s->cr2;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_CR3, s->cr3);
+  val = s->cr4 | cr4_0;
+  val &= cr4_1;
+  ret |= vmx_vmwrite(VMX_VMCS_REG_CR4, val);
+  vmx->guest.cr8 = s->cr8;
+  // EFER
+  ret |= vmx_vmwrite(VMX_VMCS_REG_IA32_EFER, s->efer);
+  // APIC-access
+  curproc->p_kvm_vcpu->lapic = s->apic_base;
+  // interrupts
+  vmx->intr[0] = s->interrupt_bitmap[0];
+  vmx->intr[1] = s->interrupt_bitmap[1];
+  vmx->intr[2] = s->interrupt_bitmap[2];
+  vmx->intr[3] = s->interrupt_bitmap[3];
+  
+  // VMCLEAR
+  ret |= vmx_vmclear(vmx->vmcs_pregion);
+  
+  if (ret)
+    return (-EFAULT);
+  
+  return (0);
+}
+
+void *vmx_get_reg_ptr(struct v_vmx *vmx, int reg)
+{
+  switch (reg)
+  {
+  case KVM_REG_RAX:
+    return ((void *)&(vmx->guest.rax));
+  case KVM_REG_RBX:
+    return ((void *)&(vmx->guest.rbx));
+  case KVM_REG_RCX:
+    return ((void *)&(vmx->guest.rcx));
+  case KVM_REG_RDX:
+    return ((void *)&(vmx->guest.rdx));
+  case KVM_REG_RSI:
+    return ((void *)&(vmx->guest.rsi));
+  case KVM_REG_RDI:
+    return ((void *)&(vmx->guest.rdi));
+  case KVM_REG_RBP:
+    return ((void *)&(vmx->guest.rbp));
+  case KVM_REG_R8:
+    return ((void *)&(vmx->guest.r8));
+  case KVM_REG_R9:
+    return ((void *)&(vmx->guest.r9));
+  case KVM_REG_R10:
+    return ((void *)&(vmx->guest.r10));
+  case KVM_REG_R11:
+    return ((void *)&(vmx->guest.r11));
+  case KVM_REG_R12:
+    return ((void *)&(vmx->guest.r12));
+  case KVM_REG_R13:
+    return ((void *)&(vmx->guest.r13));
+  case KVM_REG_R14:
+    return ((void *)&(vmx->guest.r14));
+  case KVM_REG_R15:
+    return ((void *)&(vmx->guest.r15));
+  }
+  return ((void *)1);
+}
+
+int vmx_get_msrs(struct v_vmx *vmx, struct kvm_msrs *m)
+{
+  int i;
+  int ret;
+
+  ret = 0;
+
+  // VMXPTRLD
+  ret = vmx_vmxptrld(vmx->vmcs_pregion);
+  if (ret)
+    return (-EFAULT);
+  
+  for (i = 0; i < m->nmsrs; i++)
+    switch(m->entries[i].index)
+    {
+    case MSR_EFER:
+      vmx_vmread(VMX_VMCS_REG_IA32_EFER, &(m->entries[i].data));
+      break;
+    case MSR_SYSENTER_CS:
+      vmx_vmread(VMX_VMCS_REG_IA32_SYSENTER_CS, &(m->entries[i].data));
+      break;
+    case MSR_SYSENTER_ESP:
+      vmx_vmread(VMX_VMCS_REG_IA32_SYSENTER_ESP, &(m->entries[i].data));
+      break;
+    case MSR_SYSENTER_EIP:
+      vmx_vmread(VMX_VMCS_REG_IA32_SYSENTER_EIP, &(m->entries[i].data));
+      break;
+    case MSR_STAR:
+      m->entries[i].data = ((struct vmx_msr_entry *)vmx->vgmsr)[0].data;
+      break;
+    case MSR_LSTAR:
+      m->entries[i].data = ((struct vmx_msr_entry *)vmx->vgmsr)[1].data;
+      break;
+    case MSR_CSTAR:
+      m->entries[i].data = ((struct vmx_msr_entry *)vmx->vgmsr)[2].data;
+      break;
+    case MSR_SFMASK:
+      m->entries[i].data = ((struct vmx_msr_entry *)vmx->vgmsr)[3].data;
+      break;
+    case MSR_FSBASE:
+      vmx_vmread(VMX_VMCS_REG_FS_BASE, &(m->entries[i].data));
+      break;
+    case MSR_GSBASE:
+      vmx_vmread(VMX_VMCS_REG_GS_BASE, &(m->entries[i].data));
+      break;
+    case MSR_KERNELGSBASE:
+      m->entries[i].data = ((struct vmx_msr_entry *)vmx->vgmsr)[4].data;
+      break;
+    case MSR_CR_PAT:
+      vmx_vmread(VMX_VMCS_REG_IA32_PAT, &(m->entries[i].data));
+      break;
+    case MSR_PERF_GLOBAL_CTRL:
+      vmx_vmread(VMX_VMCS_REG_IA32_PERF_GLOBAL_CTRL, &(m->entries[i].data));
+      break;
+    default:
+      m->entries[i].data = 0;
+      break;
+    }
+
+  // VMCLEAR
+  ret |= vmx_vmclear(vmx->vmcs_pregion);
+  
+  if (ret)
+    return (-EFAULT);
+
+  return (0);
+}
+
+int vmx_set_msrs(struct v_vmx *vmx, struct kvm_msrs *m)
+{
+  int i;
+  int ret;
+
+  ret = 0;
+
+  // VMXPTRLD
+  ret = vmx_vmxptrld(vmx->vmcs_pregion);
+  if (ret)
+    return (-EFAULT);
+  
+  for (i = 0; i < m->nmsrs; i++)
+    switch(m->entries[i].index)
+    {
+    case MSR_EFER:
+      vmx_vmwrite(VMX_VMCS_REG_IA32_EFER, m->entries[i].data);
+      break;
+    case MSR_SYSENTER_CS:
+      vmx_vmwrite(VMX_VMCS_REG_IA32_SYSENTER_CS, m->entries[i].data);
+      break;
+    case MSR_SYSENTER_ESP:
+      vmx_vmwrite(VMX_VMCS_REG_IA32_SYSENTER_ESP, m->entries[i].data);
+      break;
+    case MSR_SYSENTER_EIP:
+      vmx_vmwrite(VMX_VMCS_REG_IA32_SYSENTER_EIP, m->entries[i].data);
+      break;
+    case MSR_STAR:
+      ((struct vmx_msr_entry *)vmx->vgmsr)[0].data = m->entries[i].data;
+      break;
+    case MSR_LSTAR:
+      ((struct vmx_msr_entry *)vmx->vgmsr)[1].data = m->entries[i].data;
+      break;
+    case MSR_CSTAR:
+      ((struct vmx_msr_entry *)vmx->vgmsr)[2].data = m->entries[i].data;
+      break;
+    case MSR_SFMASK:
+      ((struct vmx_msr_entry *)vmx->vgmsr)[3].data = m->entries[i].data;
+      break;
+    case MSR_FSBASE:
+      vmx_vmwrite(VMX_VMCS_REG_FS_BASE, m->entries[i].data);
+      break;
+    case MSR_GSBASE:
+      vmx_vmwrite(VMX_VMCS_REG_GS_BASE, m->entries[i].data);
+      break;
+    case MSR_KERNELGSBASE:
+      ((struct vmx_msr_entry *)vmx->vgmsr)[4].data = m->entries[i].data;
+      break;
+    case MSR_CR_PAT:
+	vmx_vmwrite(VMX_VMCS_REG_IA32_PAT, m->entries[i].data);
+      break;
+    case MSR_PERF_GLOBAL_CTRL:
+      vmx_vmwrite(VMX_VMCS_REG_IA32_PERF_GLOBAL_CTRL, m->entries[i].data);
+      break;
+    }
+
+  // VMCLEAR
+  ret |= vmx_vmclear(vmx->vmcs_pregion);
+  
+  if (ret)
+    return (-EFAULT);
+  
+  return (0);
+}
+
+int vmx_set_interrupt(struct v_vmx *vmx, u_int32_t intr)
+{
+  if (intr == 0xffffffff)
+  {
+    if (vmx->nmi == 1)
+      return (-EEXIST);
+    vmx->nmi = 1;
+    return (0);
+  }
+  if (intr > 255)
+    return (-EINVAL);
+  // KVM says at most one bit may be set
+  if (vmx->intr[0] || vmx->intr[1] || vmx->intr[2] || vmx->intr[3])
+    //  if (vmx->intr[intr / 64] & ((u_int64_t) 1 << (intr % 64)))
+    return (-EEXIST);
+  vmx->intr[intr / 64] |= ((u_int64_t) 1 << (intr % 64));
+  
+  return (0);
+}
+
+int vmx_run(struct v_vmx *vmx, struct kvm_run *run, struct kvm_p *kp,
+	    struct proc *p)
+{
+  xdtr_t gdtr;
+  xdtr_t idtr;
+  u_int64_t intr_hdlr;
+  u_int64_t qual;
+  u_int64_t exit;
+  u_int64_t tr;
+  u_int64_t want;
+  u_int64_t val2;
+  u_int64_t val;
+  u_int64_t rsp;
+  u_int64_t ret;
+  u_int64_t cr0_0;
+  u_int64_t cr0_1;
+  u_int64_t cr4_0;
+  u_int64_t cr4_1;
+  int i;
+
+  // save FPU/MMX/SSE... state
+  // XXX implement lazy save/restore
+  fxsave(&p->p_addr->u_pcb.pcb_savefpu);
+
+  // VMXPTRLD
+  ret = vmx_vmxptrld(vmx->vmcs_pregion);
+  if (ret)
+    return (-EFAULT);
+
+  // always save these so we can restore them with their limit after
+  asm volatile("sgdt %0;" :: "m"(gdtr));
+  asm volatile("sidt %0;" :: "m"(idtr));
+
+  ret = 0;
+
+  // for now we set the host state everytime
+  // XXX check if some of it _never_ changes and put it with the
+  // rest of initialization that is only done on the first run
+  // set host state
+  val = rcr0();
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_CR0, val);
+  val = rcr3();
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_CR3, val);
+  val = rcr4();
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_CR4, val);
+  val = rdmsr(MSR_FSBASE);
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_FS_BASE, val);
+  val = rdmsr(MSR_GSBASE);
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_GS_BASE, val);
+  asm volatile("mov %%cs, %%ax;" : "=a"(val));
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_CS_SEL, val & 0xfff8);
+  asm volatile("mov %%ss, %%ax;" : "=a"(val));
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_SS_SEL, val & 0xfff8);
+  asm volatile("mov %%ds, %%ax;" : "=a"(val));
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_DS_SEL, val & 0xfff8);
+  asm volatile("mov %%es, %%ax;" : "=a"(val));
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_ES_SEL, val & 0xfff8);
+  asm volatile("mov %%fs, %%ax;" : "=a"(val));
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_FS_SEL, val & 0xfff8);
+  asm volatile("mov %%gs, %%ax;" : "=a"(val));
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_GS_SEL, val & 0xfff8);
+  asm volatile("str %%ax;" : "=a"(tr));
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_TR_SEL, tr & 0xfff8);
+  val2 = tr & 0xfff8;
+  // GDT
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_GDTR_BASE, gdtr.base);
+  // TR
+  val2 += gdtr.base;
+  val = *(u_int16_t *)(val2 + 2);
+  val |= (u_int64_t)(*(u_int8_t *)(val2 + 4)) << 16;
+  val |= (u_int64_t)(*(u_int8_t *)(val2 + 7)) << 24;
+  val |= (u_int64_t)(*(u_int32_t *)(val2 + 8)) << 32;
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_TR_BASE, val);
+  // IDT
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_IDTR_BASE, idtr.base);
+  // MSRs
+  val = rdmsr(MSR_SYSENTER_CS);
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_IA32_SYSENTER_CS, val & 0xffffffff);
+  val = rdmsr(MSR_SYSENTER_EIP);
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_IA32_SYSENTER_EIP, val);
+  val = rdmsr(MSR_SYSENTER_ESP);
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_IA32_SYSENTER_ESP, val);
+  val = rdmsr(MSR_CR_PAT);
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_IA32_PAT, val);
+  val = rdmsr(MSR_EFER);
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_IA32_EFER, val);
+  val = rdmsr(MSR_PERF_GLOBAL_CTRL);
+  ret |= vmx_vmwrite(VMX_VMCS_HOST_IA32_PERF_GLOBAL_CTRL, val);
+
+  // set return control if not already done
+  if (!(vmx->flags & VMX_VM_FL_STARTED))
+  {
+    // set PIN-BASED control
+    val = rdmsr(IA32_VMX_BASIC);
+    if (val & ((u_int64_t)1 << 55))              // depending on bit-55
+      val = rdmsr(IA32_VMX_TRUE_PINBASED_CTLS);  // we use this
+    else
+      val = rdmsr(IA32_VMX_PINBASED_CTLS);       // or this (intel vol 3C A-3)
+    val2 = val >> 32;
+    val &= 0xffffffff;
+    want = 0x29;  // exit on: ext int, nmi, vnmi
+    want |= val;  // add the forced-1 bits
+    want &= val2; // clears the forced-0 bits
+    ret |= vmx_vmwrite(VMX_VMCS_PIN_CTRL, want);
+
+    // set PROC-BASED control
+    val = rdmsr(IA32_VMX_BASIC);
+    if (val & ((u_int64_t)1 << 55))              // depending on bit-55
+      val = rdmsr(IA32_VMX_TRUE_PROCBASED_CTLS); // we use this
+    else
+      val = rdmsr(IA32_VMX_PROCBASED_CTLS);      // or this (intel vol 3C A-3)
+    val2 = val >> 32;
+    val &= 0xffffffff;
+    want = 0x92180080;  // see manual
+    want |= val;        // add the forced-1 bits
+    want &= val2;       // clears the forced-0 bits
+    ret |= vmx_vmwrite(VMX_VMCS_PROC_CTRL, want);
+    vmx->proc_based = want;
+
+    // set MSR guest/host save regions
+    ret |= vmx_vmwrite(VMX_VMCS_ENTRY_MSR_LOAD_COUNT, 5);
+    ret |= vmx_vmwrite(VMX_VMCS_EXIT_MSR_LOAD_COUNT, 5);
+    ret |= vmx_vmwrite(VMX_VMCS_EXIT_MSR_STORE_COUNT, 5);
+    ret |= vmx_vmwrite(VMX_VMCS_ENTRY_MSR_LOAD, vmx->pgmsr);
+    ret |= vmx_vmwrite(VMX_VMCS_EXIT_MSR_STORE, vmx->pgmsr);
+    ret |= vmx_vmwrite(VMX_VMCS_EXIT_MSR_LOAD, vmx->phmsr);
+    ((struct vmx_msr_entry *)vmx->vhmsr)[0].index = MSR_STAR;
+    ((struct vmx_msr_entry *)vmx->vhmsr)[0].data = rdmsr(MSR_STAR);
+    ((struct vmx_msr_entry *)vmx->vhmsr)[1].index = MSR_LSTAR;
+    ((struct vmx_msr_entry *)vmx->vhmsr)[1].data = rdmsr(MSR_LSTAR);
+    ((struct vmx_msr_entry *)vmx->vhmsr)[2].index = MSR_CSTAR;
+    ((struct vmx_msr_entry *)vmx->vhmsr)[2].data = rdmsr(MSR_CSTAR);
+    ((struct vmx_msr_entry *)vmx->vhmsr)[3].index = MSR_SFMASK;
+    ((struct vmx_msr_entry *)vmx->vhmsr)[3].data = rdmsr(MSR_SFMASK);
+    ((struct vmx_msr_entry *)vmx->vhmsr)[4].index = MSR_KERNELGSBASE;
+    ((struct vmx_msr_entry *)vmx->vhmsr)[4].data = rdmsr(MSR_KERNELGSBASE);
+    ((struct vmx_msr_entry *)vmx->vgmsr)[0].index = MSR_STAR;
+    ((struct vmx_msr_entry *)vmx->vgmsr)[1].index = MSR_LSTAR;
+    ((struct vmx_msr_entry *)vmx->vgmsr)[2].index = MSR_CSTAR;
+    ((struct vmx_msr_entry *)vmx->vgmsr)[3].index = MSR_SFMASK;
+    ((struct vmx_msr_entry *)vmx->vgmsr)[4].index = MSR_KERNELGSBASE;
+
+    // set IO and MSR bmap addresses
+    ret |= vmx_vmwrite(VMX_VMCS_IO_BMAP_A, vmx->pio_bmap_a);
+    ret |= vmx_vmwrite(VMX_VMCS_IO_BMAP_B, vmx->pio_bmap_b);
+    ret |= vmx_vmwrite(VMX_VMCS_MSR_BMAP, vmx->pmsr_bmap);
+
+    // set the bitmaps
+    memset((void *)vmx->vio_bmap_a, 0xff, PAGE_SIZE);
+    memset((void *)vmx->vio_bmap_b, 0xff, PAGE_SIZE);
+    memset((void *)vmx->vmsr_bmap, 0xff, PAGE_SIZE);
+    // authorize the guest to read and write the following MSRs
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_EFER);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_SYSENTER_CS);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_SYSENTER_ESP);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_SYSENTER_EIP);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_STAR);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_LSTAR);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_CSTAR);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_SFMASK);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_FSBASE);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_GSBASE);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_KERNELGSBASE);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_CR_PAT);
+    VMX_MSR_BMAP_AUTH(vmx->vmsr_bmap, MSR_PERF_GLOBAL_CTRL);
+
+    // set SECONDARY PROC-BASED control
+    val = rdmsr(IA32_VMX_PROCBASED2_CTLS);
+    val2 = val >> 32;
+    val &= 0xffffffff;
+    want = 0x82; // see manual
+    // want = 0xA2; // see manual
+    want |= val;  // add the forced-1 bits
+    want &= val2; // clears the forced-0 bits
+    ret |= vmx_vmwrite(VMX_VMCS_SEC_PROC_CTRL, 0x82);
+
+    // set VPID
+    ret |= vmx_vmwrite(VMX_VMCS_VPID, kp->id);
+
+    // set PAUSE-LOOP exit values
+    //    ret |= vmx_vmwrite(VMX_VMCS_PLE_GAP, 256); // XXX
+    //    ret |= vmx_vmwrite(VMX_VMCS_PLE_WINDOW, 256); // XXX
+
+    // exit on no exceptions
+    ret |= vmx_vmwrite(VMX_VMCS_EXC_BMAP, 0);
+
+    // no CR3 targets
+    ret |= vmx_vmwrite(VMX_VMCS_CR3_TGT_COUNT, 0);
+
+    // set our EPTP PML4
+    val2 = (3 << 3); // page walk of 4
+    val = rdmsr(IA32_VMX_EPT_VPID_CAP);
+    if (val & (1 << 14))
+      val2 |= 6; // WB
+    ret |= vmx_vmwrite(VMX_VMCS_EPTP, kp->ppml4 | val2);
+
+    // set EXIT control
+    val = rdmsr(IA32_VMX_BASIC);
+    if (val & ((u_int64_t)1 << 55))              // depending on bit-55
+      val = rdmsr(IA32_VMX_TRUE_EXIT_CTLS);      // we use this
+    else
+      val = rdmsr(IA32_VMX_EXIT_CTLS);           // or this (intel vol 3C A-3)
+    val2 = val >> 32;
+    val &= 0xffffffff;
+    want = 0x3c8200; // see manual
+    want |= val;     // add the forced-1 bits
+    want &= val2;    // clears the forced-0 bits
+    ret |= vmx_vmwrite(VMX_VMCS_EXIT_CTRL, want);
+
+    // set ENTRY control
+    val = rdmsr(IA32_VMX_BASIC);
+    if (val & ((u_int64_t)1 << 55))              // depending on bit-55
+      val = rdmsr(IA32_VMX_TRUE_ENTRY_CTLS);     // we use this
+    else
+      val = rdmsr(IA32_VMX_ENTRY_CTLS);          // or this (intel vol 3C A-3)
+    val2 = val >> 32;
+    val &= 0xffffffff;
+    want = 0xe004; // see manual
+    want |= val;   // add the forced-1 bits
+    want &= val2;  // clears the forced-0 bits
+    ret |= vmx_vmwrite(VMX_VMCS_ENTRY_CTRL, want);
+
+    // set our shadow CR0 and CR4
+    ret |= vmx_vmwrite(VMX_VMCS_CR0_MASK, 0xffffffff7ffaffe0);
+    ret |= vmx_vmwrite(VMX_VMCS_CR4_MASK, 0x3800);
+    ret |= vmx_vmwrite(VMX_VMCS_CR0_SHADOW, 0x20);
+    ret |= vmx_vmwrite(VMX_VMCS_CR4_SHADOW, 0);
+
+    // init to 0
+    ret |= vmx_vmwrite(VMX_VMCS_TSC_OFFSET, 0);
+    ret |= vmx_vmwrite(VMX_VMCS_PFAULT_ERR_MASK, 0);
+    ret |= vmx_vmwrite(VMX_VMCS_PFAULT_ERR_MATCH, 0);
+    ret |= vmx_vmwrite(VMX_VMCS_ENTRY_INTR_INF, 0);
+    ret |= vmx_vmwrite(VMX_VMCS_ENTRY_EXC_ERR, 0);
+    ret |= vmx_vmwrite(VMX_VMCS_ENTRY_INSN_LEN, 0);
+    ret |= vmx_vmwrite(VMX_VMCS_LINK_PTR, 0xffffffffffffffff);
+
+    // set host RIP on VM exit
+    val = (u_int64_t)&vmx_exit;
+    ret |= vmx_vmwrite(VMX_VMCS_HOST_RIP, val);
+  }
+
+  // ensure on every launch/resume
+  ret |= vmx_vmwrite(VMX_VMCS_INT_STATE, 0);
+  ret |= vmx_vmwrite(VMX_VMCS_ACT_STATE, 0);
+  ret |= vmx_vmwrite(VMX_VMCS_PENDING_DBG_EXC, 0);
+
+  // exit if a vmwrite failed
+  if (ret)
+  {
+    vmx_vmclear(vmx->vmcs_pregion);
+    return (-EFAULT);
+  }
+
+  asm volatile("pushfq;"                 // save flags
+	       "cli;");                  // and clear interrupts
+  
+  // inject NMI if needed
+  if (vmx->nmi)
+  {
+    vmx_vmwrite(VMX_VMCS_ENTRY_INTR_INF, 2 | (2 << 8) | (1 << 31));
+    vmx->nmi = 0;
+  }
+  else
+    // inject interrupt if needed
+    if (vmx->intr[0] || vmx->intr[1] || vmx->intr[2] || vmx->intr[3])
+    {
+      // if guest rflags.if is clear, we can't inject
+      vmx_vmread(VMX_VMCS_REG_RFLAGS, &ret);
+      if ((ret & ((u_int64_t)1 << 9))) // test IF
+	for (i = 0; i < 256; i++)
+	  if (vmx->intr[i / 64] & ((u_int64_t) 1 << (i % 64)))
+	  {
+	    vmx->intr[i / 64] &= ~((u_int64_t) 1 << (i % 64));
+	    ret = vmx_vmwrite(VMX_VMCS_ENTRY_INTR_INF, i | (1 << 31));
+	    break;
+	  }
+    }
+
+  // load the registers with data from kvm_run
+  vmx->guest.cr8 = run->cr8;
+
+  // ask for an interrupt window exit
+  want = vmx->proc_based;
+  if (run->request_interrupt_window)
+    want |= (1 << 2);
+  ret |= vmx_vmwrite(VMX_VMCS_PROC_CTRL, want);
+  
+  // unlock the kernel
+  KERNEL_UNLOCK();
+
+  // if insn_len is set, we have to advance RIP
+  if (p->p_kvm_vcpu->insn_len)
+  {
+    vmx_vmread(VMX_VMCS_REG_RIP, &exit);
+    vmx_vmwrite(VMX_VMCS_REG_RIP, exit + p->p_kvm_vcpu->insn_len);
+    p->p_kvm_vcpu->insn_len = 0;
+  }
+  
+  // restore guest FPU/MMX/SSE...
+  fxrstor(&(vmx->guest.fpu));
+  
+  asm volatile("push %%rbx;"             // save host context
+	       "push %%rcx;"
+	       "push %%rdx;"
+	       "push %%rsi;"
+	       "push %%rdi;"
+	       "push %%r8;"
+	       "push %%r9;"
+	       "push %%r10;"
+	       "push %%r11;"
+	       "push %%r12;"
+	       "push %%r13;"
+	       "push %%r14;"
+	       "push %%r15;"
+	       "sldt %%bx;"
+	       "push %%rbx;"
+	       "push %%rbp;"
+	       "push %%rcx;"
+	       "mov %%rsp, %%rax;"
+	       : "=a" (rsp) : "c"(&(vmx->guest)) : "rsp", "rbx");
+  vmx_vmwrite(VMX_VMCS_HOST_RSP, rsp);
+  asm volatile("mov %%rax, %%rbp;"     // restore CPU state
+	       "mov 0x78(%%rbp), %%rax;"
+	       "mov %%rax, %%cr2;"
+	       "mov 0x00(%%rbp), %%rax;"
+	       "mov 0x08(%%rbp), %%rbx;"
+	       "mov 0x10(%%rbp), %%rcx;"
+	       "mov 0x18(%%rbp), %%rdx;"
+	       "mov 0x20(%%rbp), %%rsi;"
+	       "mov 0x28(%%rbp), %%rdi;"
+	       "mov 0x30(%%rbp), %%r8;"
+	       "mov 0x38(%%rbp), %%r9;"
+	       "mov 0x40(%%rbp), %%r10;"
+	       "mov 0x48(%%rbp), %%r11;"
+	       "mov 0x50(%%rbp), %%r12;"
+	       "mov 0x58(%%rbp), %%r13;"
+	       "mov 0x60(%%rbp), %%r14;"
+	       "mov 0x68(%%rbp), %%r15;"
+	       "mov 0x70(%%rbp), %%rbp;"
+	       ".byte 0x0f,0x01,0xc2;" // VMLAUNCH
+	       "pop %%rbp;"            // pop useless &vmx->guest
+	       "pop %%rbp;"            // our rbp
+	       "mov $1, %%rax;"        // if we are here, launch failed
+	       "lahf;"
+	       "jmp vmx_restore_ctx;" :: "a"(&(vmx->guest))
+	       : /*"rax",*/ "rbx", "rcx", "rdx", "rsi",
+		 "rdi", "rsp", "r8", "r9", "r10", "r11", "r12", "r13",
+		 "r14", "r15");
+  asm volatile("vmx_exit:"               // a VMEXIT happened
+	       "cli;"
+	       "push %%rbp;"             // save guest rbp
+	       "mov 8(%%rsp), %%rbp;"  // &vmx->guest
+	       "mov %%rax, 0x00(%%rbp);" // save guest context
+	       "mov %%rbx, 0x08(%%rbp);"
+	       "mov %%rcx, 0x10(%%rbp);"
+	       "mov %%rdx, 0x18(%%rbp);"
+	       "mov %%rsi, 0x20(%%rbp);"
+	       "mov %%rdi, 0x28(%%rbp);"
+	       "mov %%r8, 0x30(%%rbp);"
+	       "mov %%r9, 0x38(%%rbp);"
+	       "mov %%r10, 0x40(%%rbp);"
+	       "mov %%r11, 0x48(%%rbp);"
+	       "mov %%r12, 0x50(%%rbp);"
+	       "mov %%r13, 0x58(%%rbp);"
+	       "mov %%r14, 0x60(%%rbp);"
+	       "mov %%r15, 0x68(%%rbp);"
+	       "mov %%cr2, %%rax;"
+	       "mov %%rax, 0x78(%%rbp);"
+	       "pop %%rax;"
+	       "mov %%rax, 0x70(%%rbp);" // guest rbp
+	       "pop %%rbp;"              // &vmp->vmi->guest
+	       "pop %%rbp;"              // our rbp
+	       ::: "rax", "rbx", "rcx", "rdx", "rsi", "rdi", "rsp", "r8", "r9",
+		"r10", "r11", "r12", "r13", "r14", "r15");
+  asm volatile("mov $0, %%rax;"
+	       "vmx_restore_ctx:"        // restore host context
+	       "lgdt %1;"
+	       "lidt %2;"
+	       "pop %%rbx;"
+	       "lldt %%bx;"
+	       "pop %%r15;"
+	       "pop %%r14;"
+	       "pop %%r13;"
+	       "pop %%r12;"
+	       "pop %%r11;"
+	       "pop %%r10;"
+	       "pop %%r9;"
+	       "pop %%r8;"
+	       "pop %%rdi;"
+	       "pop %%rsi;"
+	       "pop %%rdx;"
+	       "pop %%rcx;"
+	       "pop %%rbx;"
+	       : "=a" (ret), "=m"(gdtr), "=m"(idtr)
+	       :: "r15", "r14", "r13", "r12", "r11", "r10", "r9", "r8", "rdi",
+		"rsi", "rdx", "rcx", "rbx", "rsp");
+
+  // save guest FPU/MMX/SSE...
+  fxsave(&(vmx->guest.fpu));
+
+  // restore FPU/MMX/SSE...
+  fxrstor(&(p->p_addr->u_pcb.pcb_savefpu));
+
+  asm volatile("popfq;");  // restore flags
+  
+  // default exit reason
+  run->exit_reason = KVM_EXIT_IRQ_WINDOW_OPEN;
+  
+  vmx_vmread(VMX_VMCS_EXIT_REASON, &exit);
+  if (exit != VMX_REASON_EXT_INTR)
+    KERNEL_LOCK();
+
+  if (!ret)
+  {
+    vmx->flags |= VMX_VM_FL_STARTED;
+
+    // fill the run struct
+    switch (exit)
+    {
+    case VMX_REASON_EXT_INTR: // external interrupt
+      vmx_vmread(VMX_VMCS_EXIT_INTR_INF, &val);
+      if (val & ((u_int64_t)1 << 31)) // exit is valid
+      {
+	val &= 0xff;
+	if (val >= 32) // ext interrupt
+	{
+	  intr_hdlr = ((u_int64_t)idt[val].gd_hioffset << 16) +
+	    idt[val].gd_looffset;
+	  asm volatile("mov %%ss, %%bx;"
+		       "mov %%rsp, %%rcx;"
+		       "andq $0xfffffffffffffff0, %%rsp;"
+		       "mov %%cs, %%dx;"
+		       "pushq %%bx;"
+		       "pushq %%rcx;"
+		       "pushfq;"
+		       "pushq %%dx;"
+		       "cli;"
+		       "callq *%%rax;"
+		       :: "a"(intr_hdlr) : "rbx", "rcx", "rdx");
+	  KERNEL_LOCK();
+	}
+      }
+      break;
+      
+    case VMX_REASON_CPUID: // CPUID
+      if ((vmx->guest.rax == 4) || (vmx->guest.rax == 0xb))
+	i = p->p_kvm_vcpu->cpuid.nent;
+      else
+	for (i = 0; i < p->p_kvm_vcpu->cpuid.nent; i++)
+	  if (p->p_kvm_vcpu->cpuid.entries[i].function == vmx->guest.rax)
+	  {
+	    vmx->guest.rax = p->p_kvm_vcpu->cpuid.entries[i].eax;
+	    vmx->guest.rbx = p->p_kvm_vcpu->cpuid.entries[i].ebx;
+	    vmx->guest.rcx = p->p_kvm_vcpu->cpuid.entries[i].ecx;
+	    vmx->guest.rdx = p->p_kvm_vcpu->cpuid.entries[i].edx;
+	    break;
+	  }
+      if (i == p->p_kvm_vcpu->cpuid.nent)
+      {
+	vmx->guest.rax = 0;
+	vmx->guest.rbx = 0;
+	vmx->guest.rcx = 0;
+	vmx->guest.rdx = 0;
+      }
+      p->p_kvm_vcpu->insn_len = 2;
+      break;
+
+    case VMX_REASON_HLT: // HLT
+      run->exit_reason = KVM_EXIT_HLT;
+      p->p_kvm_vcpu->insn_len = 1;
+      break;
+      
+    case VMX_REASON_VMCALL: // hypercall
+      run->exit_reason = KVM_EXIT_HYPERCALL;
+      run->hypercall.nr = vmx->guest.rax;
+      run->hypercall.args[0] = vmx->guest.rbx;
+      run->hypercall.args[1] = vmx->guest.rcx;
+      run->hypercall.args[2] = vmx->guest.rdx;
+      run->hypercall.args[3] = vmx->guest.rsi;
+      run->hypercall.args[4] = 0;
+      run->hypercall.args[5] = 0;
+      p->p_kvm_vcpu->insn_len = 3;
+      break;
+      
+    case VMX_REASON_IO_INSN: // I/O port access
+      vmx_vmread(VMX_VMCS_EXIT_QUALIF, &qual);
+      run->exit_reason = KVM_EXIT_IO;
+      run->io.direction = ((qual >> 3) & 1) ? KVM_EXIT_IO_IN : KVM_EXIT_IO_OUT;
+      run->io.size = (qual & 0x7) + 1;
+      run->io.port = (qual >> 16) & 0xffff;
+      // offset in kvm_run struct
+      run->io.data_offset = offsetof(struct kvm_run, s);
+      p->p_kvm_vcpu->io.dir = run->io.direction;
+      p->p_kvm_vcpu->io.size = run->io.size;
+      vmx_vmread(VMX_VMCS_EXIT_INSN_LEN, &(p->p_kvm_vcpu->insn_len));
+      p->p_kvm_vcpu->io.user_ptr = (void *)(((vaddr_t)run) + run->io.data_offset);
+      p->p_kvm_vcpu->io.flag = 1;
+      p->p_kvm_vcpu->io.rep = (qual >> 5) & 1;
+      p->p_kvm_vcpu->io.str = (qual >> 4) & 1;
+      vmx_vmread(VMX_VMCS_EXIT_INSN_INF, &qual);
+      p->p_kvm_vcpu->io.seg = (qual >> 15) & 0b111;
+      qual = (qual >> 7) & 0b111;
+      switch (qual)
+      {
+      case 1:
+	p->p_kvm_vcpu->io.addr_size = 4;
+	break;
+      case 2:
+	p->p_kvm_vcpu->io.addr_size = 8;
+	break;
+      default:
+	p->p_kvm_vcpu->io.addr_size = 2;
+	break;
+      }
+      break;
+      
+    case VMX_REASON_RDMSR: // MSR read
+      vmx_vmread(VMX_VMCS_EXIT_INSN_LEN, &(p->p_kvm_vcpu->insn_len));
+      qual = vmx->guest.rcx & 0xffffffff;
+      switch (qual)
+      {
+      case MSR_APICBASE:
+	vmx->guest.rax = p->p_kvm_vcpu->lapic & 0xffffffff;
+	vmx->guest.rdx = p->p_kvm_vcpu->lapic >> 32;
+	break;
+      default:
+	vmx->guest.rax = 0;
+	vmx->guest.rdx = 0;
+	break;
+      }
+      break;
+      
+    case VMX_REASON_WRMSR: // MSR write
+      vmx_vmread(VMX_VMCS_EXIT_INSN_LEN, &(p->p_kvm_vcpu->insn_len));
+      qual = vmx->guest.rcx & 0xffffffff;
+      val = vmx->guest.rax & 0xffffffff;
+      val |= vmx->guest.rdx >> 32;
+      switch (qual)
+      {
+      case MSR_APICBASE:
+	p->p_kvm_vcpu->lapic = (vmx->guest.rax & 0xffffffff)
+	  | (vmx->guest.rdx << 32);
+	break;
+      }
+      break;
+      
+    case VMX_REASON_EPT_FAULT: // EPT fault
+      run->exit_reason = KVM_EXIT_MMIO;
+      vmx_vmread(VMX_VMCS_GUEST_PHYS, &(p->p_kvm_vcpu->mmio.gpaddr));
+      vmx_vmread(VMX_VMCS_EXIT_QUALIF, &qual);
+      p->p_kvm_vcpu->mmio.acc =
+	((qual & (1 << 3)) ? 1 : 0) |
+	((qual & (1 << 1)) ? 0b10 : 0) |
+	((qual & (1 << 2)) ? 0b100 : 0);
+      break;
+
+    case VMX_REASON_INTR_WIN:
+    case VMX_REASON_NMI_WIN:
+      break;
+
+      //    case VMX_REASON_TRIPLE_FAULT:
+      //      run->exit_reason = KVM_EXIT_SYSTEM_EVENT;
+      //      run->system_event.type = 3; // crash
+      //      break;
+
+    case VMX_REASON_CR:
+      vmx_vmread(VMX_VMCS_EXIT_QUALIF, &qual);
+      if (((((qual >> 4) & 0b11) == 1) && ((qual & 0b1111) != 8)) ||
+	  (((qual >> 5) & 0b1) != 0))
+      {
+	printf("VMX: unhandled CR access %llx\n", qual);
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = exit;
+	break;
+      }
+      switch ((qual >> 8) & 0b1111)
+      {
+      case 0: // RAX
+	val = ((u_int64_t)&(vmx->guest.rax));
+	break;
+      case 1: // RCX
+	val = ((u_int64_t)&(vmx->guest.rcx));
+	break;
+      case 2: // RDX
+	val = ((u_int64_t)&(vmx->guest.rdx));
+	break;
+      case 3: // RBX
+	val = ((u_int64_t)&(vmx->guest.rbx));
+	break;
+      case 5: // RBP
+	val = ((u_int64_t)&(vmx->guest.rbp));
+	break;
+      case 6: // RSI
+	val = ((u_int64_t)&(vmx->guest.rsi));
+	break;
+      case 7: // RDI
+	val = ((u_int64_t)&(vmx->guest.rdi));
+	break;
+      case 8: // R8
+	val = ((u_int64_t)&(vmx->guest.r8));
+	break;
+      case 9: // R9
+	val = ((u_int64_t)&(vmx->guest.r9));
+	break;
+      case 10: // R10
+	val = ((u_int64_t)&(vmx->guest.r10));
+	break;
+      case 11: // R11
+	val = ((u_int64_t)&(vmx->guest.r11));
+	break;
+      case 12: // R12
+	val = ((u_int64_t)&(vmx->guest.r12));
+	break;
+      case 13: // R13
+	val = ((u_int64_t)&(vmx->guest.r13));
+	break;
+      case 14: // R14
+	val = ((u_int64_t)&(vmx->guest.r14));
+	break;
+      case 15: // R15
+	val = ((u_int64_t)&(vmx->guest.r15));
+	break;
+      default:
+	printf("VMX: unhandled CR access with RSP\n");
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = exit;
+	break;	
+      }
+      switch (qual & 0b1111)
+      {
+      case 0: // CR0
+	cr0_0 = rdmsr(IA32_VMX_CR0_FIXED0);
+	cr0_1 = rdmsr(IA32_VMX_CR0_FIXED1);
+	cr0_0 &= 0x7ffffffe; // unrestricted guest permits to relax PG, PE
+	(*(u_int64_t *)val) |= cr0_0;
+	(*(u_int64_t *)val) &= cr0_1;
+	vmx_vmwrite(VMX_VMCS_REG_CR0, (*(u_int64_t *)val));
+	if (((*(u_int64_t *)val) & (1 << 31)) &&
+	    ((*(u_int64_t *)val) & 1)) // enabling PG + PE
+	{
+	  vmx_vmread(VMX_VMCS_REG_IA32_EFER, &val);
+	  if ((val & (1 << 8)) && (!(val & (1 << 10)))) // LME set but no LMA
+	  {
+	    // we must have exited during a mode transition to long mode
+	    // we have to handle LMA, if not we'll have an entry failure
+	    vmx_vmread(VMX_VMCS_ENTRY_CTRL, &want);
+	    want |= (1 << 9); // set LMA on next entry
+	    vmx_vmwrite(VMX_VMCS_ENTRY_CTRL, want);
+	    val |= (1 << 10); // set LMA in EFER
+	    vmx_vmwrite(VMX_VMCS_REG_IA32_EFER, val);
+	  }
+	}
+	break;
+      case 4: // CR4
+	cr4_0 = rdmsr(IA32_VMX_CR4_FIXED0);
+	cr4_1 = rdmsr(IA32_VMX_CR4_FIXED1);
+	(*(u_int64_t *)val) |= cr4_0;
+	(*(u_int64_t *)val) &= cr4_1;
+	vmx_vmwrite(VMX_VMCS_REG_CR4, val);
+	break;
+      case 8: // CR8
+	if (((qual >> 4) & 0b11) == 1)
+	  (*(u_int64_t *)val) = vmx->guest.cr8;
+	else
+	  vmx->guest.cr8 = (*(u_int64_t *)val);
+	break;
+      default:
+	printf("VMX: unhandled CR register in CR access %llx\n", qual);
+	run->exit_reason = KVM_EXIT_UNKNOWN;
+	run->hw.hardware_exit_reason = exit;
+	break;
+      }
+      vmx_vmread(VMX_VMCS_EXIT_INSN_LEN, &(p->p_kvm_vcpu->insn_len));
+      break;
+      
+    default:
+      printf("VMX: unhandled vmexit %llx %llx\n", ret, exit);
+      run->exit_reason = KVM_EXIT_UNKNOWN;
+      run->hw.hardware_exit_reason = exit;
+      break;
+    }
+
+    vmx_vmread(VMX_VMCS_REG_RFLAGS, &ret);
+    if ((ret & ((u_int64_t)1 << 9))) // test IF
+    {
+      run->ready_for_interrupt_injection = 1;
+      run->request_interrupt_window = 0;
+    }
+    else
+      run->ready_for_interrupt_injection = 0;
+
+    run->if_flag = (ret & ((u_int64_t)1 << 9)) ? 1 : 0;
+    run->flags = 0;
+    run->cr8 = vmx->guest.cr8;
+  }
+  else
+  {
+    run->exit_reason = KVM_EXIT_FAIL_ENTRY;
+    run->fail_entry.hardware_entry_failure_reason = exit;
+  }
+
+  // VMCLEAR
+  ret = vmx_vmclear(vmx->vmcs_pregion);
+
+  if (curcpu()->ci_schedstate.spc_schedflags & SPCF_SHOULDYIELD)
+    preempt(NULL);
+  
+  if (ret)
+    return (-EFAULT);
+  
+  return (0);
+}
+
+int vmx_get_prot(void)
+{
+  return (0x70); // WB, NO PAT
+}
+
+struct kvm_hw_ptr *vmx_get_hw(void)
+{
+  kvm_vmx_ptr.init = (void *)vmx_init;
+  kvm_vmx_ptr.free = (void *)vmx_free;
+  kvm_vmx_ptr.get_prot = (void *)vmx_get_prot;
+  kvm_vmx_ptr.get_regs = (void *)vmx_get_regs;
+  kvm_vmx_ptr.set_regs = (void *)vmx_set_regs;
+  kvm_vmx_ptr.get_fpu = (void *)vmx_get_fpu;
+  kvm_vmx_ptr.set_fpu = (void *)vmx_set_fpu;
+  kvm_vmx_ptr.get_sregs = (void *)vmx_get_sregs;
+  kvm_vmx_ptr.set_sregs = (void *)vmx_set_sregs;
+  kvm_vmx_ptr.get_reg_ptr = (void *)vmx_get_reg_ptr;
+  kvm_vmx_ptr.get_msrs = (void *)vmx_get_msrs;
+  kvm_vmx_ptr.set_msrs = (void *)vmx_set_msrs;
+  kvm_vmx_ptr.set_intr = (void *)vmx_set_interrupt;
+  kvm_vmx_ptr.run = (void *)vmx_run;
+  return (&kvm_vmx_ptr);
+}
diff --git a/sys/arch/amd64/conf/GENERIC b/sys/arch/amd64/conf/GENERIC
index 8eb1b13..ce6bddd 100644
--- a/sys/arch/amd64/conf/GENERIC
+++ b/sys/arch/amd64/conf/GENERIC
@@ -637,6 +637,7 @@ owctr*	at onewire?		# Counter device
 pseudo-device	pctr		1
 pseudo-device	nvram		1
 pseudo-device	hotplug		1	# devices hot plugging
+pseudo-device	kvm		1	# KVM API
 
 # mouse & keyboard multiplexor pseudo-devices
 pseudo-device	wsmux		2
diff --git a/sys/arch/amd64/conf/files.amd64 b/sys/arch/amd64/conf/files.amd64
index 5d58b34..66c9258 100644
--- a/sys/arch/amd64/conf/files.amd64
+++ b/sys/arch/amd64/conf/files.amd64
@@ -13,6 +13,8 @@ file	arch/amd64/amd64/identcpu.c
 file	arch/amd64/amd64/via.c
 file	arch/amd64/amd64/aes_intel.S		crypto
 file	arch/amd64/amd64/aesni.c		crypto
+file	arch/amd64/amd64/svm.c
+file	arch/amd64/amd64/vmx.c
 file	arch/amd64/amd64/amd64errata.c
 file	arch/amd64/amd64/mem.c
 file	arch/amd64/amd64/amd64_mem.c		mtrr
@@ -240,6 +242,12 @@ file	arch/amd64/amd64/vmm.c			vmm		needs-flag
 file	arch/amd64/amd64/vmm_support.S		vmm
 
 #
+# KVM
+#
+pseudo-device	kvm
+file   arch/amd64/amd64/kvm.c			kvm		needs-flag
+
+#
 # Machine-independent SD/MMC drivers
 #
 include "dev/sdmmc/files.sdmmc"
diff --git a/sys/arch/amd64/include/cpu.h b/sys/arch/amd64/include/cpu.h
index 809766a..e28f075 100644
--- a/sys/arch/amd64/include/cpu.h
+++ b/sys/arch/amd64/include/cpu.h
@@ -183,6 +183,9 @@ struct cpu_info {
 	union		vmm_cpu_cap ci_vmm_cap;
 	paddr_t		ci_vmxon_region_pa;
 	struct vmxon_region *ci_vmxon_region;
+
+  struct vmxinfo		*ci_vmx;
+  struct svminfo		*ci_svm;
 };
 
 #define CPUF_BSP	0x0001		/* CPU is the original BSP */
diff --git a/sys/arch/amd64/include/kvm.h b/sys/arch/amd64/include/kvm.h
new file mode 100644
index 0000000..9233fc3
--- /dev/null
+++ b/sys/arch/amd64/include/kvm.h
@@ -0,0 +1,1044 @@
+#ifndef _MACHINE_KVM_H_
+#define _MACHINE_KVM_H_
+
+#define KVM_RWX 0x7
+#define KVM_RX 0x5
+#define KVM_G2PML4E(X) ((u_int64_t)((X >> 36) & 0xff8))
+#define KVM_G2PDPTE(X) ((u_int64_t)((X >> 27) & 0xff8))
+#define KVM_G2PDE(X) ((u_int64_t)((X >> 18) & 0xff8))
+#define KVM_G2PTE(X) ((u_int64_t)((X >> 9) & 0xff8))
+int kvm_map_range(struct proc *p, struct kvm_p *kp, u_int64_t gaddr,
+		  u_int64_t haddr, u_int64_t size, int unmap, u_int16_t prot);
+int kvm_set_prot(struct kvm_p *kp, u_int64_t gaddr, u_int16_t prot);
+
+#define KVM_REG_RAX 0
+#define KVM_REG_RBX 1
+#define KVM_REG_RCX 2
+#define KVM_REG_RDX 3
+#define KVM_REG_RSI 4
+#define KVM_REG_RDI 5
+#define KVM_REG_RBP 6
+#define KVM_REG_R8 7
+#define KVM_REG_R9 8
+#define KVM_REG_R10 9
+#define KVM_REG_R11 10
+#define KVM_REG_R12 11
+#define KVM_REG_R13 12
+#define KVM_REG_R14 13
+#define KVM_REG_R15 14
+
+#define KVM_MMIO_MOV 0
+#define KVM_MMIO_EN 1
+#define KVM_MMIO_TO_MMIO 2
+#define KVM_MMIO_AND 4
+#define KVM_MMIO_CMP 8
+#define KVM_MMIO_OR 16
+
+#define KVM_FLAGS_CLR_MASK 0xfffffffffffff72a
+#define KVM_FLAGS_SET_MASK 0x8d7
+
+struct v_mmio
+{
+  void *ptr;
+  u_int64_t ptr2;
+  u_int64_t size;
+  u_int32_t dir;
+  u_int32_t flag;
+  u_int8_t acc;
+  u_int64_t gpaddr;
+};
+
+struct v_io
+{
+  void *ptr;
+  void *user_ptr;
+  u_int32_t port;
+  u_int64_t count;
+  u_int64_t size;
+  u_int32_t dir;
+  u_int32_t flag;
+  u_int8_t rep;
+  u_int8_t str;
+  u_int32_t addr_size;
+  u_int8_t seg;
+};
+
+typedef struct xdtr {
+  u_int16_t lim;
+  u_int64_t base;
+} __attribute__ ((packed)) xdtr_t;
+
+#define PRFX_REP 1
+#define PRFX_SEG_ES 2
+#define PRFX_ADDRSIZE 4
+#define PRFX_OPSIZE 8
+#define PRFX_REXR 16
+#define PRFX_REXB 32
+#define PRFX_REXW 64
+#define PRFX_REXX 128
+#define PRFX_REX 256
+
+#define KVM_VIRT_NONE 0
+#define KVM_VIRT_SVM 1
+#define KVM_VIRT_VMX 2
+
+#define KVM_DEFAULT_LAPIC_BASE 0xfee00000
+#define KVM_LAPIC_EN 0x800
+#define KVM_LAPIC_BSP 0x100
+
+#define KVM_DFLT_IDENTITY_BASE 0xfffbc000
+
+#define KVM_API_VERSION 12
+
+#define KVM_NR_INTERRUPTS 256
+
+#define KVM_CAP_IRQCHIP 0
+#define KVM_CAP_HLT 1
+#define KVM_CAP_MMU_SHADOW_CACHE_CONTROL 2
+#define KVM_CAP_USER_MEMORY 3
+#define KVM_CAP_SET_TSS_ADDR 4
+#define KVM_CAP_VAPIC 6
+#define KVM_CAP_EXT_CPUID 7
+#define KVM_CAP_CLOCKSOURCE 8
+#define KVM_CAP_NR_VCPUS 9
+#define KVM_CAP_NR_MEMSLOTS 10
+#define KVM_CAP_PIT 11
+#define KVM_CAP_NOP_IO_DELAY 12
+#define KVM_CAP_PV_MMU 13
+#define KVM_CAP_MP_STATE 14
+#define KVM_CAP_COALESCED_MMIO 15
+#define KVM_CAP_SYNC_MMU 16
+#define KVM_CAP_IOMMU 18
+#define KVM_CAP_DESTROY_MEMORY_REGION_WORKS 21
+#define KVM_CAP_USER_NMI 22
+#define KVM_CAP_SET_GUEST_DEBUG 23
+#define KVM_CAP_REINJECT_CONTROL 24
+#define KVM_CAP_IRQ_ROUTING 25
+#define KVM_CAP_IRQ_INJECT_STATUS 26
+#define KVM_CAP_ASSIGN_DEV_IRQ 29
+#define KVM_CAP_JOIN_MEMORY_REGIONS_WORKS 30
+#define KVM_CAP_MCE 31
+#define KVM_CAP_IRQFD 32
+#define KVM_CAP_PIT2 33
+#define KVM_CAP_SET_BOOT_CPU_ID 34
+#define KVM_CAP_PIT_STATE2 35
+#define KVM_CAP_IOEVENTFD 36
+#define KVM_CAP_SET_IDENTITY_MAP_ADDR 37
+#define KVM_CAP_XEN_HVM 38
+#define KVM_CAP_ADJUST_CLOCK 39
+#define KVM_CAP_INTERNAL_ERROR_DATA 40
+#define KVM_CAP_VCPU_EVENTS 41
+#define KVM_CAP_S390_PSW 42
+#define KVM_CAP_PPC_SEGSTATE 43
+#define KVM_CAP_HYPERV 44
+#define KVM_CAP_HYPERV_VAPIC 45
+#define KVM_CAP_HYPERV_SPIN 46
+#define KVM_CAP_PCI_SEGMENT 47
+#define KVM_CAP_PPC_PAIRED_SINGLES 48
+#define KVM_CAP_INTR_SHADOW 49
+#define KVM_CAP_DEBUGREGS 50
+#define KVM_CAP_X86_ROBUST_SINGLESTEP 51
+#define KVM_CAP_PPC_OSI 52
+#define KVM_CAP_PPC_UNSET_IRQ 53
+#define KVM_CAP_ENABLE_CAP 54
+#define KVM_CAP_XSAVE 55
+#define KVM_CAP_XCRS 56
+#define KVM_CAP_PPC_GET_PVINFO 57
+#define KVM_CAP_PPC_IRQ_LEVEL 58
+#define KVM_CAP_ASYNC_PF 59
+#define KVM_CAP_TSC_CONTROL 60
+#define KVM_CAP_GET_TSC_KHZ 61
+#define KVM_CAP_PPC_BOOKE_SREGS 62
+#define KVM_CAP_SPAPR_TCE 63
+#define KVM_CAP_PPC_SMT 64
+#define KVM_CAP_PPC_RMA65
+#define KVM_CAP_MAX_VCPUS 66
+#define KVM_CAP_PPC_HIOR 67
+#define KVM_CAP_PPC_PAPR 68
+#define KVM_CAP_SW_TLB 69
+#define KVM_CAP_ONE_REG 70
+#define KVM_CAP_S390_GMAP 71
+#define KVM_CAP_TSC_DEADLINE_TIMER 72
+#define KVM_CAP_S390_UCONTROL 73
+#define KVM_CAP_SYNC_REGS 74
+#define KVM_CAP_PCI_2_3 75
+#define KVM_CAP_KVMCLOCK_CTRL 76
+#define KVM_CAP_SIGNAL_MSI 77
+#define KVM_CAP_PPC_GET_SMMU_INFO 78
+#define KVM_CAP_S390_COW 79
+#define KVM_CAP_PPC_ALLOC_HTAB 80
+#define KVM_CAP_READONLY_MEM 81
+#define KVM_CAP_IRQFD_RESAMPLE 82
+#define KVM_CAP_PPC_BOOKE_WATCHDOG 83
+#define KVM_CAP_PPC_HTAB_FD 84
+#define KVM_CAP_S390_CSS_SUPPORT 85
+#define KVM_CAP_PPC_EPR 86
+#define KVM_CAP_ARM_PSCI 87
+#define KVM_CAP_ARM_SET_DEVICE_ADDR 88
+#define KVM_CAP_DEVICE_CTRL 89
+#define KVM_CAP_IRQ_MPIC 90
+#define KVM_CAP_PPC_RTAS 91
+#define KVM_CAP_IRQ_XICS 92
+#define KVM_CAP_ARM_EL1_32BIT 93
+#define KVM_CAP_SPAPR_MULTITCE 94
+#define KVM_CAP_EXT_EMUL_CPUID 95
+#define KVM_CAP_HYPERV_TIME 96
+#define KVM_CAP_IOAPIC_POLARITY_IGNORED 97
+#define KVM_CAP_ENABLE_CAP_VM 98
+#define KVM_CAP_S390_IRQCHIP 99
+#define KVM_CAP_IOEVENTFD_NO_LENGTH 100
+#define KVM_CAP_VM_ATTRIBUTES 101
+#define KVM_CAP_ARM_PSCI_0_2 102
+#define KVM_CAP_PPC_FIXUP_HCALL 103
+#define KVM_CAP_PPC_ENABLE_HCALL 104
+#define KVM_CAP_CHECK_EXTENSION_VM 105
+#define KVM_CAP_S390_USER_SIGP 106
+#define KVM_CAP_S390_VECTOR_REGISTERS 107
+#define KVM_CAP_S390_MEM_OP 108
+#define KVM_CAP_S390_USER_STSI 109
+#define KVM_CAP_S390_SKEYS 110
+#define KVM_CAP_MIPS_FPU 111
+#define KVM_CAP_MIPS_MSA 112
+#define KVM_CAP_S390_INJECT_IRQ 113
+#define KVM_CAP_S390_IRQ_STATE 114
+#define KVM_CAP_PPC_HWRNG 115
+#define KVM_CAP_DISABLE_QUIRKS 116
+#define KVM_CAP_X86_SMM 117
+#define KVM_CAP_MULTI_ADDRESS_SPACE 118
+#define KVM_CAP_GUEST_DEBUG_HW_BPS 119
+#define KVM_CAP_GUEST_DEBUG_HW_WPS 120
+#define KVM_CAP_SPLIT_IRQCHIP 121
+#define KVM_CAP_IOEVENTFD_ANY_LENGTH 122
+#define KVM_CAP_HYPERV_SYNIC 123
+#define KVM_CAP_S390_RI 124
+#define KVM_CAP_SPAPR_TCE_64 125
+#define KVM_CAP_ARM_PMU_V3 126
+#define KVM_CAP_VCPU_ATTRIBUTES 127
+#define KVM_CAP_MAX_VCPU_ID 128
+#define KVM_CAP_X2APIC_API 129
+#define KVM_CAP_S390_USER_INSTR0 130
+#define KVM_CAP_MSI_DEVID 131
+#define KVM_CAP_PPC_HTM 132
+
+#define KVM_EXIT_UNKNOWN 0
+#define KVM_EXIT_EXCEPTION 1
+#define KVM_EXIT_IO 2
+#define KVM_EXIT_HYPERCALL 3
+#define KVM_EXIT_DEBUG 4
+#define KVM_EXIT_HLT 5
+#define KVM_EXIT_MMIO 6
+#define KVM_EXIT_IRQ_WINDOW_OPEN 7
+#define KVM_EXIT_SHUTDOWN 8
+#define KVM_EXIT_FAIL_ENTRY 9
+#define KVM_EXIT_INTR 10
+#define KVM_EXIT_SET_TPR 11
+#define KVM_EXIT_TPR_ACCESS 12
+#define KVM_EXIT_S390_SIEIC 13
+#define KVM_EXIT_S390_RESET 14
+#define KVM_EXIT_DCR 15
+#define KVM_EXIT_NMI 16
+#define KVM_EXIT_INTERNAL_ERROR 17
+#define KVM_EXIT_OSI 18
+#define KVM_EXIT_PAPR_HCALL 19
+#define KVM_EXIT_S390_UCONTROL 20
+#define KVM_EXIT_WATCHDOG 21
+#define KVM_EXIT_S390_TSCH 22
+#define KVM_EXIT_EPR 23
+#define KVM_EXIT_SYSTEM_EVENT 24
+#define KVM_EXIT_S390_STSI 25
+#define KVM_EXIT_IOAPIC_EOI 26
+#define KVM_EXIT_HYPERV 27
+
+#define KVM_EXIT_IO_IN 0
+#define KVM_EXIT_IO_OUT 1
+
+#define KVM_EXIT_HYPERV_SYNIC 1
+#define KVM_EXIT_HYPERV_HCALL 2
+
+#define KVM_INTERNAL_ERROR_EMULATION 1
+#define KVM_INTERNAL_ERROR_SIMUL_EX 2
+#define KVM_INTERNAL_ERROR_DELIVERY_EV 3
+
+#define KVM_SYSTEM_EVENT_SHUTDOWN 1
+#define KVM_SYSTEM_EVENT_RESET 2
+#define KVM_SYSTEM_EVENT_CRASH 3
+
+#define KVM_CPUID_SIGNATURE 0x40000000
+#define KVM_CPUID_FEATURES 0x40000001
+#define KVM_FEATURE_CLOCKSOURCE 0
+#define KVM_FEATURE_NOP_IO_DELAY 1
+#define KVM_FEATURE_MMU_OP 2
+#define KVM_FEATURE_CLOCKSOURCE2 3
+#define KVM_FEATURE_ASYNC_PF 4
+#define KVM_FEATURE_STEAL_TIME 5
+#define KVM_FEATURE_PV_EOI 6
+#define KVM_FEATURE_PV_UNHALT 7
+#define KVM_CPUID_FLAG_SIGNIFCANT_INDEX (1)
+#define KVM_CPUID_FLAG_STATEFUL_FUNC (1 << 1)
+#define KVM_CPUID_FLAG_STATE_READ_NEXT (1 << 2)
+
+#define MSR_KVM_WALL_CLOCK_NEW 0x4b564d00
+#define MSR_KVM_SYSTEM_TIME_NEW 0x4b564d01
+#define MSR_KVM_ASYNC_PF_EN 0x4b564d02
+#define MSR_KVM_STEAL_TIME 0x4b564d03
+#define MSR_KVM_PV_EOI_EN 0x4b564d04
+
+#define KVM_DEV_ASSIGN_ENABLE_IOMMU (1)
+#define KVM_DEV_ASSIGN_PCI_2_3 (1 << 1)
+#define KVM_DEV_ASSIGN_MASK_INTX (1 << 2)
+
+#define KVM_MAX_MSIX_PER_DEV 256
+
+#define KVM_RUN_X86_SMM 1
+
+struct kvm_coalesced_mmio
+{
+  u_int64_t phys_addr;
+  u_int32_t len, pad;
+  u_int8_t data[8];
+};
+
+struct kvm_coalesced_mmio_ring
+{
+  u_int32_t first, last;
+  struct kvm_coalesced_mmio coalesced_mmio[0];
+};
+
+#define KVM_COALESCED_MMIO_MAX				  \
+  ((PAGE_SIZE - sizeof(struct kvm_coalesced_mmio_ring)) / \
+   sizeof(struct kvm_coalesced_mmio))
+
+struct kvm_msr_list
+{
+  u_int32_t nmsrs;
+  u_int32_t indices[512];
+};
+
+struct kvm_dirty_log
+{
+  u_int32_t slot;
+  u_int32_t padding1;
+  union
+  {
+    void *dirty_bitmap;
+    u_int64_t padding2;
+  };
+};
+
+struct kvm_regs
+{
+  u_int64_t rax, rbx, rcx, rdx, rsi, rdi, rsp, rbp;
+  u_int64_t r8, r9, r10, r11, r12, r13, r14, r15;
+  u_int64_t rip, rflags;
+};
+
+struct kvm_segment
+{
+  u_int64_t base;
+  u_int32_t limit;
+  u_int16_t selector;
+  u_int8_t type, present, dpl, db, s, l ,g, avl, unusable, padding;
+};
+
+struct kvm_dtable
+{
+  u_int64_t base;
+  u_int16_t limit, padding[3];
+};
+
+struct kvm_sregs
+{
+  struct kvm_segment cs, ds, es, fs, gs, ss, tr, ldt;
+  struct kvm_dtable gdt, idt;
+  u_int64_t cr0, cr2, cr3, cr4, cr8, efer, apic_base;
+  u_int64_t interrupt_bitmap[(KVM_NR_INTERRUPTS + 63) / 64];
+};
+
+struct kvm_translation
+{
+  u_int64_t linear_address, physical_address;
+  u_int8_t valid, writeable, usermode, pad[5];
+};
+
+struct kvm_interrupt
+{
+  u_int32_t irq;
+};
+
+struct kvm_msr_entry
+{
+  u_int32_t index;
+  u_int32_t reserved;
+  u_int64_t data;
+};
+
+struct kvm_msrs
+{
+  u_int32_t nmsrs, padding;
+  struct kvm_msr_entry entries[255];
+};
+
+struct kvm_cpuid_entry
+{
+  u_int32_t function, eax, ebx, ecx, edx, padding;
+};
+
+struct kvm_cpuid
+{
+  u_int32_t nent, padding;
+  struct kvm_cpuid_entry entries[128];
+};
+
+struct kvm_signal_mask
+{
+  u_int32_t len;
+  u_int8_t sigset[0];
+};
+
+struct kvm_fpu
+{
+  u_int8_t fpr[8][16];
+  u_int16_t fcw, fsw;
+  u_int8_t ftwx, pad1;
+  u_int16_t last_opcode;
+  u_int64_t last_ip, last_dp;
+  u_int8_t xmm[16][16];
+  u_int32_t mxcsr, pad2;
+};
+
+struct kvm_userspace_memory_region
+{
+  u_int32_t slot, flags;
+  u_int64_t guest_phys_addr, memory_size, userspace_addr;
+};
+
+struct kvm_cpuid_entry2
+{
+  u_int32_t function, index, flags, eax, ebx, ecx, edx, padding[3];
+} __attribute__((packed));
+struct kvm_cpuid2
+{
+  u_int32_t nent, padding;
+  struct kvm_cpuid_entry2 entries[32];
+} __attribute__((packed));
+
+struct kvm_mp_state
+{
+  u_int32_t mp_state;
+};
+
+#define KVM_GET_API_VERSION _IO('s', 1)
+#define KVM_CREATE_VM _IO('s', 2)
+#define KVM_GET_MSR_INDEX_LIST _IOWR('s', 3, struct kvm_msr_list)
+#define KVM_CHECK_EXTENSION _IO('s', 4)
+#define KVM_GET_VCPU_MMAP_SIZE _IO('s', 5)
+#define KVM_GET_SUPPORTED_CPUID _IOWR('s', 6, struct kvm_cpuid2)
+#define KVM_CREATE_VCPU _IO('v', 1)
+#define KVM_GET_DIRTY_LOG _IOW('v', 2, struct kvm_dirty_log)
+#define KVM_SET_USER_MEMORY_REGION _IOW('v', 3,				\
+					struct kvm_userspace_memory_region)
+#define KVM_SET_TSS_ADDR _IO('v', 4)
+#define KVM_SET_IDENTITY_MAP_ADDR _IO('v', 5)
+#define KVM_RUN _IO('c', 1)
+#define KVM_GET_REGS _IOR('c', 2, struct kvm_regs)
+#define KVM_SET_REGS _IOW('c', 3, struct kvm_regs)
+#define KVM_GET_SREGS _IOR('c', 4, struct kvm_sregs)
+#define KVM_SET_SREGS _IOW('c', 5, struct kvm_sregs)
+#define KVM_TRANSLATE _IOWR('c', 6, struct kvm_translation)
+#define KVM_INTERRUPT _IOW('c', 7, struct kvm_interrupt)
+#define KVM_GET_MSRS _IOWR('c', 8, struct kvm_msrs)
+#define KVM_SET_MSRS _IOW('c', 9, struct kvm_msrs)
+#define KVM_SET_CPUID _IOW('c', 10, struct kvm_cpuid)
+#define KVM_SET_SIGNAL_MASK _IOW('c', 11, struct kvm_signal_mask)
+#define KVM_GET_FPU _IOR('c', 12, struct kvm_fpu)
+#define KVM_SET_FPU _IOW('c', 13, struct kvm_fpu)
+#define KVM_GET_MP_STATE _IOR('c', 14, struct kvm_mp_state)
+#define KVM_SET_MP_STATE _IOW('c', 15, struct kvm_mp_state)
+#define KVM_SET_CPUID2 _IOW('c', 16, struct kvm_cpuid2)
+#define KVM_NMI _IO('c', 17)
+  
+struct kvm_debug_exit_arch
+{
+  u_int32_t exception;
+  u_int32_t pad;
+  u_int64_t pc;
+  u_int64_t dr6;
+  u_int64_t dr7;
+};
+
+struct kvm_hyperv_exit
+{
+  u_int32_t type;
+  union
+  {
+    struct
+    {
+      u_int32_t msr;
+      u_int64_t control;
+      u_int64_t evt_page;
+      u_int64_t msg_page;
+    } synic;
+    struct
+    {
+      u_int64_t input;
+      u_int64_t result;
+      u_int64_t params[2];
+    } hcall;
+  } u;
+};
+
+struct kvm_sync_regs
+{
+};
+
+struct kvm_run
+{
+  u_int8_t request_interrupt_window;
+  u_int8_t padding1[7];
+  u_int32_t exit_reason;
+  u_int8_t ready_for_interrupt_injection;
+  u_int8_t if_flag;
+  u_int16_t flags;
+  u_int64_t cr8;
+  u_int64_t apic_base;
+  union
+  {
+    struct
+    {
+      u_int64_t hardware_exit_reason;
+    } hw;
+    struct
+    {
+      u_int64_t hardware_entry_failure_reason;
+    } fail_entry;
+    struct
+    {
+      u_int32_t exception, error_code;
+    } ex;
+    struct
+    {
+      u_int8_t direction, size;
+      u_int16_t port;
+      u_int32_t count;
+      u_int64_t data_offset;
+    } io;
+    struct
+    {
+      struct kvm_debug_exit_arch arch;
+    } debug;
+    struct
+    {
+      u_int64_t phys_addr;
+      u_int8_t data[8];
+      u_int32_t len;
+      u_int8_t is_write;
+    } mmio;
+    struct
+    {
+      u_int64_t nr;
+      u_int64_t args[6];
+      u_int64_t ret;
+      u_int32_t longmode;
+      u_int32_t pad;
+    } hypercall;
+    struct
+    {
+      u_int64_t rip;
+      u_int32_t is_write;
+      u_int32_t pad;
+    } tpr_access;
+    struct
+    {
+      u_int8_t icptcode;
+      u_int16_t ipa;
+      u_int32_t ipb;
+    } s390_sieic;
+    struct
+    {
+      u_int64_t trans_exc_code;
+      u_int32_t pgm_code;
+    } s390_ucontrol;
+    struct
+    {
+      u_int32_t dcrn;
+      u_int32_t data;
+      u_int8_t  is_write;
+    } dcr;
+    struct
+    {
+      u_int32_t suberror;
+      u_int32_t ndata;
+      u_int64_t data[16];
+    } internal;
+    struct
+    {
+      u_int64_t gprs[32];
+    } osi;
+    struct
+    {
+      u_int64_t nr;
+      u_int64_t ret;
+      u_int64_t args[9];
+    } papr_hcall;
+    struct
+    {
+      u_int16_t subchannel_id;
+      u_int16_t subchannel_nr;
+      u_int32_t io_int_parm;
+      u_int32_t io_int_word;
+      u_int32_t ipb;
+      u_int8_t dequeued;
+    } s390_tsch;
+    struct
+    {
+      u_int32_t epr;
+    } epr;
+    struct
+    {
+      u_int32_t type;
+      u_int64_t flags;
+    } system_event;
+    struct
+    {
+      u_int64_t addr;
+      u_int8_t ar;
+      u_int8_t reserved;
+      u_int8_t fc;
+      u_int8_t sel1;
+      u_int16_t sel2;
+    } s390_stsi;
+    struct
+    {
+      u_int8_t vector;
+    } eoi;
+    struct kvm_hyperv_exit hyperv;
+    char padding[256];
+  };
+  u_int64_t kvm_valid_regs;
+  u_int64_t kvm_dirty_regs;
+  union
+  {
+    struct kvm_sync_regs regs;
+    char padding[2048];
+  } s;
+};
+
+struct kvm_vcpu
+{
+  struct kvm_cpuid2 cpuid;
+  struct kvm_run *kr;
+  paddr_t lapic;
+  struct v_io io;
+  struct v_mmio mmio;
+  u_int64_t insn_len;
+  void *hw_data;
+};
+
+struct kvm_guest_mem
+{
+  int used;
+  paddr_t gpaddr;
+  vaddr_t vaddr;
+  size_t size;
+  u_int64_t *dirty;
+  size_t dirty_size;
+};
+
+struct v_kvm_vpage
+{
+  vaddr_t vp;
+  paddr_t pp;
+  size_t sz;
+  SLIST_ENTRY(v_kvm_vpage) next;
+};
+
+struct kvm_p
+{
+  struct kvm_vcpu *vcpu[16];
+  struct kvm_guest_mem mem[32];
+  vaddr_t vpml4;
+  paddr_t ppml4;
+  SLIST_HEAD(v_kvm_head, v_kvm_vpage) vp;
+  int created;
+  int id;
+  struct mutex vcpu_mtx;
+  u_int16_t vcpu_count;
+  int shutdown;
+  paddr_t identity_base;
+  paddr_t tss_base;
+};
+
+//
+// not implemented
+//
+
+#define KVM_ENABLE_CAP 0
+struct kvm_enable_cap
+{
+  u_int32_t cap, flags;
+  u_int64_t args[4];
+  u_int8_t pad[64];
+};
+#define KVM_GET_ONE_REG 0
+#define KVM_SET_ONE_REG 0
+struct kvm_one_reg
+{
+  u_int64_t id;
+  u_int64_t addr;
+};
+#define KVM_CREATE_DEVICE 0
+#define KVM_CREATE_DEVICE_TEST 1
+struct kvm_create_device
+{
+  u_int32_t type, fd, flags;
+};
+#define KVM_SET_DEVICE_ATTR 0
+#define KVM_GET_DEVICE_ATTR 0
+#define KVM_HAS_DEVICE_ATTR 0
+struct kvm_device_attr
+{
+  u_int32_t flags, group;
+  u_int64_t attr, addr;
+};
+#define KVM_IRQ_LINE 0
+#define KVM_IRQ_LINE_STATUS 0
+struct kvm_irq_level
+{
+  union
+  {
+    u_int32_t irq;
+    int32_t status;
+  };
+  u_int32_t level;
+};
+#define KVM_CREATE_IRQCHIP 0
+#define KVM_MEM_LOG_DIRTY_PAGES (1UL << 0)
+#define KVM_MEM_READONLY (1UL << 1)
+#define KVM_REGISTER_COALESCED_MMIO 0
+#define KVM_UNREGISTER_COALESCED_MMIO 0
+struct kvm_coalesced_mmio_zone
+{
+  u_int64_t addr;
+  u_int32_t size, pad;
+};
+#define KVM_IOEVENTFD 0
+#define KVM_IOEVENTFD_FLAG_DATAMATCH (1)
+#define KVM_IOEVENTFD_FLAG_PIO (1 << 1)
+#define KVM_IOEVENTFD_FLAG_DEASSIGN (1 << 2)
+#define KVM_IOEVENTFD_FLAG_VIRTIO_CCW_NOTIFY (1 << 3)
+struct kvm_ioeventfd
+{
+  u_int64_t datamatch, addr;
+  u_int32_t len;
+  int32_t fd;
+  u_int32_t flags;
+  u_int8_t pad[36];
+};
+#define KVM_SET_GSI_ROUTING 0
+#define KVM_IRQ_ROUTING_IRQCHIP 1
+#define KVM_IRQ_ROUTING_MSI 2
+#define KVM_IRQ_ROUTING_S390_ADAPTER 3
+#define KVM_IRQ_ROUTING_HV_SINT 4
+struct kvm_irq_routing_irqchip
+{
+  u_int32_t irqchip, pin;
+};
+struct kvm_irq_routing_msi
+{
+  u_int32_t address_lo, address_hi, data;
+  union
+  {
+    u_int32_t pad, devid;
+  };
+};
+struct kvm_irq_routing_s390_adapter
+{
+  u_int64_t ind_addr, summary_addr, ind_offset;
+  u_int32_t summary_offset, adapter_id;
+};
+struct kvm_irq_routing_hv_sint
+{
+  u_int32_t vcpu, sint;
+};
+struct kvm_irq_routing_entry
+{
+  u_int32_t gsi, type, flags, pad;
+  union
+  {
+    struct kvm_irq_routing_irqchip irqchip;
+    struct kvm_irq_routing_msi msi;
+    struct kvm_irq_routing_s390_adapter adapter;
+    struct kvm_irq_routing_hv_sint hv_sint;
+    u_int32_t pad[8];
+  } u;
+};
+struct kvm_irq_routing
+{
+  u_int32_t nr, flags;
+  struct kvm_irq_routing_entry entries[0];
+};
+#define KVM_SIGNAL_MSI 0
+struct kvm_msi
+{
+  u_int32_t address_lo, address_hi, data, flags, devid;
+  u_int8_t pad[12];
+};
+#define KVM_IRQFD 0
+#define KVM_IRQFD_FLAG_DEASSIGN (1)
+#define KVM_IRQFD_FLAG_RESAMPLE (1 << 1)
+struct kvm_irqfd
+{
+  u_int32_t fd, gsi, flags, resamplefd;
+  u_int8_t pad[16];
+};
+#define KVM_SET_GUEST_DEBUG 0
+#define KVM_GUESTDBG_ENABLE 1
+#define KVM_GUESTDBG_SINGLESTEP 2
+#define KVM_GUESTDBG_USE_SW_BP 0x10000
+#define KVM_GUESTDBG_USE_HW_BP 0x20000
+#define KVM_GUESTDBG_INJECT_DB 0x40000
+#define KVM_GUESTDBG_INJECT_BP 0x80000
+struct kvm_guest_debug_arch
+{
+  u_int64_t debugreg[8];
+};
+struct kvm_guest_debug
+{
+  u_int32_t control, pad;
+  struct kvm_guest_debug_arch arch;
+};
+#define KVM_GET_CLOCK 0
+#define KVM_SET_CLOCK 0
+struct kvm_clock_data
+{
+  u_int64_t clock;
+  u_int32_t flags, pad[9];
+};
+#define KVM_KVMCLOCK_CTRL 0
+#define KVM_TPR_ACCESS_REPORTING 0
+struct kvm_tpr_access_ctl
+{
+  u_int32_t enabled, flags, reserved[8];
+};
+#define KVM_SET_VAPIC_ADDR 0
+struct kvm_vapic_addr
+{
+  u_int64_t vapic_addr;
+};
+#define KVM_GET_LAPIC 0
+#define KVM_SET_LAPIC 0
+struct kvm_lapic_state
+{
+  char regs[0x400];
+};
+#define KVM_GET_IRQCHIP 0
+#define KVM_SET_IRQCHIP 0
+#define KVM_IRQCHIP_PIC_MASTER 0
+#define KVM_IRQCHIP_PIC_SLAVE 1
+#define KVM_IRQCHIP_IOAPIC 2
+#define KVM_NR_IRQCHIPS 3
+struct kvm_pic_state
+{
+  u_int8_t last_irr, irr, imr, isr, priority_add, irq_base, read_reg_select;
+  u_int8_t poll, special_mask, init_state, auto_eoi, rotate_on_auto_eoi;
+  u_int8_t special_fully_nested_mode, init4, elcr, elcr_mask;
+};
+struct kvm_ioapic_state
+{
+  u_int64_t base_address;
+  u_int32_t ioregsel, id, irr, pad;
+  union
+  {
+    u_int64_t bits;
+    struct
+    {
+      u_int8_t vector, delivery_mode:3, dest_mode:1, delivery_status:1;
+      u_int8_t polarity:1, remote_irr:1, trig_mode:1, mask:1, reserve:7;
+      u_int8_t reserved[4], dest_id;
+    };
+  } redirtbl[24];
+};
+struct kvm_irqchip
+{
+  u_int32_t chip_id, pad;
+  union
+  {
+    char dummy[512];
+    struct kvm_pic_state pic;
+    struct kvm_ioapic_state ioapic;
+  } chip;
+};
+#define KVM_GET_PIT 0
+#define KVM_SET_PIT 0
+#define KVM_GET_PIT2 0
+#define KVM_SET_PIT2 0
+#define KVM_PIT_FLAGS_HPET_LEGACY 1
+struct kvm_pit_channel_state
+{
+  u_int32_t count;
+  u_int16_t latched_count;
+  u_int8_t count_latched, status_latched, status, read_state, write_state;
+  u_int8_t write_latch, rw_mode, mode, bcd, gate;
+  int64_t count_load_time;
+};
+struct kvm_pit_state2
+{
+  struct kvm_pit_channel_state channels[3];
+  u_int32_t flags, reserved[9];
+};
+struct kvm_pit_state
+{
+  struct kvm_pit_channel_state channels[3];
+};
+#define KVM_CREATE_PIT 0
+#define KVM_CREATE_PIT2 0
+struct kvm_pit_config
+{
+  u_int32_t flags, pad[15];
+};
+#define KVM_REINJECT_CONTROL 0
+struct kvm_reinject_control
+{
+  u_int8_t pit_reinject, reserved[31];
+};
+#define KVM_GET_VCPU_EVENTS 0
+#define KVM_SET_VCPU_EVENTS 0
+#define KVM_VCPUEVENT_VALID_NMI_PENDING 1
+#define KVM_VCPUEVENT_VALID_SIPI_VECTOR 2
+#define KVM_VCPUEVENT_VALID_SHADOW 4
+#define KVM_VCPUEVENT_VALID_SMM 8
+struct kvm_vcpu_events
+{
+  struct
+  {
+    u_int8_t injected, nr, has_error_code, pad;
+    u_int32_t error_code;
+  } exception;
+  struct
+  {
+    u_int8_t injected, nr, soft, shadow;
+  } interrupt;
+  struct
+  {
+    u_int8_t injected, pending, masked, pad;
+  } nmi;
+  u_int32_t sipi_vector;
+  u_int32_t flags;
+  struct
+  {
+    u_int8_t smm;
+    u_int8_t pending;
+    u_int8_t smm_inside_nmi;
+    u_int8_t latched_init;
+  } smi;
+};
+#define KVM_GET_DEBUGREGS 0
+#define KVM_SET_DEBUGREGS 0
+struct kvm_debugregs
+{
+  u_int64_t db[4], dr6, dr7, flags, reserved[9];
+};
+#define KVM_SMI 0
+#define KVM_MP_STATE_RUNNABLE 0
+#define KVM_MP_STATE_UNINITIALIZED 1
+#define KVM_MP_STATE_INIT_RECEIVED 2
+#define KVM_MP_STATE_HALTED 3
+#define KVM_MP_STATE_SIPI_RECEIVED 4
+#define KVM_MP_STATE_STOPPED 5
+#define KVM_MP_STATE_CHECK_STOP 6
+#define KVM_MP_STATE_OPERATING 7
+#define KVM_MP_STATE_LOAD 8
+#define KVM_ASSIGN_PCI_DEVICE 0
+#define KVM_DEASSIGN_PCI_DEVICE 0
+struct kvm_assigned_pci_dev
+{
+  u_int32_t assigned_dev_id, busnr, devfn, flags, segnr;
+  union
+  {
+    u_int32_t reserved[11];
+  };
+};
+#define KVM_ASSIGN_IRQ 0
+#define KVM_DEASSIGN_IRQ 0
+#define KVM_ASSIGN_DEV_IRQ 0
+#define KVM_DEASSIGN_DEV_IRQ 0
+#define KVM_DEV_IRQ_HOST_INTX (1)
+#define KVM_DEV_IRQ_HOST_MSI (1 << 1)
+#define KVM_DEV_IRQ_HOST_MSIX (1 << 2)
+#define KVM_DEV_IRQ_GUEST_INTX (1 << 8)
+#define KVM_DEV_IRQ_GUEST_MSI (1 << 9)
+#define KVM_DEV_IRQ_GUEST_MSIX (1 << 10)
+struct kvm_assigned_irq {
+  u_int32_t assigned_dev_id, host_irq, guest_irq, flags;
+  union
+  {
+    u_int32_t reserved[12];
+  };
+};
+#define KVM_ASSIGN_SET_INTX_MASK 0
+#define KVM_ASSIGN_SET_MSIX_NR 0
+struct kvm_assigned_msix_nr
+{
+  u_int32_t assigned_dev_id;
+  u_int16_t entry_nr, padding;
+};
+#define KVM_ASSIGN_SET_MSIX_ENTRY 0
+struct kvm_assigned_msix_entry
+{
+  u_int32_t assigned_dev_id, gsi;
+  u_int16_t entry, padding[3];
+};
+#define KVM_SET_TSC_KHZ 0
+#define KVM_GET_TSC_KHZ 0
+#define KVM_GET_NR_MMU_PAGES 0
+#define KVM_SET_NR_MMU_PAGES 0
+#define KVM_GET_XCRS 0
+#define KVM_SET_XCRS 0
+#define KVM_MAX_XCRS 16
+struct kvm_xcr
+{
+  u_int32_t xcr, reserved;
+  u_int64_t value;
+};
+struct kvm_xcrs
+{
+  u_int32_t nr_xcrs;
+  u_int32_t flags;
+  struct kvm_xcr xcrs[KVM_MAX_XCRS];
+  u_int64_t padding[16];
+};
+#define KVM_GET_XSAVE 0
+#define KVM_SET_XSAVE 0
+struct kvm_xsave
+{
+  u_int32_t region[1024];
+};
+#define KVM_X86_GET_MCE_CAP_SUPPORTED 0
+#define KVM_X86_SET_MCE 0
+struct kvm_x86_mce
+{
+  u_int64_t status, addr, misc, mcg_status;
+  u_int8_t bank, pad1[7];
+  u_int64_t pad2[3];
+};
+#define KVM_X86_SETUP_MCE 0
+
+struct kvm_hw_ptr
+{
+  void* (*init)(struct proc *);
+  void (*free)(void *);
+  int (*get_prot)(void);
+  int (*get_regs)(void *, struct kvm_regs *);
+  int (*set_regs)(void *, struct kvm_regs *);
+  int (*get_fpu)(void *, struct kvm_fpu *);
+  int (*set_fpu)(void *, struct kvm_fpu *);
+  int (*get_sregs)(void *, struct kvm_sregs *, struct kvm_p *);
+  int (*set_sregs)(void *, struct kvm_sregs *, struct kvm_p *);
+  void* (*get_reg_ptr)(void *, int);
+  int (*get_msrs)(void *, struct kvm_msrs *);
+  int (*set_msrs)(void *, struct kvm_msrs *);
+  int (*set_intr)(void *, u_int32_t);
+  int (*run)(void *, struct kvm_run *, struct kvm_p *, struct proc *);
+};
+
+#endif /* !_MACHINE_KVM_H_ */
diff --git a/sys/arch/amd64/include/svm.h b/sys/arch/amd64/include/svm.h
new file mode 100644
index 0000000..d5dc00c
--- /dev/null
+++ b/sys/arch/amd64/include/svm.h
@@ -0,0 +1,261 @@
+#ifndef _MACHINE_SVM_H_
+#define _MACHINE_SVM_H_
+
+#define SVM_HSAVE_PA 0xc0010117
+
+#define SVM_CR 0xc0010114
+#define SVM_CR_SVMDIS ((u_int64_t)1 << 4)
+
+#define SVM_CPUID 0x8000000a
+#define SVM_CPUID_NPT 1
+
+#define SVM_EFER_SVME ((u_int64_t)1 << 12)
+#define SVM_SVMEN 1
+
+struct svm_regs {
+  u_int16_t sel;
+  u_int16_t attr;
+  u_int32_t lim;
+  u_int64_t base;
+} __attribute__ ((packed));
+
+#define SVM_EXIT_CR_START        0x000
+#define SVM_EXIT_CR_END          0x01f
+#define SVM_EXIT_DR_START        0x020
+#define SVM_EXIT_DR_END          0x03f
+#define SVM_EXIT_EXC_START       0x040
+#define SVM_EXIT_EXC_END         0x05f
+#define SVM_EXIT_INTR            0x060
+#define SVM_EXIT_NMI             0x061
+#define SVM_EXIT_SMI             0x062
+#define SVM_EXIT_INIT            0x063
+#define SVM_EXIT_VINTR           0x064
+#define SVM_EXIT_CR0_SEL_WR      0x065
+#define SVM_EXIT_IDTR_READ       0x066
+#define SVM_EXIT_GDTR_READ       0x067
+#define SVM_EXIT_LDTR_READ       0x068
+#define SVM_EXIT_TR_READ         0x069
+#define SVM_EXIT_IDTR_WRITE      0x06a
+#define SVM_EXIT_GDTR_WRITE      0x06b
+#define SVM_EXIT_LDTR_WRITE      0x06c
+#define SVM_EXIT_TR_WRITE        0x06d
+#define SVM_EXIT_RDTSC           0x06e
+#define SVM_EXIT_RDPMC           0x06f
+#define SVM_EXIT_PUSHF           0x070
+#define SVM_EXIT_POPF            0x071
+#define SVM_EXIT_CPUID           0x072
+#define SVM_EXIT_RSM             0x073
+#define SVM_EXIT_IRET            0x074
+#define SVM_EXIT_SWINT           0x075
+#define SVM_EXIT_INVD            0x076
+#define SVM_EXIT_PAUSE           0x077
+#define SVM_EXIT_HLT             0x078
+#define SVM_EXIT_INVLPG          0x079
+#define SVM_EXIT_INVLPGA         0x07a
+#define SVM_EXIT_IOIO            0x07b
+#define SVM_EXIT_MSR             0x07c
+#define SVM_EXIT_TASK_SWITCH     0x07d
+#define SVM_EXIT_FERR_FREEZE     0x07e
+#define SVM_EXIT_SHUTDOWN        0x07f
+#define SVM_EXIT_VMRUN           0x080
+#define SVM_EXIT_VMMCALL         0x081
+#define SVM_EXIT_VMLOAD          0x082
+#define SVM_EXIT_VMSAVE          0x083
+#define SVM_EXIT_STGI            0x084
+#define SVM_EXIT_CLGI            0x085
+#define SVM_EXIT_SKINIT          0x086
+#define SVM_EXIT_RDTSCP          0x087
+#define SVM_EXIT_ICEBP           0x088
+#define SVM_EXIT_WBINVD          0x089
+#define SVM_EXIT_MONITOR         0x08a
+#define SVM_EXIT_MWAIT           0x08b
+#define SVM_EXIT_MWAIT_COND      0x08c
+#define SVM_EXIT_XSETBV          0x08d
+#define SVM_EXIT_NPF             0x400
+#define SVM_EXIT_AVIC_INC_IPI    0x401
+#define SVM_EXIT_AVIC_NOACCEL    0x402
+#define SVM_EXIT_INVALID         -1
+
+struct svm_vmcb {
+  u_int16_t int_cr_read;         // 0x000
+  u_int16_t int_cr_write;        // 0x002
+  u_int16_t int_dr_read;         // 0x004
+  u_int16_t int_dr_write;        // 0x006
+  u_int32_t int_exc;             // 0x008
+  u_int64_t intercept;           // 0x00c
+  u_int64_t mbz00[5];            // 0x014
+  u_int16_t pause_filter_thresh; // 0x03c
+  u_int16_t pause_filter_count;  // 0x03e
+  u_int64_t iopm_base_pa;        // 0x040
+  u_int64_t msrpm_base_pa;       // 0x048
+  int64_t   tsc_offset;          // 0x050
+  u_int32_t asid;                // 0x058
+  u_int8_t  tlb_ctl;             // 0x05c
+  u_int32_t mbz01 : 24;          // 0x05d
+  u_int8_t  v_tpr;               // 0x060
+  u_int8_t  v_irq : 1;           // 0x061
+  u_int8_t  mbz02 : 7;           // 0x061
+  u_int8_t  v_intr_prio : 4;     // 0x062
+  u_int8_t  v_ign_tpr : 1;       // 0x062
+  u_int8_t  mbz03 : 3;           // 0x062
+  u_int8_t  v_intr_masking : 1;  // 0x063
+  u_int8_t  mbz04 : 6;           // 0x063
+  u_int8_t  avic_en : 1;         // 0x063
+  u_int8_t  v_intr_vec;          // 0x064
+  u_int64_t mbz05 : 24;          // 0x065
+  u_int8_t  intr_shadow : 1;     // 0x068
+  u_int64_t mbz06 : 63;          // 0x068
+  int64_t   exit_code;           // 0x070
+  u_int64_t exit_info1;          // 0x078
+  u_int64_t exit_info2;          // 0x080
+  u_int64_t exit_int_info;       // 0x088
+  u_int8_t  np_enable : 1;       // 0x090
+  u_int64_t mbz07 : 63;          // 0x090
+  u_int64_t apic_bar : 52;       // 0x098
+  u_int16_t mbz08 : 12;          // 0x098
+  u_int64_t mbz09;               // 0x0a0
+  u_int64_t evt_inj;             // 0x0a8
+  u_int64_t n_cr3;               // 0x0b0
+  u_int8_t  lbr_virt_en : 1;     // 0x0b8
+  u_int64_t mbz10 : 63;          // 0x0b8
+  u_int32_t vmcb_clean;          // 0x0c0
+  u_int32_t mbz11;               // 0x0c4
+  u_int64_t nrip;                // 0x0c8
+  u_int8_t  byte_fetched;        // 0x0d0
+  u_int8_t  insr_bytes[15];      // 0x0d1
+  u_int64_t apic_back_page : 52; // 0x0e0
+  u_int16_t mbz12 : 12;          // 0x0e0
+  u_int64_t mbz13;               // 0x0e8
+  u_int64_t logical_table : 52;  // 0x0f0
+  u_int16_t mbz14 : 12;          // 0x0f0
+  u_int8_t  avic_phys_max_idx;   // 0x0f8
+  u_int8_t  mbz15 : 4;           // 0x0f8
+  u_int64_t physical_table : 40; // 0x0f8
+  u_int16_t mbz16 : 12;          // 0x0f8
+  u_int64_t mbz17[96];           // 0x100
+  struct svm_regs es;            // 0x400
+  struct svm_regs cs;            // 0x410
+  struct svm_regs ss;            // 0x420
+  struct svm_regs ds;            // 0x430
+  struct svm_regs fs;            // 0x440
+  struct svm_regs gs;            // 0x450
+  struct svm_regs gdtr;          // 0x460
+  struct svm_regs ldtr;          // 0x470
+  struct svm_regs idtr;          // 0x480
+  struct svm_regs tr;            // 0x490
+  u_int8_t  mbz18[43];           // 0x4a0
+  u_int8_t  cpl;                 // 0x4cb
+  u_int32_t mbz19;               // 0x4cc
+  u_int64_t efer;                // 0x4d0
+  u_int64_t mbz20[14];           // 0x4d8
+  u_int64_t cr4;                 // 0x548
+  u_int64_t cr3;                 // 0x550
+  u_int64_t cr0;                 // 0x558
+  u_int64_t dr7;                 // 0x560
+  u_int64_t dr6;                 // 0x568
+  u_int64_t rflags;              // 0x570
+  u_int64_t rip;                 // 0x578
+  u_int64_t mbz21[11];           // 0x580
+  u_int64_t rsp;                 // 0x5d8
+  u_int64_t mbz22[3];            // 0x5e0
+  u_int64_t rax;                 // 0x5f8
+  u_int64_t star;                // 0x600
+  u_int64_t lstar;               // 0x608
+  u_int64_t cstar;               // 0x610
+  u_int64_t sfmask;              // 0x618
+  u_int64_t kern_gs_base;        // 0x620
+  u_int64_t sysenter_cs;         // 0x628
+  u_int64_t sysenter_esp;        // 0x630
+  u_int64_t sysenter_eip;        // 0x638
+  u_int64_t cr2;                 // 0x640
+  u_int64_t mbz23[4];            // 0x648
+  u_int64_t g_pat;               // 0x668
+  u_int64_t dbg_ctl;             // 0x670
+  u_int64_t br_from;             // 0x678
+  u_int64_t br_to;               // 0x680
+  u_int64_t last_excp_from;      // 0x688
+  u_int64_t last_excp_to;        // 0x690
+} __attribute__ ((packed));
+
+struct svm_guest {
+  u_int64_t rbx;
+  u_int64_t rcx;
+  u_int64_t rdx;
+  u_int64_t rsi;
+  u_int64_t rdi;
+  u_int64_t r8;
+  u_int64_t r9;
+  u_int64_t r10;
+  u_int64_t r11;
+  u_int64_t r12;
+  u_int64_t r13;
+  u_int64_t r14;
+  u_int64_t r15;
+  u_int64_t rbp;
+  u_int64_t pdpte0;
+  u_int64_t pdpte1;
+  u_int64_t pdpte2;
+  u_int64_t pdpte3;
+  u_int64_t smbase;
+  u_int64_t fsbase;
+  u_int64_t gsbase;
+  struct savefpu fpu __attribute__((aligned(16)));;
+} __attribute__ ((packed));
+
+struct svminfo {
+  u_int32_t flags;
+  u_int32_t nasid;
+  u_int32_t features;
+  u_int32_t revision;
+  paddr_t phost;
+  vaddr_t vhost;
+};
+
+struct v_svm_vpage
+{
+  vaddr_t vp;
+  paddr_t pp;
+  size_t sz;
+  SLIST_ENTRY(v_svm_vpage) next;
+};
+
+#define SVM_VM_FL_STARTED 1
+struct v_svm {
+  SLIST_HEAD(vp_svm_head, v_svm_vpage) vp;
+  u_int32_t flags;
+  struct svm_vmcb *vmcb;
+  paddr_t p_vmcb;
+  struct svm_guest guest;
+  u_int64_t intr[4];
+  u_int32_t nmi;
+  paddr_t p_iobm;
+  vaddr_t v_iobm;
+  paddr_t p_msrbm;
+  vaddr_t v_msrbm;
+  u_int64_t tsc;
+};
+
+struct kvm_hw_ptr *svm_get_hw(void);
+
+#define SVM_MSR_BMAP_AUTH(B, X)						\
+  if (X < 0x2000)							\
+  {									\
+    ((u_int8_t *)B)[(X * 2) / 8] &=					\
+      ~((u_int8_t)1 << (((X * 2) & 0x1fff) % 8));			\
+    ((u_int8_t *)B)[((X * 2) + 1) / 8] &=				\
+      ~((u_int8_t)1 << ((((X * 2) + 1) & 0x1fff) % 8));			\
+  } else if ((X > 0xc0000000) && (X < 0xc0002000))			\
+  {									\
+    ((u_int8_t *)B)[(((X-0xc0000000) * 2) / 8) + 0x800] &=		\
+      ~((u_int8_t)1 << ((((X-0xc0000000) * 2) & 0x1fff) % 8));		\
+    ((u_int8_t *)B)[((((X-0xc0000000) * 2) + 1) / 8) + 0x800] &=	\
+      ~((u_int8_t)1 << (((((X-0xc0000000) * 2) + 1) & 0x1fff) % 8));	\
+  } else if ((X > 0xc0010000) && (X < 0xc0012000))			\
+  {									\
+    ((u_int8_t *)B)[(((X-0xc0010000) * 2) / 8) + 0x1000] &=		\
+      ~((u_int8_t)1 << ((((X-0xc0010000) * 2) & 0x1fff) % 8));		\
+    ((u_int8_t *)B)[((((X-0xc0010000) * 2) + 1) / 8) + 0x1000] &=	\
+      ~((u_int8_t)1 << (((((X-0xc0010000) * 2) + 1) & 0x1fff) % 8));	\
+  }
+
+#endif /* !_MACHINE_SVM_H_ */
diff --git a/sys/arch/amd64/include/vmx.h b/sys/arch/amd64/include/vmx.h
new file mode 100644
index 0000000..c85422f
--- /dev/null
+++ b/sys/arch/amd64/include/vmx.h
@@ -0,0 +1,292 @@
+#ifndef _MACHINE_VMX_H_
+#define _MACHINE_VMX_H_
+
+struct vmx_msr_entry
+{
+  u_int32_t index;
+  u_int32_t mbz;
+  u_int64_t data;
+} __attribute__ ((packed));
+
+struct vmx_guest {
+  u_int64_t rax;
+  u_int64_t rbx;
+  u_int64_t rcx;
+  u_int64_t rdx;
+  u_int64_t rsi;
+  u_int64_t rdi;
+  u_int64_t r8;
+  u_int64_t r9;
+  u_int64_t r10;
+  u_int64_t r11;
+  u_int64_t r12;
+  u_int64_t r13;
+  u_int64_t r14;
+  u_int64_t r15;
+  u_int64_t rbp;
+  u_int64_t cr2;
+  u_int64_t cr8;
+  struct savefpu fpu __attribute__((aligned(16)));;
+} __attribute__ ((packed));
+
+struct vmxinfo {
+  u_int8_t flags;
+  u_int32_t vmcs_rev_id;
+  u_int16_t vmcs_region_size;
+  u_int8_t vmcs_region_type;
+  u_int8_t vmcs_flags;
+  vaddr_t vmxon_vregion;
+  paddr_t vmxon_pregion;
+  paddr_t vmcs_current;
+};
+
+struct v_vmx_vpage
+{
+  vaddr_t vp;
+  paddr_t pp;
+  SLIST_ENTRY(v_vmx_vpage) next;
+};
+
+struct v_vmx
+{
+  SLIST_HEAD(vp_vmx_head, v_vmx_vpage) vp;
+  vaddr_t vmcs_vregion;
+  paddr_t vmcs_pregion;
+  u_int32_t flags;
+  vaddr_t vio_bmap_a;
+  paddr_t pio_bmap_a;
+  vaddr_t vio_bmap_b;
+  paddr_t pio_bmap_b;
+  vaddr_t vgmsr;
+  paddr_t pgmsr;
+  vaddr_t vhmsr;
+  paddr_t phmsr;
+  vaddr_t vmsr_bmap;
+  paddr_t pmsr_bmap;
+  u_int64_t intr[4];
+  u_int32_t nmi;
+  u_int64_t tsc;
+  u_int64_t proc_based;
+  struct vmx_guest guest;
+};
+#define VMX_VM_FL_STARTED 1
+#define VMX_VM_FL_IO 2
+#define VMX_VM_FL_MMIO 4
+
+#define VMX_VMXON 1
+
+#define VMX_VMCS_REG_CR0 0x6800
+#define VMX_VMCS_REG_CR3 0x6802
+#define VMX_VMCS_REG_CR4 0x6804
+#define VMX_VMCS_REG_DR7 0x681a
+#define VMX_VMCS_REG_RSP 0x681c
+#define VMX_VMCS_REG_RIP 0x681e
+#define VMX_VMCS_REG_RFLAGS 0x6820
+#define VMX_VMCS_REG_CS_SEL 0x0802
+#define VMX_VMCS_REG_CS_BASE 0x6808
+#define VMX_VMCS_REG_CS_LIM 0x4802
+#define VMX_VMCS_REG_CS_ACC 0x4816
+#define VMX_VMCS_REG_SS_SEL 0x0804
+#define VMX_VMCS_REG_SS_BASE 0x680a
+#define VMX_VMCS_REG_SS_LIM 0x4804
+#define VMX_VMCS_REG_SS_ACC 0x4818
+#define VMX_VMCS_REG_DS_SEL 0x0806
+#define VMX_VMCS_REG_DS_BASE 0x680c
+#define VMX_VMCS_REG_DS_LIM 0x4806
+#define VMX_VMCS_REG_DS_ACC 0x481a
+#define VMX_VMCS_REG_ES_SEL 0x0800
+#define VMX_VMCS_REG_ES_BASE 0x6806
+#define VMX_VMCS_REG_ES_LIM 0x4800
+#define VMX_VMCS_REG_ES_ACC 0x4814
+#define VMX_VMCS_REG_FS_SEL 0x0808
+#define VMX_VMCS_REG_FS_BASE 0x680e
+#define VMX_VMCS_REG_FS_LIM 0x4808
+#define VMX_VMCS_REG_FS_ACC 0x481c
+#define VMX_VMCS_REG_GS_SEL 0x080a
+#define VMX_VMCS_REG_GS_BASE 0x6810
+#define VMX_VMCS_REG_GS_LIM 0x480a
+#define VMX_VMCS_REG_GS_ACC 0x481e
+#define VMX_VMCS_REG_LDTR_SEL 0x080c
+#define VMX_VMCS_REG_LDTR_BASE 0x6812
+#define VMX_VMCS_REG_LDTR_LIM 0x480c
+#define VMX_VMCS_REG_LDTR_ACC 0x4820
+#define VMX_VMCS_REG_TR_SEL 0x080e
+#define VMX_VMCS_REG_TR_BASE 0x6814
+#define VMX_VMCS_REG_TR_LIM 0x480e
+#define VMX_VMCS_REG_TR_ACC 0x4822
+#define VMX_VMCS_REG_GDTR_BASE 0x6816
+#define VMX_VMCS_REG_GDTR_LIM 0x4810
+#define VMX_VMCS_REG_IDTR_BASE 0x6818
+#define VMX_VMCS_REG_IDTR_LIM 0x4812
+#define VMX_VMCS_REG_IA32_DEBUGCTL 0x2802
+#define VMX_VMCS_REG_IA32_SYSENTER_CS 0x482a
+#define VMX_VMCS_REG_IA32_SYSENTER_ESP 0x6824
+#define VMX_VMCS_REG_IA32_SYSENTER_EIP 0x6826
+#define VMX_VMCS_REG_IA32_PERF_GLOBAL_CTRL 0x2808
+#define VMX_VMCS_REG_IA32_PAT 0x2804
+#define VMX_VMCS_REG_IA32_EFER 0x2806
+#define VMX_VMCS_INT_STATE 0x4824
+#define VMX_VMCS_ACT_STATE 0x4826
+#define VMX_VMCS_SMBASE 0x4828
+#define VMX_VMCS_PREEMPT_TIMER 0x482e
+#define VMX_VMCS_PENDING_DBG_EXC 0x6822
+#define VMX_VMCS_INTR_STATUS 0x0810
+#define VMX_VMCS_LINK_PTR 0x2800
+#define VMX_VMCS_PDPTE0 0x280a
+#define VMX_VMCS_PDPTE1 0x280c
+#define VMX_VMCS_PDPTE2 0x280e
+#define VMX_VMCS_PDPTE3 0x2810
+
+#define VMX_VMCS_HOST_CR0 0x6c00
+#define VMX_VMCS_HOST_CR3 0x6c02
+#define VMX_VMCS_HOST_CR4 0x6c04
+#define VMX_VMCS_HOST_RSP 0x6c14
+#define VMX_VMCS_HOST_RIP 0x6c16
+#define VMX_VMCS_HOST_CS_SEL 0x0c02
+#define VMX_VMCS_HOST_SS_SEL 0x0c04
+#define VMX_VMCS_HOST_DS_SEL 0x0c06
+#define VMX_VMCS_HOST_ES_SEL 0x0c00
+#define VMX_VMCS_HOST_FS_SEL 0x0c08
+#define VMX_VMCS_HOST_GS_SEL 0x0c0a
+#define VMX_VMCS_HOST_TR_SEL 0x0c0c
+#define VMX_VMCS_HOST_FS_BASE 0x6c06
+#define VMX_VMCS_HOST_GS_BASE 0x6c08
+#define VMX_VMCS_HOST_TR_BASE 0x6c0a
+#define VMX_VMCS_HOST_GDTR_BASE 0x6c0c
+#define VMX_VMCS_HOST_IDTR_BASE 0x6c0e
+#define VMX_VMCS_HOST_IA32_SYSENTER_CS 0x4c00
+#define VMX_VMCS_HOST_IA32_SYSENTER_ESP 0x6c10
+#define VMX_VMCS_HOST_IA32_SYSENTER_EIP 0x6c12
+#define VMX_VMCS_HOST_IA32_PERF_GLOBAL_CTRL 0x2c04
+#define VMX_VMCS_HOST_IA32_PAT 0x2c00
+#define VMX_VMCS_HOST_IA32_EFER 0x2c02
+
+#define VMX_VMCS_VPID 0x0000
+#define VMX_VMCS_IO_BMAP_A 0x2000
+#define VMX_VMCS_IO_BMAP_B 0x2002
+#define VMX_VMCS_MSR_BMAP 0x2004
+#define VMX_VMCS_EXIT_MSR_STORE 0x2006
+#define VMX_VMCS_EXIT_MSR_LOAD 0x2008
+#define VMX_VMCS_ENTRY_MSR_LOAD 0x200a
+#define VMX_VMCS_EXECUTIVE 0x200c
+#define VMX_VMCS_TSC_OFFSET 0x2010
+#define VMX_VMCS_VAPIC_ADDR 0x2012
+#define VMX_VMCS_PAPIC_ADDR 0x2014
+#define VMX_VMCS_EPTP 0x201a
+#define VMX_VMCS_PIN_CTRL 0x4000
+#define VMX_VMCS_PROC_CTRL 0x4002
+#define VMX_VMCS_EXC_BMAP 0x4004
+#define VMX_VMCS_PFAULT_ERR_MASK 0x4006
+#define VMX_VMCS_PFAULT_ERR_MATCH 0x4008
+#define VMX_VMCS_CR3_TGT_COUNT 0x400a
+#define VMX_VMCS_EXIT_CTRL 0x400c
+#define VMX_VMCS_EXIT_MSR_STORE_COUNT 0x400e
+#define VMX_VMCS_EXIT_MSR_LOAD_COUNT 0x4010
+#define VMX_VMCS_ENTRY_CTRL 0x4012
+#define VMX_VMCS_ENTRY_MSR_LOAD_COUNT 0x4014
+#define VMX_VMCS_ENTRY_INTR_INF 0x4016
+#define VMX_VMCS_ENTRY_EXC_ERR 0x4018
+#define VMX_VMCS_ENTRY_INSN_LEN 0x401a
+#define VMX_VMCS_TPR_THRESHOLD 0x401c
+#define VMX_VMCS_SEC_PROC_CTRL 0x401e
+#define VMX_VMCS_PLE_GAP 0x4020
+#define VMX_VMCS_PLE_WINDOW 0x4022
+#define VMX_VMCS_INSN_ERR 0x4400
+#define VMX_VMCS_EXIT_REASON 0x4402
+#define VMX_VMCS_EXIT_INTR_INF 0x4404
+#define VMX_VMCS_EXIT_INTR_ERR 0x4406
+#define VMX_VMCS_IDT_INF 0x4408
+#define VMX_VMCS_IDT_ERR 0x440a
+#define VMX_VMCS_EXIT_INSN_LEN 0x440c
+#define VMX_VMCS_EXIT_INSN_INF 0x440e
+#define VMX_VMCS_CR0_MASK 0x6000
+#define VMX_VMCS_CR4_MASK 0x6002
+#define VMX_VMCS_CR0_SHADOW 0x6004
+#define VMX_VMCS_CR4_SHADOW 0x6006
+#define VMX_VMCS_CR3_TGT0 0x6008
+#define VMX_VMCS_CR3_TGT1 0x6008
+#define VMX_VMCS_CR3_TGT2 0x6008
+#define VMX_VMCS_CR3_TGT3 0x6008
+#define VMX_VMCS_EXIT_QUALIF 0x6400
+#define VMX_VMCS_IO_RCX 0x6402
+#define VMX_VMCS_IO_RSI 0x6404
+#define VMX_VMCS_IO_RDI 0x6406
+#define VMX_VMCS_IO_RIP 0x6408
+#define VMX_VMCS_GUEST_LIN 0x640a
+#define VMX_VMCS_GUEST_PHYS 0x2400
+#define VMX_VMCS_GUEST_DEBUG_PEND 0x6826
+
+#define VMX_ACTIVATE_SECONDARY_CONTROL ((u_int64_t)1 << 63)
+#define VMX_ENABLE_EPT ((u_int64_t)1 << 33)
+#define VMX_EPT_2MB ((u_int64_t)1 << 16)
+#define VMX_UNRESTRICTED_GUEST ((u_int64_t)1 << 5)
+#define VMX_DUAL_MONITOR ((u_int64_t)1 << 49)
+#define VMX_ENABLE ((u_int64_t)1 << 2)
+#define VMX_FEATURE_LOCK ((u_int64_t)1)
+
+#define VMX_REASON_EXC_OR_NMI 0
+#define VMX_REASON_EXT_INTR 1
+#define VMX_REASON_TRIPLE_FAULT 2
+#define VMX_REASON_INIT 3
+#define VMX_REASON_SIPI 4
+#define VMX_REASON_IO_SMI 5
+#define VMX_REASON_SMI 6
+#define VMX_REASON_INTR_WIN 7
+#define VMX_REASON_NMI_WIN 8
+#define VMX_REASON_TASK_SWITCH 9
+#define VMX_REASON_CPUID 10
+#define VMX_REASON_GETSEC 11
+#define VMX_REASON_HLT 12
+#define VMX_REASON_INVD 13
+#define VMX_REASON_INVLPG 14
+#define VMX_REASON_RDPMC 15
+#define VMX_REASON_RDTSC 16
+#define VMX_REASON_RSM 17
+#define VMX_REASON_VMCALL 18
+#define VMX_REASON_VMCLEAR 19
+#define VMX_REASON_VMLAUNCH 20
+#define VMX_REASON_VMPTRLD 21
+#define VMX_REASON_VMPTRST 22
+#define VMX_REASON_VMREAD 23
+#define VMX_REASON_VMRESUME 24
+#define VMX_REASON_VMWRITE 25
+#define VMX_REASON_VMXOFF 26
+#define VMX_REASON_VMXON 27
+#define VMX_REASON_CR 28
+#define VMX_REASON_DR 29
+#define VMX_REASON_IO_INSN 30
+#define VMX_REASON_RDMSR 31
+#define VMX_REASON_WRMSR 32
+#define VMX_REASON_INV_GUEST_STATE 33
+#define VMX_REASON_INV_MSR_LOAD 34
+#define VMX_REASON_MWAIT 36
+#define VMX_REASON_MONITOR_TRAP_FL 37
+#define VMX_REASON_MONITOR 39
+#define VMX_REASON_PAUSE 40
+#define VMX_REASON_MACH_CHECK 41
+#define VMX_REASON_TPR_THRESHOLD 43
+#define VMX_REASON_APIC 44
+#define VMX_REASON_DTR 46
+#define VMX_REASON_TR 47
+#define VMX_REASON_EPT_FAULT 48
+#define VMX_REASON_EPT_CONF 49
+#define VMX_REASON_INVEPT 50
+#define VMX_REASON_RDTSCP 51
+#define VMX_REASON_PREEMPT 52
+#define VMX_REASON_INVVPID 53
+#define VMX_REASON_WBINVD 54
+#define VMX_REASON_XSETBV 55
+
+#define VMX_MSR_BMAP_AUTH(B, X)				\
+  *(u_int8_t *)(B +					\
+		((X & 0xffff0000) ? 1024 : 0)	+	\
+		((X & 0x1fff) / 8)) &=			\
+    ~((u_int8_t)1 << ((X & 0x1fff) % 8));		\
+  *(u_int8_t *)(B +					\
+		((X & 0xffff0000) ? 3072 : 2048 ) +	\
+		((X & 0x1fff) / 8)) &=			\
+    ~((u_int8_t)1 << ((X & 0x1fff) % 8));
+
+struct kvm_hw_ptr *vmx_get_hw(void);
+
+#endif /* !_MACHINE_VMX_H_ */
diff --git a/sys/sys/proc.h b/sys/sys/proc.h
index 7c3b7f0..8ded73c 100644
--- a/sys/sys/proc.h
+++ b/sys/sys/proc.h
@@ -270,6 +270,10 @@ struct process {
      "\017NOZOMBIE" "\020STOPPED" "\021SYSTEM" "\022EMBRYO" "\023ZOMBIE" \
      "\024NOBROADCASTKILL" "\025PLEDGE")
 
+#ifdef __amd64__
+struct kvm_p;
+#endif
+
 
 struct proc {
 	TAILQ_ENTRY(proc) p_runq;
@@ -318,6 +322,10 @@ struct proc {
 	void	*p_emuldata;		/* Per-process emulation data, or */
 					/* NULL. Malloc type M_EMULDATA */
 	int	 p_siglist;		/* Signals arrived but not delivered. */
+#ifdef __amd64__
+        struct  kvm_p *p_kvm;
+        struct  kvm_vcpu *p_kvm_vcpu;
+#endif
 
 /* End area that is zeroed on creation. */
 #define	p_endzero	p_startcopy
